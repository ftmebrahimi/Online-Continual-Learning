{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1BIBJep39wY",
        "outputId": "fbbb2543-30dd-4dd3-8e5d-7d8171f24a9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting comet-ml\n",
            "  Downloading comet_ml-3.44.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting everett<3.2.0,>=1.0.1 (from everett[ini]<3.2.0,>=1.0.1->comet-ml)\n",
            "  Downloading everett-3.1.0-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (4.23.0)\n",
            "Requirement already satisfied: psutil>=5.6.3 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (5.9.5)\n",
            "Collecting python-box<7.0.0 (from comet-ml)\n",
            "  Downloading python_box-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.8 kB)\n",
            "Collecting requests-toolbelt>=0.8.0 (from comet-ml)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (2.31.0)\n",
            "Collecting semantic-version>=2.8.0 (from comet-ml)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting sentry-sdk>=1.1.0 (from comet-ml)\n",
            "  Downloading sentry_sdk-2.11.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting simplejson (from comet-ml)\n",
            "  Downloading simplejson-3.19.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (2.0.7)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (1.14.1)\n",
            "Collecting wurlitzer>=1.0.2 (from comet-ml)\n",
            "  Downloading wurlitzer-3.1.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting dulwich!=0.20.33,>=0.20.6 (from comet-ml)\n",
            "  Downloading dulwich-0.22.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: rich>=13.3.2 in /usr/local/lib/python3.10/dist-packages (from comet-ml) (13.7.1)\n",
            "Collecting configobj (from everett[ini]<3.2.0,>=1.0.1->comet-ml)\n",
            "  Downloading configobj-5.0.8-py2.py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (0.19.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet-ml) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet-ml) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet-ml) (2024.7.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.2->comet-ml) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.2->comet-ml) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet-ml) (0.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from configobj->everett[ini]<3.2.0,>=1.0.1->comet-ml) (1.16.0)\n",
            "Downloading comet_ml-3.44.3-py3-none-any.whl (682 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.3/682.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dulwich-0.22.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (979 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m979.1/979.1 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading everett-3.1.0-py2.py3-none-any.whl (35 kB)\n",
            "Downloading python_box-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading sentry_sdk-2.11.0-py2.py3-none-any.whl (303 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.6/303.6 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wurlitzer-3.1.1-py3-none-any.whl (8.6 kB)\n",
            "Downloading simplejson-3.19.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading configobj-5.0.8-py2.py3-none-any.whl (36 kB)\n",
            "Installing collected packages: everett, wurlitzer, simplejson, sentry-sdk, semantic-version, python-box, dulwich, configobj, requests-toolbelt, comet-ml\n",
            "  Attempting uninstall: python-box\n",
            "    Found existing installation: python-box 7.2.0\n",
            "    Uninstalling python-box-7.2.0:\n",
            "      Successfully uninstalled python-box-7.2.0\n",
            "Successfully installed comet-ml-3.44.3 configobj-5.0.8 dulwich-0.22.1 everett-3.1.0 python-box-6.1.0 requests-toolbelt-1.0.0 semantic-version-2.10.0 sentry-sdk-2.11.0 simplejson-3.19.2 wurlitzer-3.1.1\n"
          ]
        }
      ],
      "source": [
        "pip install comet-ml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEkdjhYBhN1n"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import copy\n",
        "import pdb\n",
        "import itertools\n",
        "import random\n",
        "import string\n",
        "import math\n",
        "from pathlib import Path\n",
        "from collections import OrderedDict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import MNIST\n",
        "import torch.nn.utils.weight_norm as weightNorm\n",
        "from torch.nn.functional import relu, avg_pool2d\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data.sampler import Sampler, RandomSampler\n",
        "import torchvision.transforms.functional as TorchVisionFunc\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset, ConcatDataset, Subset\n",
        "\n",
        "from comet_ml import Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MxXNmOBKF4N"
      },
      "outputs": [],
      "source": [
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def save_model(model, path):\n",
        "    torch.save(model.cpu(), path)\n",
        "\n",
        "def load_model(path):\n",
        "    model = torch.load(path)\n",
        "    return model\n",
        "\n",
        "def setup_experiment(experiment, config):\n",
        "    Path(f\"./checkpoints/{config['dataset']}\").mkdir(parents=True, exist_ok=True)\n",
        "    init_model = ResNet18(config=config) if 'cifar' in config['dataset'] else MLP(config)\n",
        "    save_model(init_model, f\"./checkpoints/{config['dataset']}/init.pth\")\n",
        "    experiment.log_parameters(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2WlAU5VMw5E"
      },
      "source": [
        "# Compute Grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hutcgNocMv2J"
      },
      "outputs": [],
      "source": [
        "def flatten_grads(m, numpy_output=False, bias=True, only_linear=False):\n",
        "    total_grads = []\n",
        "    for name, param in m.named_parameters():\n",
        "        if only_linear:\n",
        "            if (bias or not 'bias' in name) and 'linear' in name:\n",
        "                total_grads.append(param.grad.detach().view(-1))\n",
        "        else:\n",
        "            if (bias or not 'bias' in name) and not 'bn' in name and not 'IC' in name:\n",
        "                try:\n",
        "                    total_grads.append(param.grad.detach().view(-1))\n",
        "                except AttributeError:\n",
        "                    pass\n",
        "\n",
        "    total_grads = torch.cat(total_grads)\n",
        "    if numpy_output:\n",
        "        return total_grads.cpu().detach().numpy()\n",
        "    return total_grads\n",
        "\n",
        "\n",
        "def compute_and_flatten_example_grads(m, criterion, data, target, task_id):\n",
        "    _eg = []\n",
        "    criterion2 = nn.CrossEntropyLoss(reduction='none').to(DEVICE)\n",
        "    m.eval()\n",
        "    m.zero_grad()\n",
        "    pred = m(data, task_id)\n",
        "    loss = criterion2(pred, target)\n",
        "    for idx in range(len(data)):\n",
        "        loss[idx].backward(retain_graph=True)\n",
        "        _g = flatten_grads(m, numpy_output=True)\n",
        "        _eg.append(torch.Tensor(_g))\n",
        "        m.zero_grad()\n",
        "    return torch.stack(_eg)\n",
        "\n",
        "def flatten_example_grads(m, numpy_output=False):\n",
        "    total_grads = []\n",
        "    for param in m.parameters():\n",
        "            total_grads.append(param.grad1.view(param.grad1.size()[0], -1))\n",
        "    total_grads = torch.cat(total_grads, 1)\n",
        "    if numpy_output:\n",
        "        return total_grads.cpu().detach().numpy()\n",
        "    return total_grads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_etpVn0M2TP"
      },
      "source": [
        "# Coreset Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGXNSnIZwC-L"
      },
      "outputs": [],
      "source": [
        "def sample_selection(g, eg):\n",
        "    ng = torch.norm(g)\n",
        "    neg = torch.norm(eg, dim=1)\n",
        "    mean_sim = torch.matmul(g,eg.t()) / torch.maximum(ng*neg, torch.ones_like(neg)*1e-6)\n",
        "    negd = torch.unsqueeze(neg, 1)\n",
        "\n",
        "    cross_div = torch.matmul(eg,eg.t()) / torch.maximum(torch.matmul(negd, negd.t()), torch.ones_like(negd)*1e-6)\n",
        "    mean_div = torch.mean(cross_div, 0)\n",
        "\n",
        "    measure = mean_sim - mean_div\n",
        "    _, u_idx = torch.sort(measure, descending=True)\n",
        "    return u_idx.cpu().numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YifnpGnkD5W6"
      },
      "outputs": [],
      "source": [
        "def classwise_fair_selection(task, cand_target, sorted_index, num_per_label, config, is_shuffle=True):\n",
        "    num_examples_per_task = config['memory_size'] // task\n",
        "    num_examples_per_class = num_examples_per_task // config['n_classes']\n",
        "    num_residuals = num_examples_per_task - num_examples_per_class * config['n_classes']\n",
        "    residuals =  np.sum([(num_examples_per_class - n_c)*(num_examples_per_class > n_c) for n_c in num_per_label])\n",
        "    num_residuals += residuals\n",
        "\n",
        "    # Get the number of coreset instances per class\n",
        "    while True:\n",
        "        n_less_sample_class =  np.sum([(num_examples_per_class > n_c) for n_c in num_per_label])\n",
        "        num_class = (config['n_classes']-n_less_sample_class)\n",
        "        if (num_residuals // num_class) > 0:\n",
        "            num_examples_per_class += (num_residuals // num_class)\n",
        "            num_residuals -= (num_residuals // num_class) * num_class\n",
        "        else:\n",
        "            break\n",
        "    # Get best coresets per class\n",
        "    selected = []\n",
        "    target_tid = np.floor(max(cand_target)/config['n_classes'])\n",
        "\n",
        "    for j in range(config['n_classes']):\n",
        "        position = np.squeeze((cand_target[sorted_index]==j+(target_tid*config['n_classes'])).nonzero())\n",
        "        if position.numel() > 1:\n",
        "            selected.append(position[:num_examples_per_class])\n",
        "        elif position.numel() == 0:\n",
        "            continue\n",
        "        else:\n",
        "            selected.append([position])\n",
        "    # Fill rest space as best residuals\n",
        "    selected = np.concatenate(selected)\n",
        "    unselected = np.array(list(set(np.arange(num_examples_per_task))^set(selected)))\n",
        "    final_num_residuals = num_examples_per_task - len(selected)\n",
        "    best_residuals = unselected[:final_num_residuals]\n",
        "    selected = np.concatenate([selected, best_residuals])\n",
        "\n",
        "    if is_shuffle:\n",
        "        random.shuffle(selected)\n",
        "\n",
        "    return sorted_index[selected.astype(int)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Snn4jL5dJSDP"
      },
      "outputs": [],
      "source": [
        "def select_coreset(loader, task, model, candidates, config, candidate_size=250, fair_selection=True):\n",
        "    criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "    temp_optimizer = torch.optim.SGD(model.parameters(), lr=config['seq_lr'], momentum=config['momentum'])\n",
        "    temp_optimizer.zero_grad()\n",
        "\n",
        "    if fair_selection:\n",
        "        # collect candidates\n",
        "        cand_data, cand_target = [], []\n",
        "        cand_size = len(candidates)\n",
        "        for batch_idx, (data, target, task_id) in enumerate(loader['sequential'][task]['train']):\n",
        "            if batch_idx == cand_size:\n",
        "                break\n",
        "            try:\n",
        "                cand_data.append(data[candidates[batch_idx]])\n",
        "                cand_target.append(target[candidates[batch_idx]])\n",
        "            except IndexError:\n",
        "                pass\n",
        "        cand_data = torch.cat(cand_data, 0)\n",
        "        cand_target = torch.cat(cand_target, 0)\n",
        "\n",
        "        random_pick_up = torch.randperm(len(cand_target))[:candidate_size]\n",
        "        cand_data = cand_data[random_pick_up]\n",
        "        cand_target = cand_target[random_pick_up]\n",
        "\n",
        "        num_per_label = [len((cand_target==(jj+config['n_classes']*(task-1))).nonzero()) for jj in range(config['n_classes'])]\n",
        "        #print('num samples per label', num_per_label)\n",
        "\n",
        "        num_examples_per_task = config['memory_size'] // task\n",
        "        pred = model(cand_data.to(DEVICE), task)\n",
        "        loss = criterion(pred, cand_target.long().to(DEVICE))\n",
        "        loss.backward()\n",
        "\n",
        "        # Coreset update\n",
        "        _eg = compute_and_flatten_example_grads(model, criterion, cand_data.to(DEVICE), cand_target.long().to(DEVICE), task)\n",
        "        _g = torch.mean(_eg, 0)\n",
        "        sorted = sample_selection(_g, _eg)\n",
        "\n",
        "        pick = torch.randperm(len(sorted))\n",
        "        selected = classwise_fair_selection(task, cand_target, pick, num_per_label, config, is_shuffle=True)\n",
        "\n",
        "        loader['coreset'][task]['train'].data = copy.deepcopy(cand_data[selected])\n",
        "        loader['coreset'][task]['train'].targets = copy.deepcopy(cand_target[selected])\n",
        "        num_per_label = [len((cand_target[selected]==(jj+config['n_classes']*(task-1))).nonzero()) for jj in range(config['n_classes'])]\n",
        "        #print('after select_coreset, num samples per label', num_per_label)\n",
        "    else:\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBby5SigNLlX"
      },
      "outputs": [],
      "source": [
        "def reconstruct_coreset_loader(config, dataset, task):\n",
        "    trains = []\n",
        "    all_coreset = {}\n",
        "\n",
        "    for tid in range(1, task+1):\n",
        "        num_examples_per_task = config['memory_size'] // task\n",
        "        coreset = Coreset(num_examples_per_task, input_shape=[3, 32, 32])\n",
        "\n",
        "        pick_idx = torch.randperm(num_examples_per_task)\n",
        "        coreset.data = copy.deepcopy(dataset[tid]['train'].data[pick_idx])\n",
        "        coreset.targets = copy.deepcopy(dataset[tid]['train'].targets[pick_idx])\n",
        "        coreset_loader = torch.utils.data.DataLoader(coreset, batch_size=config['batch_size'], shuffle=False, num_workers=0, pin_memory=True)\n",
        "        train_loader = fast_cifar_loader(coreset_loader, tid, eval=False)\n",
        "        if ('cifar' in config['dataset']):\n",
        "            train_loader = fast_cifar_loader(coreset_loader, tid, eval=False)\n",
        "        else:\n",
        "            train_loader = fast_mnist_loader(coreset_loader, eval=False)\n",
        "        trains += train_loader\n",
        "    all_coreset = random.sample(trains[:], len(trains))\n",
        "    return all_coreset\n",
        "\n",
        "\n",
        "def get_coreset_loss(model, iterloader, config):\n",
        "    criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "    model.train()\n",
        "    coreset_loss = 0\n",
        "    count = 0\n",
        "    data, target, task_id = iterloader\n",
        "    count += len(target)\n",
        "    data = data.to(DEVICE)\n",
        "    target = target.to(DEVICE)\n",
        "    output = model(data, task_id)\n",
        "    coreset_loss += criterion(output, target.long())\n",
        "    coreset_loss /= count\n",
        "    return coreset_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeXU3AUxKZky"
      },
      "outputs": [],
      "source": [
        "def update_coreset(loader, task, model, task_id, config):\n",
        "    # Coreset update\n",
        "    num_examples_per_task = config['memory_size'] // task\n",
        "    prv_nept = config['memory_size'] // (task-1)\n",
        "\n",
        "    for tid in range(1, task):\n",
        "        criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "        temp_optimizer = torch.optim.SGD(model.parameters(), lr=config['seq_lr'], momentum=config['momentum'])\n",
        "\n",
        "        tid_coreset = loader['coreset'][tid]['train'].data\n",
        "        tid_targets = loader['coreset'][tid]['train'].targets\n",
        "\n",
        "        temp_optimizer.zero_grad()\n",
        "\n",
        "        pred = model(tid_coreset.to(DEVICE), task_id)\n",
        "        loss = criterion(pred, tid_targets.long().to(DEVICE))\n",
        "        loss.backward()\n",
        "        _tid_eg = compute_and_flatten_example_grads(model, criterion, tid_coreset.to(DEVICE), tid_targets.to(DEVICE), tid)\n",
        "        _tid_g = torch.mean(_tid_eg, 0)\n",
        "        pick = sample_selection(_tid_g, _tid_eg)\n",
        "\n",
        "        class_idx = [tid_targets.cpu().numpy() == i for i in range(config['n_classes'])]\n",
        "        num_per_label = [len((tid_targets.cpu()==(jj+config['n_classes']*(task-1))).nonzero()) for jj in range(config['n_classes'])]\n",
        "\n",
        "        selected = classwise_fair_selection(task, tid_targets, pick, num_per_label, config)\n",
        "        _nn = [len((tid_targets[selected]==(jj+config['n_classes']*(tid-1))).nonzero()) for jj in range(config['n_classes'])]\n",
        "\n",
        "        loader['coreset'][tid]['train'].data = copy.deepcopy(loader['coreset'][tid]['train'].data[selected])\n",
        "        loader['coreset'][tid]['train'].targets = copy.deepcopy(loader['coreset'][tid]['train'].targets[selected])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDF_Gk_RM_7K"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFjRBJehMozG"
      },
      "outputs": [],
      "source": [
        "def train_single_step(model, optimizer, loader, task, step, config):\n",
        "    criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "    is_last_step = True if step == config['n_substeps'] else False\n",
        "    candidates_indices=[]\n",
        "    for batch_idx, (data, target, task_id) in enumerate(loader['sequential'][task]['train']):\n",
        "        model.train()\n",
        "        data = data.to(DEVICE)\n",
        "        target = target.to(DEVICE)\n",
        "        is_ocspick = True if (config['ocspick'] and len(data) > config['batch_size']) else False\n",
        "        optimizer.zero_grad()\n",
        "        if is_ocspick and step != 1:\n",
        "            _eg = compute_and_flatten_example_grads(model, criterion, data, target, task_id)\n",
        "            _g = torch.mean(_eg, 0)\n",
        "            sorted = sample_selection(_g, _eg, config)\n",
        "            pick = sorted[:config['batch_size']]\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(data[pick], task_id)\n",
        "            loss = criterion(pred, target[pick])\n",
        "            loss.backward()\n",
        "\n",
        "            # Select coresets at final step\n",
        "            if is_last_step:\n",
        "                candidates_indices.append(pick)\n",
        "        else:\n",
        "            size = min(len(data), config['batch_size'])\n",
        "            pick = torch.randperm(len(data))[:size]\n",
        "            pred = model(data[pick], task_id)\n",
        "            loss = criterion(pred, target[pick])\n",
        "            loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if is_last_step:\n",
        "        select_coreset(loader, task, model, candidates_indices, config)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAUg7N59Y_Rb"
      },
      "outputs": [],
      "source": [
        "def train_ocs_single_step(model, optimizer, loader, task, step, config):\n",
        "    criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "    is_last_step = step == config['n_substeps']\n",
        "\n",
        "    # Concatenate previous task data and targets\n",
        "    prev_coreset = [loader['coreset'][tid]['train'].data for tid in range(1, task)]\n",
        "    prev_targets = [loader['coreset'][tid]['train'].targets for tid in range(1, task)]\n",
        "    c_x = torch.cat(prev_coreset, 0)\n",
        "    c_y = torch.cat(prev_targets, 0)\n",
        "\n",
        "    ref_loader = reconstruct_coreset_loader(config, loader['coreset'], task - 1)\n",
        "    ref_iterloader = itertools.cycle(ref_loader)  # Create an iterator that cycles through ref_loader\n",
        "\n",
        "    candidates_indices = []\n",
        "\n",
        "    for batch_idx, (data, target, task_id) in enumerate(loader['sequential'][task]['train']):\n",
        "        model.eval()\n",
        "        optimizer.zero_grad()\n",
        "        is_rand_start = step == 1\n",
        "\n",
        "        # Compute reference gradients\n",
        "        ref_pred = model(c_x.to(DEVICE), task)\n",
        "        ref_loss = criterion(ref_pred, c_y.long().to(DEVICE))\n",
        "        ref_loss.backward()\n",
        "        ref_grads = copy.deepcopy(flatten_grads(model))\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        data = data.to(DEVICE)\n",
        "        target = target.to(DEVICE)\n",
        "\n",
        "        if is_rand_start:\n",
        "            size = min(len(data), config['batch_size'])\n",
        "            pick = torch.randperm(len(data))[:size]\n",
        "        else:\n",
        "            # Compute example gradients and select samples\n",
        "            _eg = compute_and_flatten_example_grads(model, criterion, data, target, task_id)\n",
        "            _g = torch.mean(_eg, 0)\n",
        "            sorted_indices = sample_selection(_g.cuda(), _eg.cuda(), config, ref_grads=ref_grads)\n",
        "            pick = sorted_indices[:config['batch_size']]\n",
        "\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(data[pick], task_id)\n",
        "        loss = criterion(pred, target[pick])\n",
        "\n",
        "        ref_data = next(ref_iterloader)\n",
        "        ref_loss = get_coreset_loss(model, ref_data, config)\n",
        "        loss += config['ref_hyp'] * ref_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if is_last_step:\n",
        "            candidates_indices.append(pick)\n",
        "\n",
        "    if is_last_step:\n",
        "        select_coreset(loader, task, model, candidates_indices, config)\n",
        "        update_coreset(loader, task, model, task_id, config)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9x6_PYeiJBVr"
      },
      "outputs": [],
      "source": [
        "def eval_single_epoch(model, loader, config):\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "    total_loss, correct, total_samples = 0.0, 0, 0\n",
        "\n",
        "    class_correct = np.zeros(config['n_classes'])\n",
        "    class_total = np.zeros(config['n_classes'])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target, task_id in loader:\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "            output = model(data, task_id)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            total_loss += loss.item() * len(target)\n",
        "            total_samples += len(target)\n",
        "\n",
        "            preds = output.argmax(dim=1)\n",
        "            correct += (preds == target).sum().item()\n",
        "\n",
        "            for cid in range(config['n_classes']):\n",
        "                cid_index = (target == cid) if 'cifar' not in config['dataset'] else (target == cid + (task_id - 1) * config['n_classes'])\n",
        "                class_correct[cid] += (preds == target)[cid_index].sum().item()\n",
        "                class_total[cid] += cid_index.sum().item()\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    avg_acc = 100.0 * correct / total_samples\n",
        "    per_class_acc = [round(100.0 * a / b, 4) if b > 0 else 0.0 for a, b in zip(class_correct, class_total)]\n",
        "\n",
        "    return {'accuracy': avg_acc, 'per_class_accuracy': per_class_acc, 'loss': avg_loss}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-SoOeeAJBac"
      },
      "outputs": [],
      "source": [
        "def train_task_sequentially(task, train_loader, config, summary=None):\n",
        "    current_lr = config['seq_lr'] * (config['lr_decay'])**(task-1)\n",
        "    prev_model_path = f\"./checkpoints/{config['dataset']}/init.pth\"\n",
        "    model = load_model(prev_model_path).to(DEVICE)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=current_lr, momentum=config['momentum'])\n",
        "    config['n_substeps'] = int((config['stream_size'] / config['batch_size']))\n",
        "    for _step in range(1, config['n_substeps']+1):\n",
        "        if task == 1 or (config['ocspick'] == False):\n",
        "            model = train_single_step(model, optimizer, train_loader, task, _step, config)\n",
        "        else:\n",
        "            model = train_ocs_single_step(model, optimizer, train_loader, task, _step, config)\n",
        "        metrics = eval_single_epoch(model, train_loader['sequential'][task]['val'], config)\n",
        "        print('Epoch {} >> (per-task accuracy): {}'.format(_step/config['n_substeps'], np.mean(metrics['accuracy'])))\n",
        "        print('Epoch {} >> (class accuracy): {}'.format(_step/config['n_substeps'], metrics['per_class_accuracy']))\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egB4hel3NH21"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zRoLVvjZJa4"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(MLP, self).__init__()\n",
        "        self.save_acts = False\n",
        "        self.acts = {}\n",
        "        self.config = config\n",
        "        self.W1 = nn.Linear(784, config['mlp_hiddens'])\n",
        "        self.dropout_1 =  nn.Dropout(p=config['dropout'])\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.W2 = nn.Linear(config['mlp_hiddens'], config['mlp_hiddens'])\n",
        "        self.dropout_2 =  nn.Dropout(p=config['dropout'])\n",
        "\n",
        "        self.W3 = nn.Linear(config['mlp_hiddens'], 10)\n",
        "        # self.dropout_p = config['dropout']\n",
        "\n",
        "    def embed(self, x):\n",
        "        out = self.W1(x)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        if self.save_acts:\n",
        "            self.acts['layer 1'] = out.detach().clone()\n",
        "\n",
        "        if self.config['dropout'] > 0:\n",
        "            out = self.dropout_1(out)\n",
        "        out = self.W2(out)\n",
        "        self.feature = self.relu(out)\n",
        "        if self.save_acts:\n",
        "            self.acts['layer 2'] = self.feature.detach().clone()\n",
        "        if self.config['dropout'] > 0:\n",
        "            out = self.dropout_2(self.feature)\n",
        "        return out\n",
        "\n",
        "\n",
        "    def forward(self, x, task_id=None):\n",
        "        # x = x.view(-1, 784 + self.num_condition_neurons)\n",
        "        out = self.embed(x)\n",
        "        out = self.W3(self.feature)\n",
        "        # out = nn.functional.dropout(out, p=self.dropout_p)\n",
        "        return out\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, config={}):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1,\n",
        "                          stride=stride, bias=False),\n",
        "            )\n",
        "        self.IC1 = nn.Sequential(\n",
        "            nn.BatchNorm2d(planes, track_running_stats=False),\n",
        "            nn.Dropout(p=config['dropout'])\n",
        "            )\n",
        "\n",
        "        self.IC2 = nn.Sequential(\n",
        "            nn.BatchNorm2d(planes, track_running_stats=False),\n",
        "            nn.Dropout(p=config['dropout'])\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = relu(out)\n",
        "        out = self.IC1(out)\n",
        "\n",
        "        out += self.shortcut(x)\n",
        "        out = relu(out)\n",
        "        out = self.IC2(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes, nf, config={}):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = nf\n",
        "        self.conv1 = conv3x3(3, nf * 1)\n",
        "        self.bn1 = nn.BatchNorm2d(nf * 1, track_running_stats=False)\n",
        "        self.layer1 = self._make_layer(block, nf * 1, num_blocks[0], stride=1, config=config)\n",
        "        self.layer2 = self._make_layer(block, nf * 2, num_blocks[1], stride=2, config=config)\n",
        "        self.layer3 = self._make_layer(block, nf * 4, num_blocks[2], stride=2, config=config)\n",
        "        self.layer4 = self._make_layer(block, nf * 8, num_blocks[3], stride=2, config=config)\n",
        "        self.linear = nn.Linear(nf * 8 * block.expansion, num_classes)\n",
        "        self.config =config\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride, config):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride, config=config))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def embed(self, x):\n",
        "        bsz = x.size(0)\n",
        "        out = relu(self.bn1(self.conv1(x.view(bsz, 3, 32, 32))))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x, task_id):\n",
        "        out = self.embed(x)\n",
        "        out = self.linear(out)\n",
        "        t = task_id\n",
        "        if isinstance(self.config['n_classes'], int):\n",
        "            offset1 = int((t-1) * 5)\n",
        "            offset2 = int(t * 5)\n",
        "            if offset1 > 0:\n",
        "                out[:, :offset1].data.fill_(-10e10)\n",
        "            if offset2 < 100:\n",
        "                out[:, offset2:100].data.fill_(-10e10)\n",
        "            return out\n",
        "        else:\n",
        "            offsets = [sum(self.config['n_classes'][:c]) for c in range(1,len(self.config['n_classes'])+1)]\n",
        "            offset1 = int(offsets[t-1])\n",
        "            offset2 = int(offsets[t])\n",
        "            if offset1 > 0:\n",
        "                out[:, :offset1].data.fill_(-10e10)\n",
        "            if offset2 < offsets[-1]:\n",
        "                out[:, offset2:offsets[-1]].data.fill_(-10e10)\n",
        "            return out\n",
        "\n",
        "def ResNet18(nclasses=100, nf=20, config={}):\n",
        "    net = ResNet(BasicBlock, [2, 2, 2, 2], nclasses, nf, config=config)\n",
        "    return net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH8_iqekvhB_"
      },
      "source": [
        "# ِDataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoSoZlax78ME"
      },
      "source": [
        "# MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAQ0JaJYy_qI"
      },
      "outputs": [],
      "source": [
        "class RotationTransform:\n",
        "    def __init__(self, angle):\n",
        "        self.angle = angle\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return TorchVisionFunc.rotate(x, self.angle, fill=(0,))\n",
        "\n",
        "def fast_mnist_loader(loaders, eval=True, device='cpu'):\n",
        "    trains, evals = [], []\n",
        "    if eval:\n",
        "        train_loader, eval_loader = loaders\n",
        "        for data, target in train_loader:\n",
        "            data = data.to(device).view(-1, 784)\n",
        "            target = target.to(device)\n",
        "            trains.append([data, target, None])\n",
        "\n",
        "        for data, target in eval_loader:\n",
        "            data = data.to(device).view(-1, 784)\n",
        "            target = target.to(device)\n",
        "            evals.append([data, target, None])\n",
        "        return trains, evals\n",
        "    else:\n",
        "        train_loader = loaders\n",
        "\n",
        "        for data, target in train_loader:\n",
        "            data = data.to(device).view(-1, 784)\n",
        "            target = target.to(device)\n",
        "            trains.append([data, target, None])\n",
        "        return trains\n",
        "\n",
        "def get_balanced_noisy_rotated_mnist(task_id, batch_size, per_task_rotation):\n",
        "    print('Noisy Rotated MNIST')\n",
        "    rotation_degree = (task_id - 1)*per_task_rotation\n",
        "\n",
        "    train_transforms = torchvision.transforms.Compose([\n",
        "        RotationTransform(rotation_degree),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    test_transforms = torchvision.transforms.Compose([\n",
        "        RotationTransform(rotation_degree),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    train_dataset = MNIST('./data/', train=True, download=True, transform=train_transforms)\n",
        "    test_dataset = MNIST('./data/', train=False, download=True, transform=test_transforms)\n",
        "    end_idx = int(len(train_dataset.data) * 0.6)\n",
        "    shuffle = torch.randperm(len(train_dataset.data))\n",
        "    idx = shuffle[:end_idx].long()\n",
        "    for i in idx:\n",
        "        train_dataset.data[i] = torch.randn(train_dataset.data[i].size())\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
        "    test_loader = torch.utils.data.DataLoader(MNIST('./data/', train=False, download=True, transform=test_transforms),  batch_size=256, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "def get_rotated_mnist(task_id, batch_size, per_task_rotation):\n",
        "    print('Rotated MNIST')\n",
        "    rotation_degree = (task_id - 1)*per_task_rotation\n",
        "\n",
        "    transforms = torchvision.transforms.Compose([\n",
        "        RotationTransform(rotation_degree),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(MNIST('./data/', train=True, download=True, transform=transforms), batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
        "    test_loader = torch.utils.data.DataLoader(MNIST('./data/', train=False, download=True, transform=transforms),  batch_size=256, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oJhQZTR8LyR"
      },
      "source": [
        "# CIFAR100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7ce1yYKvgfx"
      },
      "outputs": [],
      "source": [
        "def fast_cifar_loader(loaders, task_id, eval=True, device='cpu'):\n",
        "    trains, evals = [], []\n",
        "    if eval:\n",
        "        train_loader, eval_loader = loaders\n",
        "        for data, target in train_loader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            trains.append([data, target, task_id])\n",
        "\n",
        "        for data, target in eval_loader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            evals.append([data, target, task_id])\n",
        "        return trains, evals\n",
        "    else:\n",
        "        for data, target in loaders:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            trains.append([data, target, task_id])\n",
        "        return trains\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "def get_split_cifar100(task_id, batch_size, cifar_train, cifar_test):\n",
        "    # Calculate class range for the task\n",
        "    start_class = (task_id - 1) * 5\n",
        "    end_class = task_id * 5\n",
        "\n",
        "    # Get indices of samples belonging to the current task's classes\n",
        "    train_indices = np.where((np.array(cifar_train.targets) >= start_class) &\n",
        "                             (np.array(cifar_train.targets) < end_class))[0]\n",
        "\n",
        "    test_indices = np.where((np.array(cifar_test.targets) >= start_class) &\n",
        "                            (np.array(cifar_test.targets) < end_class))[0]\n",
        "\n",
        "    # Create DataLoader for the training and testing subsets\n",
        "    train_loader = DataLoader(Subset(cifar_train, train_indices),\n",
        "                              batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    test_loader = DataLoader(Subset(cifar_test, test_indices),\n",
        "                             batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "def get_imbalanced_split_cifar100(task_id, batch_size, cifar_train, cifar_test):\n",
        "    # Predefined shuffle of classes to introduce imbalance\n",
        "    class_shuffle = [25, 11, 23, 76, 12, 30, 62,  6, 89, 44, 84, 29, 82,  3, 10, 24, 64, 72,\n",
        "                     21,  8, 63, 71, 68, 74,  5, 86, 22, 58, 95, 19, 47, 54, 56,  2, 20, 96,\n",
        "                     57, 38, 80, 66,  1, 59, 16, 97, 18, 73, 31, 77, 99, 15, 46, 27, 83, 40,\n",
        "                     48, 88, 33, 36, 81, 55, 85, 14, 13,  7, 65, 50, 78, 43, 91,  4, 69, 52,\n",
        "                     41, 94, 34, 51, 37,  9, 90, 35, 92, 26, 42,  0, 17, 87, 53, 93, 32, 28,\n",
        "                     75, 67, 49, 79, 61, 60, 70, 98, 45, 39]\n",
        "\n",
        "    n_class = 100\n",
        "    img_max = len(cifar_train.targets) // n_class\n",
        "    imb_factor = 1 / 10.\n",
        "\n",
        "    # Create imbalanced indices for training data\n",
        "    indices_per_class = []\n",
        "    for class_number in range(n_class):\n",
        "        class_indices = np.where(np.array(cifar_train.targets) == class_shuffle[class_number])[0]\n",
        "        num_samples = int(img_max * (imb_factor ** (class_number / (n_class - 1))))\n",
        "        indices_per_class.append(class_indices[:num_samples])\n",
        "\n",
        "    imbalanced_indices = np.concatenate(indices_per_class)\n",
        "    np.random.shuffle(imbalanced_indices)\n",
        "\n",
        "    # Create a new imbalanced CIFAR-100 training set\n",
        "    cifar_train_new = copy.deepcopy(cifar_train)\n",
        "    cifar_train_new.data = cifar_train.data[imbalanced_indices]\n",
        "    cifar_train_new.targets = [cifar_train.targets[i] for i in imbalanced_indices]\n",
        "\n",
        "    # Determine class range for the current task\n",
        "    start_class = (task_id - 1) * 5\n",
        "    end_class = task_id * 5\n",
        "\n",
        "    # Get indices for the current task's classes\n",
        "    train_indices = [i for i, target in enumerate(cifar_train_new.targets) if start_class <= target < end_class]\n",
        "    test_indices = [i for i, target in enumerate(cifar_test.targets) if start_class <= target < end_class]\n",
        "\n",
        "    # Create DataLoaders for the current task\n",
        "    train_loader = DataLoader(Subset(cifar_train_new, train_indices), batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(Subset(cifar_test, test_indices), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    print('Imbalanced SplitCifar100')\n",
        "    if task_id == 1:\n",
        "        print('Number of instances per class:', [len(indices) for indices in indices_per_class])\n",
        "    current_task_targets = np.array(cifar_train_new.targets)[train_indices]\n",
        "    print('Number of instances per class for task {}: {}'.format(\n",
        "        task_id, [np.sum(current_task_targets == c) for c in range(start_class, end_class)]\n",
        "    ))\n",
        "\n",
        "    return train_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwoXYfV88ga9"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkbVYHou8l6R"
      },
      "outputs": [],
      "source": [
        "class Coreset(torch.utils.data.Dataset):\n",
        "    def __init__(self, set_size, input_shape=[784]):\n",
        "        data_shape = [set_size] + input_shape\n",
        "\n",
        "        self.data = torch.zeros(data_shape)\n",
        "        self.targets = torch.ones((set_size)) * -1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx]\n",
        "        y = self.targets[idx]\n",
        "        return x, y\n",
        "\n",
        "\n",
        "def get_all_loaders(config, per_task_rotation=9, is_coreset=True):\n",
        "\n",
        "    dataset = config['dataset'].lower()\n",
        "    loaders = {'sequential': {}, 'coreset': {}}\n",
        "    class_arr = range(0, 100)\n",
        "\n",
        "    cifar_transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),])\n",
        "    cifar_train = torchvision.datasets.CIFAR100('./data/', train=True, download=True, transform=cifar_transforms)\n",
        "    cifar_test = torchvision.datasets.CIFAR100('./data/', train=False, download=True, transform=cifar_transforms)\n",
        "    if is_coreset:\n",
        "        print('loading coreset placeholder {}'.format(dataset))\n",
        "        for task in range(1, config['num_tasks']+1):\n",
        "            loaders['sequential'][task], loaders['coreset'][task] = {}, {}\n",
        "            print(\"loading {} for task {}\".format(dataset, task))\n",
        "            num_example_per_task = config['memory_size']//task\n",
        "            classes = np.random.choice(class_arr, 5, replace=False)\n",
        "            if 'mnist' in dataset:\n",
        "              if 'noisy' in dataset:\n",
        "                  seq_loader_train , seq_loader_val = fast_mnist_loader(get_balanced_noisy_rotated_mnist(task, config['stream_size'], per_task_rotation), 'cpu')\n",
        "              else:\n",
        "                  seq_loader_train , seq_loader_val = fast_mnist_loader(get_rotated_mnist(task, config['stream_size'], per_task_rotation), 'cpu')\n",
        "              loaders['coreset'][task]['train'] = Coreset(num_example_per_task)\n",
        "            if 'cifar' in dataset:\n",
        "              if 'imb' in dataset:\n",
        "                  seq_loader_train , seq_loader_val = fast_cifar_loader(get_imbalanced_split_cifar100(task, config['stream_size'], cifar_train, cifar_test), task, 'cpu')\n",
        "              else:\n",
        "                  seq_loader_train , seq_loader_val = fast_cifar_loader(get_split_cifar100(task, config['stream_size'], cifar_train, cifar_test), task, 'cpu')\n",
        "              loaders['coreset'][task]['train'] = Coreset(num_example_per_task, [3, 32, 32])\n",
        "            loaders['sequential'][task]['train'], loaders['sequential'][task]['val'] = seq_loader_train, seq_loader_val\n",
        "\n",
        "        return loaders\n",
        "    else:\n",
        "        # Load sequential tasks\n",
        "\n",
        "\n",
        "        for task in range(1, config['num_tasks']+1):\n",
        "            loaders['sequential'][task] = {}\n",
        "            print(\"loading {} for task {}\".format(dataset, task))\n",
        "            if 'noisy' in dataset:\n",
        "                seq_loader_train , seq_loader_val = fast_mnist_loader(get_balanced_noisy_rotated_mnist(task, config['stream_size'], per_task_rotation), 'cpu')\n",
        "            elif 'mnist' in dataset:\n",
        "               seq_loader_train , seq_loader_val = fast_mnist_loader(get_rotated_mnist(task, config['stream_size'], per_task_rotation), 'cpu')\n",
        "            elif 'imb' in dataset:\n",
        "                seq_loader_train , seq_loader_val = fast_cifar_loader(get_imbalanced_split_cifar100(task, config['stream_size'], cifar_train, cifar_test), task, 'cpu')\n",
        "            else:\n",
        "                seq_loader_train , seq_loader_val = fast_cifar_loader(get_split_cifar100(task, config['stream_size'], cifar_train, cifar_test), task, 'cpu')\n",
        "            seq_loader_train , seq_loader_val = fast_cifar_loader(get_split_cifar100(task, config['stream_size'], cifar_train, cifar_test), task, 'cpu')\n",
        "            loaders['sequential'][task]['train'], loaders['sequential'][task]['val'] = seq_loader_train, seq_loader_val\n",
        "        return loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5Z-tuh3uZjn",
        "outputId": "2a2fc59f-c2af-4685-a837-ae5de4c58458"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 169001437/169001437 [00:01<00:00, 107031985.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data/\n",
            "Files already downloaded and verified\n",
            "loading coreset placeholder rot-mnist\n",
            "loading rot-mnist for task 1\n",
            "Rotated MNIST\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 133549550.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 29302296.52it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 63449127.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 5169750.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "loading rot-mnist for task 2\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 3\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 4\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 5\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 6\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 7\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 8\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 9\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 10\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 11\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 12\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 13\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 14\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 15\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 16\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 17\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 18\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 19\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 20\n",
            "Rotated MNIST\n",
            "---- Task 1 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 83.38\n",
            "Epoch 0.2 >> (class accuracy): [88.9796, 98.9427, 75.6783, 77.9208, 91.0387, 60.3139, 89.4572, 82.393, 85.9343, 79.4846]\n",
            "Epoch 0.4 >> (per-task accuracy): 90.63\n",
            "Epoch 0.4 >> (class accuracy): [96.7347, 97.7974, 80.6202, 90.8911, 92.5662, 87.1076, 88.4134, 89.2023, 90.0411, 91.9722]\n",
            "Epoch 0.6 >> (per-task accuracy): 92.85\n",
            "Epoch 0.6 >> (class accuracy): [97.2449, 98.5903, 84.7868, 92.1782, 96.2322, 92.9372, 92.4843, 91.9261, 90.8624, 90.8821]\n",
            "Epoch 0.8 >> (per-task accuracy): 94.46\n",
            "Epoch 0.8 >> (class accuracy): [97.3469, 98.9427, 90.2132, 94.5545, 96.0285, 95.5157, 93.9457, 93.677, 90.8624, 93.1615]\n",
            "Epoch 1.0 >> (per-task accuracy): 95.43\n",
            "Epoch 1.0 >> (class accuracy): [97.449, 99.0308, 93.7984, 95.5446, 97.1487, 97.3094, 95.1983, 95.0389, 90.7598, 92.7651]\n",
            "OCS >> Task 1: {'accuracy': 95.43, 'per_class_accuracy': [97.449, 99.0308, 93.7984, 95.5446, 97.1487, 97.3094, 95.1983, 95.0389, 90.7598, 92.7651], 'loss': 0.17292804017663002}\n",
            "OCS >> (average accuracy): 95.43\n",
            "OCS >> (Forgetting): 0.0\n",
            "Maximum per-task accuracies: [95.43]\n",
            "\n",
            "---- Task 2 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 85.89\n",
            "Epoch 0.2 >> (class accuracy): [95.4082, 98.1498, 81.2016, 87.2277, 90.1222, 73.6547, 91.858, 88.9105, 71.8686, 77.7998]\n",
            "Epoch 0.4 >> (per-task accuracy): 90.16\n",
            "Epoch 0.4 >> (class accuracy): [96.1224, 98.5022, 76.938, 89.2079, 88.5947, 89.1256, 89.7704, 91.537, 88.809, 92.1705]\n",
            "Epoch 0.6 >> (per-task accuracy): 92.83\n",
            "Epoch 0.6 >> (class accuracy): [96.8367, 98.6784, 84.1085, 92.1782, 93.3809, 94.2825, 92.9019, 93.7743, 89.7331, 92.0714]\n",
            "Epoch 0.8 >> (per-task accuracy): 94.75\n",
            "Epoch 0.8 >> (class accuracy): [97.7551, 98.7665, 90.8915, 93.0693, 95.9267, 96.861, 94.9896, 95.5253, 91.5811, 91.9722]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_2118/1459498902.py:12: RuntimeWarning: divide by zero encountered in scalar floor_divide\n",
            "  if (num_residuals // num_class) > 0:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1.0 >> (per-task accuracy): 95.77\n",
            "Epoch 1.0 >> (class accuracy): [98.1633, 98.6784, 92.8295, 94.5545, 96.7413, 97.7578, 96.0334, 95.9144, 93.7372, 93.2607]\n",
            "OCS >> Task 1: {'accuracy': 94.16, 'per_class_accuracy': [97.551, 98.4141, 90.3101, 93.6634, 95.8248, 96.6368, 93.215, 94.8444, 92.8131, 88.2061], 'loss': 0.2089909038066864}\n",
            "OCS >> Task 2: {'accuracy': 95.77, 'per_class_accuracy': [98.1633, 98.6784, 92.8295, 94.5545, 96.7413, 97.7578, 96.0334, 95.9144, 93.7372, 93.2607], 'loss': 0.16299812539815903}\n",
            "OCS >> (average accuracy): 94.965\n",
            "OCS >> (Forgetting): 0.012700000000000102\n",
            "Maximum per-task accuracies: [95.43]\n",
            "\n",
            "---- Task 3 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 82.1\n",
            "Epoch 0.2 >> (class accuracy): [96.6327, 98.1498, 78.3915, 82.2772, 86.4562, 48.4305, 91.4405, 88.6187, 71.3552, 73.9346]\n",
            "Epoch 0.4 >> (per-task accuracy): 89.17\n",
            "Epoch 0.4 >> (class accuracy): [97.8571, 98.5903, 81.5891, 86.4356, 88.2892, 81.6143, 90.8142, 90.5642, 85.2156, 88.999]\n",
            "Epoch 0.6 >> (per-task accuracy): 91.37\n",
            "Epoch 0.6 >> (class accuracy): [97.0408, 98.6784, 84.6899, 87.4257, 93.1772, 91.0314, 90.1879, 92.7043, 89.0144, 88.999]\n",
            "Epoch 0.8 >> (per-task accuracy): 93.23\n",
            "Epoch 0.8 >> (class accuracy): [97.3469, 98.7665, 87.9845, 91.2871, 93.9919, 93.8341, 94.0501, 93.4825, 89.5277, 91.5758]\n",
            "Epoch 1.0 >> (per-task accuracy): 94.47\n",
            "Epoch 1.0 >> (class accuracy): [97.7551, 98.7665, 89.7287, 92.5743, 94.6029, 96.0762, 94.572, 93.9689, 92.5051, 93.9544]\n",
            "OCS >> Task 1: {'accuracy': 89.19, 'per_class_accuracy': [96.6327, 97.4449, 80.1357, 89.1089, 91.7515, 84.7534, 90.1879, 89.6887, 87.0637, 84.0436], 'loss': 0.35347903671264647}\n",
            "OCS >> Task 2: {'accuracy': 93.36, 'per_class_accuracy': [97.2449, 98.6784, 89.6318, 92.6733, 94.0937, 92.713, 93.9457, 94.4553, 90.1437, 89.3954], 'loss': 0.24129643634557724}\n",
            "OCS >> Task 3: {'accuracy': 94.47, 'per_class_accuracy': [97.7551, 98.7665, 89.7287, 92.5743, 94.6029, 96.0762, 94.572, 93.9689, 92.5051, 93.9544], 'loss': 0.21146508929729463}\n",
            "OCS >> (average accuracy): 92.33999999999999\n",
            "OCS >> (Forgetting): 0.04155000000000008\n",
            "Maximum per-task accuracies: [95.43]\n",
            "\n",
            "---- Task 4 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 77.54\n",
            "Epoch 0.2 >> (class accuracy): [96.7347, 98.5903, 76.7442, 82.1782, 89.1039, 32.3991, 92.5887, 83.9494, 69.5072, 46.9772]\n",
            "Epoch 0.4 >> (per-task accuracy): 87.18\n",
            "Epoch 0.4 >> (class accuracy): [95.5102, 97.9736, 77.0349, 86.4356, 86.558, 79.9327, 90.7098, 86.4786, 81.4168, 88.0079]\n",
            "Epoch 0.6 >> (per-task accuracy): 89.92\n",
            "Epoch 0.6 >> (class accuracy): [96.8367, 98.326, 82.1705, 86.2376, 92.1589, 87.2197, 91.2317, 89.5914, 85.9343, 88.5035]\n",
            "Epoch 0.8 >> (per-task accuracy): 92.08\n",
            "Epoch 0.8 >> (class accuracy): [97.2449, 98.4141, 86.6279, 91.0891, 93.6864, 89.9103, 93.215, 90.6615, 89.2197, 89.9901]\n",
            "Epoch 1.0 >> (per-task accuracy): 93.53\n",
            "Epoch 1.0 >> (class accuracy): [97.8571, 98.7665, 90.2132, 92.6733, 93.9919, 91.2556, 94.8852, 92.2179, 90.6571, 92.0714]\n",
            "OCS >> Task 1: {'accuracy': 79.99, 'per_class_accuracy': [95.9184, 97.7974, 67.4419, 78.0198, 88.7984, 57.8475, 88.7265, 80.4475, 69.0965, 72.0515], 'loss': 0.6494822021007538}\n",
            "OCS >> Task 2: {'accuracy': 87.14, 'per_class_accuracy': [96.4286, 96.4758, 82.2674, 86.1386, 91.7515, 71.5247, 91.4405, 88.9105, 84.8049, 79.2864], 'loss': 0.40889125084877015}\n",
            "OCS >> Task 3: {'accuracy': 91.76, 'per_class_accuracy': [97.449, 97.7093, 89.0504, 89.703, 92.057, 85.2018, 94.1545, 93.3852, 87.7823, 89.7919], 'loss': 0.2880622131347656}\n",
            "OCS >> Task 4: {'accuracy': 93.53, 'per_class_accuracy': [97.8571, 98.7665, 90.2132, 92.6733, 93.9919, 91.2556, 94.8852, 92.2179, 90.6571, 92.0714], 'loss': 0.24580444024801254}\n",
            "OCS >> (average accuracy): 88.10499999999999\n",
            "OCS >> (Forgetting): 0.0913333333333334\n",
            "Maximum per-task accuracies: [95.43]\n",
            "\n",
            "---- Task 5 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 65.43\n",
            "Epoch 0.2 >> (class accuracy): [99.1837, 99.3833, 48.8372, 78.5149, 2.2403, 17.3767, 83.6117, 85.4086, 57.7002, 72.1506]\n",
            "Epoch 0.4 >> (per-task accuracy): 84.2\n",
            "Epoch 0.4 >> (class accuracy): [94.7959, 98.2379, 74.9031, 84.0594, 89.8167, 73.3184, 87.9958, 88.2296, 82.0329, 66.3033]\n",
            "Epoch 0.6 >> (per-task accuracy): 87.68\n",
            "Epoch 0.6 >> (class accuracy): [94.7959, 98.2379, 80.9109, 84.0594, 88.7984, 80.8296, 90.1879, 87.9377, 84.4969, 84.8365]\n",
            "Epoch 0.8 >> (per-task accuracy): 89.49\n",
            "Epoch 0.8 >> (class accuracy): [96.6327, 98.2379, 83.2364, 84.8515, 91.7515, 84.1928, 92.1712, 89.6887, 85.4209, 87.4133]\n",
            "Epoch 1.0 >> (per-task accuracy): 91.0\n",
            "Epoch 1.0 >> (class accuracy): [96.8367, 98.326, 84.7868, 87.7228, 92.4644, 88.1166, 92.7975, 90.0778, 88.193, 89.7919]\n",
            "OCS >> Task 1: {'accuracy': 69.16, 'per_class_accuracy': [92.9592, 92.7753, 47.9651, 59.3069, 84.2159, 42.713, 89.2484, 70.428, 52.3614, 55.5996], 'loss': 0.9725238744735718}\n",
            "OCS >> Task 2: {'accuracy': 77.89, 'per_class_accuracy': [94.1837, 87.9295, 64.8256, 67.4257, 88.1874, 61.5471, 89.7704, 81.7121, 72.3819, 69.1774], 'loss': 0.6703487967014313}\n",
            "OCS >> Task 3: {'accuracy': 84.77, 'per_class_accuracy': [94.6939, 92.1586, 78.6822, 78.3168, 87.5764, 71.1883, 90.9186, 87.7432, 83.4702, 81.1695], 'loss': 0.47664468750953676}\n",
            "OCS >> Task 4: {'accuracy': 89.83, 'per_class_accuracy': [95.8163, 97.6211, 86.4341, 84.9505, 90.4277, 85.3139, 93.4238, 89.6887, 87.0637, 86.4222], 'loss': 0.3627452903985977}\n",
            "OCS >> Task 5: {'accuracy': 91.0, 'per_class_accuracy': [96.8367, 98.326, 84.7868, 87.7228, 92.4644, 88.1166, 92.7975, 90.0778, 88.193, 89.7919], 'loss': 0.3285504291772842}\n",
            "OCS >> (average accuracy): 82.53\n",
            "OCS >> (Forgetting): 0.1501750000000001\n",
            "Maximum per-task accuracies: [95.43]\n",
            "\n",
            "---- Task 6 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 55.86\n",
            "Epoch 0.2 >> (class accuracy): [99.3878, 99.3833, 65.2132, 69.703, 4.277, 0.2242, 41.5449, 75.1946, 47.3306, 42.7156]\n",
            "Epoch 0.4 >> (per-task accuracy): 79.83\n",
            "Epoch 0.4 >> (class accuracy): [96.2245, 98.4141, 66.0853, 80.297, 86.7617, 53.139, 89.8747, 85.7004, 82.9569, 54.9058]\n",
            "Epoch 0.6 >> (per-task accuracy): 85.32\n",
            "Epoch 0.6 >> (class accuracy): [95.102, 98.326, 75.1938, 85.3465, 88.7984, 75.2242, 90.1879, 85.7004, 82.1355, 75.1239]\n",
            "Epoch 0.8 >> (per-task accuracy): 87.29\n",
            "Epoch 0.8 >> (class accuracy): [95.8163, 97.7093, 79.0698, 85.9406, 90.3259, 80.4933, 92.0668, 86.3813, 81.8275, 81.7641]\n",
            "Epoch 1.0 >> (per-task accuracy): 88.47\n",
            "Epoch 1.0 >> (class accuracy): [95.5102, 97.6211, 81.0078, 85.3465, 93.5845, 85.2018, 92.4843, 87.5486, 83.5729, 81.8632]\n",
            "OCS >> Task 1: {'accuracy': 61.46, 'per_class_accuracy': [91.0204, 91.63, 25.6783, 38.6139, 81.0591, 28.8117, 86.2213, 73.5409, 42.5051, 50.5451], 'loss': 1.2199039593696595}\n",
            "OCS >> Task 2: {'accuracy': 70.9, 'per_class_accuracy': [93.2653, 97.2687, 40.8915, 51.8812, 88.1874, 44.0583, 87.9958, 80.642, 60.7803, 59.7621], 'loss': 0.9221504666328431}\n",
            "OCS >> Task 3: {'accuracy': 78.38, 'per_class_accuracy': [95.0, 98.1498, 58.7209, 66.1386, 86.4562, 59.1928, 88.8309, 86.0895, 70.7392, 71.0605], 'loss': 0.7041438995361328}\n",
            "OCS >> Task 4: {'accuracy': 84.9, 'per_class_accuracy': [95.3061, 98.326, 75.6783, 78.9109, 89.1039, 73.4305, 91.2317, 86.7704, 78.8501, 79.0882], 'loss': 0.5428983778953552}\n",
            "OCS >> Task 5: {'accuracy': 88.46, 'per_class_accuracy': [95.5102, 98.5022, 82.3643, 84.0594, 91.7515, 82.5112, 92.7975, 89.786, 82.3409, 83.449], 'loss': 0.45295532598495486}\n",
            "OCS >> Task 6: {'accuracy': 88.47, 'per_class_accuracy': [95.5102, 97.6211, 81.0078, 85.3465, 93.5845, 85.2018, 92.4843, 87.5486, 83.5729, 81.8632], 'loss': 0.4393482303619385}\n",
            "OCS >> (average accuracy): 78.76166666666666\n",
            "OCS >> (Forgetting): 0.18610000000000007\n",
            "Maximum per-task accuracies: [95.43]\n",
            "\n",
            "---- Task 7 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 45.03\n",
            "Epoch 0.2 >> (class accuracy): [98.9796, 98.9427, 71.0271, 80.7921, 0.0, 0.0, 0.0, 42.3152, 0.0, 42.22]\n",
            "Epoch 0.4 >> (per-task accuracy): 74.65\n",
            "Epoch 0.4 >> (class accuracy): [97.2449, 98.9427, 59.7868, 84.3564, 53.2587, 37.8924, 87.1608, 87.8405, 67.7618, 65.5104]\n",
            "Epoch 0.6 >> (per-task accuracy): 82.88\n",
            "Epoch 0.6 >> (class accuracy): [95.5102, 98.1498, 74.4186, 82.1782, 81.0591, 64.574, 89.7704, 85.6031, 79.4661, 74.7275]\n",
            "Epoch 0.8 >> (per-task accuracy): 85.15\n",
            "Epoch 0.8 >> (class accuracy): [95.2041, 97.9736, 76.7442, 85.9406, 86.2525, 73.991, 91.4405, 88.3268, 78.5421, 74.7275]\n",
            "Epoch 1.0 >> (per-task accuracy): 86.84\n",
            "Epoch 1.0 >> (class accuracy): [95.3061, 97.7093, 79.0698, 85.3465, 89.8167, 81.0538, 92.0668, 87.5486, 80.5955, 78.3944]\n",
            "OCS >> Task 1: {'accuracy': 56.44, 'per_class_accuracy': [88.4694, 88.8106, 22.9651, 28.7129, 74.6436, 20.0673, 82.4635, 66.4397, 36.7556, 49.4549], 'loss': 1.3936269678115845}\n",
            "OCS >> Task 2: {'accuracy': 64.93, 'per_class_accuracy': [91.3265, 96.1233, 34.3023, 42.0792, 78.7169, 31.1659, 85.6994, 76.4591, 47.8439, 59.8612], 'loss': 1.1223404473304748}\n",
            "OCS >> Task 3: {'accuracy': 73.02, 'per_class_accuracy': [94.2857, 97.8855, 52.6163, 57.7228, 80.9572, 44.6188, 86.8476, 83.2685, 56.2628, 70.5649], 'loss': 0.8864900359153748}\n",
            "OCS >> Task 4: {'accuracy': 79.55, 'per_class_accuracy': [94.3878, 98.1498, 69.186, 73.3663, 84.5214, 58.1839, 89.2484, 83.2685, 65.5031, 75.6194], 'loss': 0.7129100975990296}\n",
            "OCS >> Task 5: {'accuracy': 84.89, 'per_class_accuracy': [95.3061, 98.2379, 77.4225, 82.8713, 86.558, 72.87, 90.3967, 87.1595, 74.846, 80.5748], 'loss': 0.5762920872688293}\n",
            "OCS >> Task 6: {'accuracy': 86.32, 'per_class_accuracy': [95.102, 97.9736, 78.9729, 86.9307, 89.002, 77.13, 92.38, 86.6732, 79.2608, 77.7998], 'loss': 0.5261899347305298}\n",
            "OCS >> Task 7: {'accuracy': 86.84, 'per_class_accuracy': [95.3061, 97.7093, 79.0698, 85.3465, 89.8167, 81.0538, 92.0668, 87.5486, 80.5955, 78.3944], 'loss': 0.5107295736789703}\n",
            "OCS >> (average accuracy): 75.99857142857142\n",
            "OCS >> (Forgetting): 0.2123833333333334\n",
            "Maximum per-task accuracies: [95.43]\n",
            "\n",
            "---- Task 8 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 31.93\n",
            "Epoch 0.2 >> (class accuracy): [99.0816, 99.2952, 49.5155, 57.7228, 0.0, 0.0, 0.0, 0.0, 0.1027, 0.0]\n",
            "Epoch 0.4 >> (per-task accuracy): 55.82\n",
            "Epoch 0.4 >> (class accuracy): [97.449, 98.5903, 51.4535, 88.0198, 0.1018, 13.6771, 77.5574, 82.1984, 33.4702, 5.0545]\n",
            "Epoch 0.6 >> (per-task accuracy): 78.26\n",
            "Epoch 0.6 >> (class accuracy): [95.7143, 97.533, 71.5116, 80.099, 83.5031, 56.278, 89.666, 87.6459, 74.538, 42.22]\n",
            "Epoch 0.8 >> (per-task accuracy): 82.72\n",
            "Epoch 0.8 >> (class accuracy): [95.7143, 97.0925, 73.9341, 82.4752, 83.8086, 68.6099, 88.6221, 85.0195, 80.2875, 68.9792]\n",
            "Epoch 1.0 >> (per-task accuracy): 84.43\n",
            "Epoch 1.0 >> (class accuracy): [94.6939, 96.652, 77.4225, 84.0594, 85.4379, 71.861, 89.7704, 87.9377, 81.1088, 72.9435]\n",
            "OCS >> Task 1: {'accuracy': 55.15, 'per_class_accuracy': [86.7347, 98.6784, 26.938, 30.198, 62.7291, 6.278, 82.7766, 55.642, 36.2423, 56.6898], 'loss': 1.409297843360901}\n",
            "OCS >> Task 2: {'accuracy': 62.14, 'per_class_accuracy': [90.102, 99.0308, 40.9884, 46.7327, 65.0713, 11.9955, 84.5511, 69.4553, 41.4784, 63.2309], 'loss': 1.1973902276992798}\n",
            "OCS >> Task 3: {'accuracy': 67.7, 'per_class_accuracy': [92.449, 99.0308, 51.8411, 59.901, 69.7556, 19.5067, 87.0564, 75.8755, 46.6119, 66.6997], 'loss': 1.0346885189056396}\n",
            "OCS >> Task 4: {'accuracy': 73.57, 'per_class_accuracy': [93.8776, 98.6784, 65.6008, 73.3663, 78.0041, 31.9507, 87.2651, 78.6965, 53.6961, 67.3935], 'loss': 0.8861349393844604}\n",
            "OCS >> Task 5: {'accuracy': 79.0, 'per_class_accuracy': [94.6939, 98.5903, 72.8682, 83.9604, 81.8737, 49.2152, 84.6555, 82.393, 63.963, 72.2498], 'loss': 0.748998646736145}\n",
            "OCS >> Task 6: {'accuracy': 81.93, 'per_class_accuracy': [95.3061, 98.4141, 77.1318, 88.3168, 83.4012, 58.296, 86.952, 83.3658, 73.4086, 70.3667], 'loss': 0.6704873920440674}\n",
            "OCS >> Task 7: {'accuracy': 84.8, 'per_class_accuracy': [94.7959, 98.4141, 80.7171, 87.0297, 84.0122, 67.9372, 89.8747, 87.7432, 77.6181, 76.5114], 'loss': 0.6112531321525574}\n",
            "OCS >> Task 8: {'accuracy': 84.43, 'per_class_accuracy': [94.6939, 96.652, 77.4225, 84.0594, 85.4379, 71.861, 89.7704, 87.9377, 81.1088, 72.9435], 'loss': 0.6097472968578339}\n",
            "OCS >> (average accuracy): 73.59\n",
            "OCS >> (Forgetting): 0.2338857142857143\n",
            "Maximum per-task accuracies: [95.43]\n",
            "\n",
            "---- Task 9 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 26.24\n",
            "Epoch 0.2 >> (class accuracy): [0.0, 97.6211, 0.7752, 0.0, 0.0, 0.0, 0.0, 73.5409, 76.7967, 0.3964]\n",
            "Epoch 0.4 >> (per-task accuracy): 49.32\n",
            "Epoch 0.4 >> (class accuracy): [97.6531, 99.207, 37.2093, 73.9604, 1.3238, 0.4484, 35.0731, 86.1868, 47.4333, 1.6848]\n",
            "Epoch 0.6 >> (per-task accuracy): 74.01\n",
            "Epoch 0.6 >> (class accuracy): [95.0, 97.1806, 62.7907, 80.198, 65.6823, 34.0807, 81.7328, 89.6887, 67.5565, 59.1675]\n",
            "Epoch 0.8 >> (per-task accuracy): 79.66\n",
            "Epoch 0.8 >> (class accuracy): [96.0204, 96.4758, 69.6705, 80.0, 71.4868, 52.0179, 87.5783, 82.0039, 80.8008, 76.115]\n",
            "Epoch 1.0 >> (per-task accuracy): 82.74\n",
            "Epoch 1.0 >> (class accuracy): [95.0, 97.0044, 79.2636, 80.6931, 79.9389, 67.0404, 87.3695, 84.3385, 79.3634, 74.2319]\n",
            "OCS >> Task 1: {'accuracy': 45.56, 'per_class_accuracy': [79.2857, 66.5198, 19.0891, 13.0693, 52.1385, 3.139, 72.7557, 48.4436, 32.5462, 63.7265], 'loss': 1.7860066514968873}\n",
            "OCS >> Task 2: {'accuracy': 51.33, 'per_class_accuracy': [84.6939, 68.5463, 25.8721, 21.1881, 53.3605, 5.157, 79.7495, 63.4241, 44.7639, 61.6452], 'loss': 1.5767657247543334}\n",
            "OCS >> Task 3: {'accuracy': 57.06, 'per_class_accuracy': [89.2857, 85.2863, 39.1473, 35.1485, 50.5092, 8.6323, 82.881, 68.677, 46.2012, 57.5818], 'loss': 1.3308348510742187}\n",
            "OCS >> Task 4: {'accuracy': 62.29, 'per_class_accuracy': [92.3469, 91.7181, 54.2636, 54.8515, 53.666, 16.0314, 82.1503, 68.2879, 51.3347, 50.5451], 'loss': 1.1493993921279908}\n",
            "OCS >> Task 5: {'accuracy': 69.27, 'per_class_accuracy': [93.8776, 96.5639, 64.2442, 71.0891, 60.5906, 28.139, 81.524, 76.5564, 59.8563, 52.8246], 'loss': 0.948730864238739}\n",
            "OCS >> Task 6: {'accuracy': 74.03, 'per_class_accuracy': [95.5102, 97.6211, 68.6047, 82.4752, 64.1548, 38.4529, 80.5846, 77.5292, 68.4805, 60.3568], 'loss': 0.825768070602417}\n",
            "OCS >> Task 7: {'accuracy': 79.18, 'per_class_accuracy': [95.5102, 98.2379, 76.4535, 85.5446, 70.4684, 52.8027, 84.3424, 81.9066, 75.154, 66.3033], 'loss': 0.7186597489356995}\n",
            "OCS >> Task 8: {'accuracy': 82.71, 'per_class_accuracy': [95.3061, 97.2687, 80.2326, 83.5644, 78.4114, 67.6009, 87.5783, 83.0739, 79.3634, 71.556], 'loss': 0.6591906804561615}\n",
            "OCS >> Task 9: {'accuracy': 82.74, 'per_class_accuracy': [95.0, 97.0044, 79.2636, 80.6931, 79.9389, 67.0404, 87.3695, 84.3385, 79.3634, 74.2319], 'loss': 0.646063534784317}\n",
            "OCS >> (average accuracy): 67.13000000000001\n",
            "OCS >> (Forgetting): 0.3025125000000001\n",
            "Maximum per-task accuracies: [95.43]\n",
            "\n",
            "---- Task 10 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 26.01\n",
            "Epoch 0.2 >> (class accuracy): [0.0, 97.4449, 0.0, 39.703, 0.0, 0.0, 23.0689, 3.5992, 1.232, 81.665]\n",
            "Epoch 0.4 >> (per-task accuracy): 62.53\n",
            "Epoch 0.4 >> (class accuracy): [81.5306, 92.1586, 43.7984, 74.4554, 34.8269, 32.287, 85.2818, 57.393, 58.4189, 59.2666]\n",
            "Epoch 0.6 >> (per-task accuracy): 71.26\n",
            "Epoch 0.6 >> (class accuracy): [90.102, 93.3921, 68.0233, 72.2772, 55.2953, 44.0583, 83.7161, 82.7821, 70.1232, 47.4727]\n",
            "Epoch 0.8 >> (per-task accuracy): 78.66\n",
            "Epoch 0.8 >> (class accuracy): [93.4694, 96.1233, 66.8605, 77.1287, 69.6538, 59.417, 90.8142, 86.0895, 81.1088, 62.5372]\n",
            "Epoch 1.0 >> (per-task accuracy): 81.77\n",
            "Epoch 1.0 >> (class accuracy): [92.8571, 96.5639, 73.7403, 83.1683, 69.6538, 64.0135, 88.5177, 85.3113, 84.5996, 75.9167]\n",
            "OCS >> Task 1: {'accuracy': 36.53, 'per_class_accuracy': [71.2245, 3.7885, 3.4884, 27.0297, 65.2749, 2.2422, 64.6138, 21.4981, 47.9466, 62.9336], 'loss': 2.2707693691253663}\n",
            "OCS >> Task 2: {'accuracy': 41.61, 'per_class_accuracy': [82.0408, 18.7665, 6.5891, 30.099, 65.8859, 3.8117, 65.0313, 34.0467, 57.5975, 55.2032], 'loss': 2.023999747276306}\n",
            "OCS >> Task 3: {'accuracy': 47.77, 'per_class_accuracy': [88.3673, 40.4405, 10.9496, 32.3762, 56.8228, 8.296, 68.3716, 48.6381, 66.0164, 57.6809], 'loss': 1.7276185651779175}\n",
            "OCS >> Task 4: {'accuracy': 51.79, 'per_class_accuracy': [89.4898, 56.8282, 19.0891, 43.6634, 46.945, 16.1435, 66.2839, 58.9494, 68.7885, 49.8513], 'loss': 1.5139113832473754}\n",
            "OCS >> Task 5: {'accuracy': 56.46, 'per_class_accuracy': [91.0204, 73.304, 23.7403, 56.1386, 41.446, 24.1031, 67.3278, 72.2763, 70.4312, 41.0307], 'loss': 1.2877601638793945}\n",
            "OCS >> Task 6: {'accuracy': 61.31, 'per_class_accuracy': [90.0, 85.4626, 28.876, 71.4851, 39.1039, 31.3901, 69.833, 75.4864, 75.0513, 41.5263], 'loss': 1.1223817510604859}\n",
            "OCS >> Task 7: {'accuracy': 68.84, 'per_class_accuracy': [92.551, 93.6564, 44.7674, 76.2376, 42.1589, 39.0135, 78.2881, 80.3502, 80.2875, 55.6987], 'loss': 0.9188637442588806}\n",
            "OCS >> Task 8: {'accuracy': 76.72, 'per_class_accuracy': [92.551, 93.4802, 61.4341, 81.5842, 59.4705, 56.7265, 85.595, 82.8794, 83.7782, 66.2042], 'loss': 0.7464422733306885}\n",
            "OCS >> Task 9: {'accuracy': 81.33, 'per_class_accuracy': [92.449, 95.9471, 71.7054, 83.1683, 67.5153, 59.7534, 89.2484, 88.2296, 85.0103, 76.5114], 'loss': 0.6376000386238098}\n",
            "OCS >> Task 10: {'accuracy': 81.77, 'per_class_accuracy': [92.8571, 96.5639, 73.7403, 83.1683, 69.6538, 64.0135, 88.5177, 85.3113, 84.5996, 75.9167], 'loss': 0.6092396818161011}\n",
            "OCS >> (average accuracy): 60.413\n",
            "OCS >> (Forgetting): 0.3739000000000001\n",
            "Maximum per-task accuracies: [95.43]\n",
            "\n",
            "---- Task 11 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 35.42\n",
            "Epoch 0.2 >> (class accuracy): [54.5918, 98.5903, 85.9496, 0.0, 0.0, 0.0, 0.0, 29.8638, 0.0, 68.781]\n",
            "Epoch 0.4 >> (per-task accuracy): 19.97\n",
            "Epoch 0.4 >> (class accuracy): [100.0, 88.2819, 1.0659, 0.0, 0.0, 0.0, 0.0, 0.3891, 0.0, 0.0]\n",
            "Epoch 0.6 >> (per-task accuracy): 37.61\n",
            "Epoch 0.6 >> (class accuracy): [99.6939, 99.3833, 41.2791, 24.7525, 0.0, 0.0, 24.6347, 60.6031, 9.8563, 2.4777]\n",
            "Epoch 0.8 >> (per-task accuracy): 52.82\n",
            "Epoch 0.8 >> (class accuracy): [98.0612, 99.207, 55.4264, 70.7921, 0.0, 0.2242, 64.0919, 84.8249, 35.2156, 7.6313]\n",
            "Epoch 1.0 >> (per-task accuracy): 61.23\n",
            "Epoch 1.0 >> (class accuracy): [97.1429, 99.0308, 54.7481, 81.4851, 0.1018, 18.3857, 81.9415, 88.1323, 52.9774, 28.444]\n",
            "OCS >> Task 1: {'accuracy': 21.98, 'per_class_accuracy': [88.8776, 25.7269, 1.0659, 7.2277, 0.2037, 0.1121, 54.5929, 16.7315, 15.2977, 10.3072], 'loss': 2.0781232917785646}\n",
            "OCS >> Task 2: {'accuracy': 24.03, 'per_class_accuracy': [88.6735, 33.0396, 1.938, 10.495, 0.611, 0.5605, 55.3236, 25.5837, 15.8111, 7.4331], 'loss': 2.0648317947387693}\n",
            "OCS >> Task 3: {'accuracy': 28.69, 'per_class_accuracy': [93.0612, 52.2467, 4.6512, 19.2079, 0.4073, 0.5605, 56.3674, 26.2646, 23.4086, 7.4331], 'loss': 2.028631118774414}\n",
            "OCS >> Task 4: {'accuracy': 29.37, 'per_class_accuracy': [94.2857, 50.0441, 6.686, 29.4059, 0.4073, 0.8969, 55.6367, 27.1401, 20.6366, 5.3518], 'loss': 2.0023571662902833}\n",
            "OCS >> Task 5: {'accuracy': 34.96, 'per_class_accuracy': [95.7143, 75.6828, 13.0814, 40.9901, 0.3055, 1.6816, 60.2296, 31.0311, 20.9446, 3.1715], 'loss': 1.9395621719360352}\n",
            "OCS >> Task 6: {'accuracy': 39.59, 'per_class_accuracy': [96.3265, 82.9956, 21.8023, 53.3663, 0.1018, 2.3543, 60.6472, 40.6615, 24.1273, 5.2527], 'loss': 1.865327713394165}\n",
            "OCS >> Task 7: {'accuracy': 44.86, 'per_class_accuracy': [97.551, 85.9031, 35.0775, 67.1287, 0.1018, 5.7175, 62.8392, 55.1556, 28.4394, 1.6848], 'loss': 1.782238451385498}\n",
            "OCS >> Task 8: {'accuracy': 51.65, 'per_class_accuracy': [97.9592, 93.0396, 52.1318, 74.2574, 0.1018, 8.6323, 68.476, 69.2607, 40.5544, 1.9822], 'loss': 1.6866419723510742}\n",
            "OCS >> Task 9: {'accuracy': 56.52, 'per_class_accuracy': [98.0612, 97.533, 60.1744, 78.7129, 0.4073, 14.574, 71.9207, 79.1829, 50.8214, 3.5679], 'loss': 1.5938215085983276}\n",
            "OCS >> Task 10: {'accuracy': 58.72, 'per_class_accuracy': [97.551, 98.8546, 59.593, 81.7822, 0.611, 14.3498, 76.618, 84.0467, 52.6694, 10.7037], 'loss': 1.5355788635253906}\n",
            "OCS >> Task 11: {'accuracy': 61.23, 'per_class_accuracy': [97.1429, 99.0308, 54.7481, 81.4851, 0.1018, 18.3857, 81.9415, 88.1323, 52.9774, 28.444], 'loss': 1.520050199508667}\n",
            "OCS >> (average accuracy): 41.054545454545455\n",
            "OCS >> (Forgetting): 0.56393\n",
            "Maximum per-task accuracies: [95.43]\n",
            "\n",
            "---- Task 12 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 31.32\n",
            "Epoch 0.2 >> (class accuracy): [16.7347, 98.4141, 83.3333, 0.0, 0.1018, 0.0, 0.0, 34.9222, 0.0, 62.5372]\n",
            "Epoch 0.4 >> (per-task accuracy): 13.87\n",
            "Epoch 0.4 >> (class accuracy): [100.0, 35.859, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.6 >> (per-task accuracy): 22.17\n",
            "Epoch 0.6 >> (class accuracy): [99.898, 97.0044, 3.5853, 0.099, 0.0, 0.0, 0.1044, 9.5331, 0.0, 0.0]\n",
            "Epoch 0.8 >> (per-task accuracy): 38.29\n",
            "Epoch 0.8 >> (class accuracy): [99.5918, 99.2952, 38.5659, 28.5149, 0.0, 0.0, 18.1628, 64.3969, 20.1232, 0.7929]\n",
            "Epoch 1.0 >> (per-task accuracy): 50.94\n",
            "Epoch 1.0 >> (class accuracy): [98.5714, 99.207, 58.7209, 63.8614, 0.0, 0.0, 50.5219, 83.3658, 39.7331, 2.2795]\n",
            "OCS >> Task 1: {'accuracy': 26.49, 'per_class_accuracy': [97.0408, 81.9383, 4.4574, 3.4653, 0.0, 0.0, 27.8706, 32.9767, 8.3162, 0.0], 'loss': 2.1450320625305177}\n",
            "OCS >> Task 2: {'accuracy': 26.86, 'per_class_accuracy': [96.9388, 82.6432, 3.4884, 3.3663, 0.0, 0.0, 27.1399, 35.0195, 11.0883, 0.0], 'loss': 2.1394012130737305}\n",
            "OCS >> Task 3: {'accuracy': 27.73, 'per_class_accuracy': [98.0612, 88.5463, 3.5853, 7.2277, 0.0, 0.0, 24.6347, 29.4747, 16.2218, 0.0], 'loss': 2.1287759338378907}\n",
            "OCS >> Task 4: {'accuracy': 27.78, 'per_class_accuracy': [98.3673, 90.9251, 4.0698, 12.5743, 0.0, 0.0, 20.1461, 26.5564, 15.0924, 0.0], 'loss': 2.1223356914520264}\n",
            "OCS >> Task 5: {'accuracy': 29.17, 'per_class_accuracy': [98.8776, 96.1233, 6.2984, 18.9109, 0.0, 0.0, 21.0856, 27.3346, 12.115, 0.0], 'loss': 2.101195698928833}\n",
            "OCS >> Task 6: {'accuracy': 30.87, 'per_class_accuracy': [98.6735, 97.7093, 11.1434, 26.8317, 0.0, 0.0, 19.1023, 30.4475, 13.2444, 0.0], 'loss': 2.0731981288909913}\n",
            "OCS >> Task 7: {'accuracy': 33.82, 'per_class_accuracy': [98.9796, 98.5903, 23.6434, 35.0495, 0.0, 0.0, 19.5198, 37.5486, 12.5257, 0.0], 'loss': 2.039123385620117}\n",
            "OCS >> Task 8: {'accuracy': 39.26, 'per_class_accuracy': [98.9796, 99.207, 39.5349, 47.5248, 0.0, 0.0, 23.0689, 50.3891, 20.8419, 0.0], 'loss': 1.9923081607818605}\n",
            "OCS >> Task 9: {'accuracy': 43.54, 'per_class_accuracy': [99.0816, 99.4714, 50.969, 52.2772, 0.0, 0.0, 24.8434, 63.6187, 31.6222, 0.0], 'loss': 1.9379207824707032}\n",
            "OCS >> Task 10: {'accuracy': 47.7, 'per_class_accuracy': [98.5714, 99.4714, 59.3992, 59.901, 0.0, 0.0, 37.3695, 72.7626, 36.037, 0.0], 'loss': 1.8923093000411988}\n",
            "OCS >> Task 11: {'accuracy': 50.03, 'per_class_accuracy': [98.5714, 99.3833, 60.2713, 62.2772, 0.0, 0.0, 44.6764, 79.572, 41.4784, 0.7929], 'loss': 1.8581756786346435}\n",
            "OCS >> Task 12: {'accuracy': 50.94, 'per_class_accuracy': [98.5714, 99.207, 58.7209, 63.8614, 0.0, 0.0, 50.5219, 83.3658, 39.7331, 2.2795], 'loss': 1.85402991771698}\n",
            "OCS >> (average accuracy): 36.1825\n",
            "OCS >> (Forgetting): 0.6058909090909091\n",
            "Maximum per-task accuracies: [95.43]\n",
            "\n",
            "---- Task 13 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 23.76\n",
            "Epoch 0.2 >> (class accuracy): [0.6122, 90.7489, 83.624, 0.0, 0.0, 0.0, 0.0, 8.6576, 0.0, 38.4539]\n",
            "Epoch 0.4 >> (per-task accuracy): 10.34\n",
            "Epoch 0.4 >> (class accuracy): [100.0, 4.7577, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.6 >> (per-task accuracy): 15.72\n",
            "Epoch 0.6 >> (class accuracy): [100.0, 52.1586, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.8 >> (per-task accuracy): 22.4\n",
            "Epoch 0.8 >> (class accuracy): [99.898, 98.5903, 3.5853, 0.099, 0.0, 0.0, 1.357, 8.8521, 0.0, 0.0]\n",
            "Epoch 1.0 >> (per-task accuracy): 34.8\n",
            "Epoch 1.0 >> (class accuracy): [99.5918, 99.207, 29.1667, 18.1188, 0.0, 0.0, 19.9374, 49.4163, 14.8871, 4.9554]\n",
            "OCS >> Task 1: {'accuracy': 21.73, 'per_class_accuracy': [99.3878, 96.0352, 1.2597, 0.099, 0.0, 0.0, 4.8017, 4.2802, 0.5133, 0.0], 'loss': 2.1987638233184814}\n",
            "OCS >> Task 2: {'accuracy': 22.04, 'per_class_accuracy': [99.3878, 96.0352, 1.5504, 0.099, 0.0, 0.0, 5.7411, 5.2529, 1.4374, 0.0], 'loss': 2.19439755859375}\n",
            "OCS >> Task 3: {'accuracy': 22.12, 'per_class_accuracy': [99.3878, 97.7093, 1.1628, 0.396, 0.0, 0.0, 6.1587, 4.572, 0.7187, 0.0], 'loss': 2.191248865509033}\n",
            "OCS >> Task 4: {'accuracy': 22.05, 'per_class_accuracy': [99.4898, 97.6211, 2.2287, 0.9901, 0.0, 0.0, 4.5929, 3.5019, 0.924, 0.0], 'loss': 2.1877670959472657}\n",
            "OCS >> Task 5: {'accuracy': 22.48, 'per_class_accuracy': [99.6939, 98.8546, 1.6473, 2.6733, 0.0, 0.0, 5.2192, 5.0584, 0.308, 0.0], 'loss': 2.1799204277038573}\n",
            "OCS >> Task 6: {'accuracy': 22.92, 'per_class_accuracy': [99.6939, 98.9427, 2.8101, 4.9505, 0.0, 0.0, 2.1921, 8.8521, 0.1027, 0.0], 'loss': 2.1686429512023926}\n",
            "OCS >> Task 7: {'accuracy': 23.54, 'per_class_accuracy': [99.7959, 99.3833, 4.5543, 7.1287, 0.0, 0.0, 1.9833, 10.1167, 0.616, 0.0], 'loss': 2.15422061920166}\n",
            "OCS >> Task 8: {'accuracy': 24.93, 'per_class_accuracy': [99.7959, 99.4714, 10.9496, 9.703, 0.0, 0.0, 2.5052, 13.6187, 1.1294, 0.0], 'loss': 2.1329100494384767}\n",
            "OCS >> Task 9: {'accuracy': 26.82, 'per_class_accuracy': [99.7959, 99.6476, 18.2171, 12.3762, 0.0, 0.0, 3.5491, 20.1362, 1.9507, 0.0], 'loss': 2.1049524215698243}\n",
            "OCS >> Task 10: {'accuracy': 30.12, 'per_class_accuracy': [99.5918, 99.5595, 24.9031, 14.2574, 0.0, 0.0, 8.4551, 35.8949, 4.7228, 0.892], 'loss': 2.0766326095581054}\n",
            "OCS >> Task 11: {'accuracy': 32.97, 'per_class_accuracy': [99.7959, 99.4714, 31.0078, 16.0396, 0.0, 0.0, 11.691, 46.2062, 9.0349, 3.2706], 'loss': 2.051878046798706}\n",
            "OCS >> Task 12: {'accuracy': 34.71, 'per_class_accuracy': [99.6939, 99.4714, 36.2403, 18.5149, 0.0, 0.0, 15.762, 45.5253, 13.347, 5.4509], 'loss': 2.0442910797119143}\n",
            "OCS >> Task 13: {'accuracy': 34.8, 'per_class_accuracy': [99.5918, 99.207, 29.1667, 18.1188, 0.0, 0.0, 19.9374, 49.4163, 14.8871, 4.9554], 'loss': 2.041573394012451}\n",
            "OCS >> (average accuracy): 26.24846153846154\n",
            "OCS >> (Forgetting): 0.6989416666666669\n",
            "Maximum per-task accuracies: [95.43]\n",
            "\n",
            "---- Task 14 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 20.76\n",
            "Epoch 0.2 >> (class accuracy): [0.9184, 88.1938, 73.7403, 0.0, 0.0, 0.0, 0.0, 0.3891, 0.0, 29.8315]\n",
            "Epoch 0.4 >> (per-task accuracy): 10.36\n",
            "Epoch 0.4 >> (class accuracy): [99.898, 4.5815, 0.4845, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.6 >> (per-task accuracy): 9.8\n",
            "Epoch 0.6 >> (class accuracy): [100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.8 >> (per-task accuracy): 12.93\n",
            "Epoch 0.8 >> (class accuracy): [100.0, 27.5771, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 1.0 >> (per-task accuracy): 21.14\n",
            "Epoch 1.0 >> (class accuracy): [99.898, 97.0044, 1.4535, 0.0, 0.0, 0.0, 1.5658, 0.3891, 0.0, 0.0]\n",
            "OCS >> Task 1: {'accuracy': 19.28, 'per_class_accuracy': [99.898, 83.6123, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.235880211639404}\n",
            "OCS >> Task 2: {'accuracy': 18.68, 'per_class_accuracy': [100.0, 78.2379, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.233559066772461}\n",
            "OCS >> Task 3: {'accuracy': 18.53, 'per_class_accuracy': [99.898, 76.9163, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0973, 0.0, 0.0], 'loss': 2.233696319961548}\n",
            "OCS >> Task 4: {'accuracy': 17.14, 'per_class_accuracy': [99.898, 64.6696, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0973, 0.0, 0.0], 'loss': 2.23066040763855}\n",
            "OCS >> Task 5: {'accuracy': 18.57, 'per_class_accuracy': [99.898, 77.1806, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1946, 0.0, 0.0], 'loss': 2.2281360900878906}\n",
            "OCS >> Task 6: {'accuracy': 18.73, 'per_class_accuracy': [99.898, 78.7665, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.222700563430786}\n",
            "OCS >> Task 7: {'accuracy': 18.77, 'per_class_accuracy': [99.898, 79.1189, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2171742557525635}\n",
            "OCS >> Task 8: {'accuracy': 19.43, 'per_class_accuracy': [99.898, 84.9339, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2066739372253417}\n",
            "OCS >> Task 9: {'accuracy': 20.69, 'per_class_accuracy': [100.0, 95.5947, 0.3876, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.191706631851196}\n",
            "OCS >> Task 10: {'accuracy': 20.65, 'per_class_accuracy': [99.898, 95.1542, 0.0, 0.0, 0.0, 0.0, 0.1044, 0.4864, 0.0, 0.0], 'loss': 2.1770622127532957}\n",
            "OCS >> Task 11: {'accuracy': 21.18, 'per_class_accuracy': [99.898, 98.6784, 0.0969, 0.0, 0.0, 0.0, 0.1044, 1.6537, 0.0, 0.0], 'loss': 2.1611694530487062}\n",
            "OCS >> Task 12: {'accuracy': 21.1, 'per_class_accuracy': [100.0, 98.5022, 0.6783, 0.0, 0.0, 0.0, 0.2088, 0.2918, 0.0, 0.0], 'loss': 2.1521215656280517}\n",
            "OCS >> Task 13: {'accuracy': 21.09, 'per_class_accuracy': [99.898, 98.326, 0.3876, 0.0, 0.0, 0.0, 0.3132, 0.6809, 0.0, 0.0], 'loss': 2.1456002361297606}\n",
            "OCS >> Task 14: {'accuracy': 21.14, 'per_class_accuracy': [99.898, 97.0044, 1.4535, 0.0, 0.0, 0.0, 1.5658, 0.3891, 0.0, 0.0], 'loss': 2.144964628601074}\n",
            "OCS >> (average accuracy): 19.641428571428573\n",
            "OCS >> (Forgetting): 0.7590384615384616\n",
            "Maximum per-task accuracies: [95.43]\n",
            "\n",
            "---- Task 15 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 20.81\n",
            "Epoch 0.2 >> (class accuracy): [0.0, 91.4537, 72.9651, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.7413]\n",
            "Epoch 0.4 >> (per-task accuracy): 17.27\n",
            "Epoch 0.4 >> (class accuracy): [99.7959, 57.8855, 7.4612, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.4866]\n",
            "Epoch 0.6 >> (per-task accuracy): 9.89\n",
            "Epoch 0.6 >> (class accuracy): [100.0, 0.793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.8 >> (per-task accuracy): 10.31\n",
            "Epoch 0.8 >> (class accuracy): [100.0, 4.4934, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 1.0 >> (per-task accuracy): 16.71\n",
            "Epoch 1.0 >> (class accuracy): [100.0, 60.8811, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "OCS >> Task 1: {'accuracy': 10.6, 'per_class_accuracy': [100.0, 7.0485, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2586697315216067}\n",
            "OCS >> Task 2: {'accuracy': 10.66, 'per_class_accuracy': [100.0, 7.5771, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2568227676391603}\n",
            "OCS >> Task 3: {'accuracy': 10.99, 'per_class_accuracy': [100.0, 10.4846, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2585622409820556}\n",
            "OCS >> Task 4: {'accuracy': 10.09, 'per_class_accuracy': [100.0, 2.5551, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2565544303894045}\n",
            "OCS >> Task 5: {'accuracy': 9.94, 'per_class_accuracy': [100.0, 1.2335, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.255857749938965}\n",
            "OCS >> Task 6: {'accuracy': 10.02, 'per_class_accuracy': [100.0, 1.9383, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.253645408630371}\n",
            "OCS >> Task 7: {'accuracy': 10.86, 'per_class_accuracy': [100.0, 9.3392, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2521969181060792}\n",
            "OCS >> Task 8: {'accuracy': 10.53, 'per_class_accuracy': [100.0, 6.4317, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.246220100402832}\n",
            "OCS >> Task 9: {'accuracy': 11.52, 'per_class_accuracy': [100.0, 15.1542, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.238842244720459}\n",
            "OCS >> Task 10: {'accuracy': 10.3, 'per_class_accuracy': [100.0, 4.4053, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2319958526611328}\n",
            "OCS >> Task 11: {'accuracy': 11.52, 'per_class_accuracy': [100.0, 15.1542, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.222981604766846}\n",
            "OCS >> Task 12: {'accuracy': 12.73, 'per_class_accuracy': [100.0, 25.815, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.215777611923218}\n",
            "OCS >> Task 13: {'accuracy': 14.72, 'per_class_accuracy': [100.0, 43.348, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.207433003997803}\n",
            "OCS >> Task 14: {'accuracy': 14.32, 'per_class_accuracy': [100.0, 39.8238, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2030807929992675}\n",
            "OCS >> Task 15: {'accuracy': 16.71, 'per_class_accuracy': [100.0, 60.8811, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.1983310501098634}\n",
            "OCS >> (average accuracy): 11.700666666666667\n",
            "OCS >> (Forgetting): 0.8408714285714288\n",
            "Maximum per-task accuracies: [95.43]\n",
            "\n",
            "---- Task 16 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 18.86\n",
            "Epoch 0.2 >> (class accuracy): [0.102, 80.9692, 63.9535, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.3271]\n",
            "Epoch 0.4 >> (per-task accuracy): 21.03\n",
            "Epoch 0.4 >> (class accuracy): [96.0204, 78.5903, 14.5349, 0.0, 0.0, 0.0, 0.0, 0.4864, 0.0, 11.3974]\n",
            "Epoch 0.6 >> (per-task accuracy): 10.67\n",
            "Epoch 0.6 >> (class accuracy): [100.0, 7.6652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.8 >> (per-task accuracy): 9.8\n",
            "Epoch 0.8 >> (class accuracy): [100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 1.0 >> (per-task accuracy): 9.87\n",
            "Epoch 1.0 >> (class accuracy): [100.0, 0.6167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "OCS >> Task 1: {'accuracy': 9.82, 'per_class_accuracy': [100.0, 0.1762, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2715821792602537}\n",
            "OCS >> Task 2: {'accuracy': 9.81, 'per_class_accuracy': [100.0, 0.0881, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2703854274749755}\n",
            "OCS >> Task 3: {'accuracy': 9.8, 'per_class_accuracy': [100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.272371196746826}\n",
            "OCS >> Task 4: {'accuracy': 9.8, 'per_class_accuracy': [100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.270405237197876}\n",
            "OCS >> Task 5: {'accuracy': 9.8, 'per_class_accuracy': [100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.270603321456909}\n",
            "OCS >> Task 6: {'accuracy': 9.8, 'per_class_accuracy': [100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.269053821182251}\n",
            "OCS >> Task 7: {'accuracy': 9.8, 'per_class_accuracy': [100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2692786720275877}\n",
            "OCS >> Task 8: {'accuracy': 9.8, 'per_class_accuracy': [100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2654503219604494}\n",
            "OCS >> Task 9: {'accuracy': 9.8, 'per_class_accuracy': [100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2613209365844726}\n",
            "OCS >> Task 10: {'accuracy': 9.8, 'per_class_accuracy': [100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.257847033691406}\n",
            "OCS >> Task 11: {'accuracy': 9.8, 'per_class_accuracy': [100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2535174335479735}\n",
            "OCS >> Task 12: {'accuracy': 9.8, 'per_class_accuracy': [100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2498374523162843}\n",
            "OCS >> Task 13: {'accuracy': 9.84, 'per_class_accuracy': [100.0, 0.3524, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.244311277770996}\n",
            "OCS >> Task 14: {'accuracy': 9.86, 'per_class_accuracy': [100.0, 0.5286, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2394284732818606}\n",
            "OCS >> Task 15: {'accuracy': 9.86, 'per_class_accuracy': [100.0, 0.5286, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2345061595916746}\n",
            "OCS >> Task 16: {'accuracy': 9.87, 'per_class_accuracy': [100.0, 0.6167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.232535996246338}\n",
            "OCS >> (average accuracy): 9.81625\n",
            "OCS >> (Forgetting): 0.8561733333333332\n",
            "Maximum per-task accuracies: [95.43]\n",
            "\n",
            "---- Task 17 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 19.21\n",
            "Epoch 0.2 >> (class accuracy): [0.0, 74.6256, 67.2481, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 37.6611]\n",
            "Epoch 0.4 >> (per-task accuracy): 24.94\n",
            "Epoch 0.4 >> (class accuracy): [81.1224, 89.0749, 36.3372, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.0208]\n",
            "Epoch 0.6 >> (per-task accuracy): 18.69\n",
            "Epoch 0.6 >> (class accuracy): [99.898, 77.0044, 1.2597, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2973]\n",
            "Epoch 0.8 >> (per-task accuracy): 12.36\n",
            "Epoch 0.8 >> (class accuracy): [100.0, 22.5551, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 1.0 >> (per-task accuracy): 10.0\n",
            "Epoch 1.0 >> (class accuracy): [100.0, 1.7621, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "OCS >> Task 1: {'accuracy': 9.82, 'per_class_accuracy': [100.0, 0.1762, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2793256427764894}\n",
            "OCS >> Task 2: {'accuracy': 9.81, 'per_class_accuracy': [100.0, 0.0881, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.278958494949341}\n",
            "OCS >> Task 3: {'accuracy': 9.8, 'per_class_accuracy': [100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2809390979766846}\n",
            "OCS >> Task 4: {'accuracy': 9.8, 'per_class_accuracy': [100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2792448497772217}\n",
            "OCS >> Task 5: {'accuracy': 9.8, 'per_class_accuracy': [100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2800634735107423}\n",
            "OCS >> Task 6: {'accuracy': 9.8, 'per_class_accuracy': [100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2789703552246094}\n",
            "OCS >> Task 7: {'accuracy': 9.8, 'per_class_accuracy': [100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.279586171722412}\n",
            "OCS >> Task 8: {'accuracy': 9.8, 'per_class_accuracy': [100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2771440979003907}\n",
            "OCS >> Task 9: {'accuracy': 9.8, 'per_class_accuracy': [100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.274493981552124}\n",
            "OCS >> Task 10: {'accuracy': 9.8, 'per_class_accuracy': [100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2726133518218994}\n",
            "OCS >> Task 11: {'accuracy': 9.8, 'per_class_accuracy': [100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2707801208496092}\n",
            "OCS >> Task 12: {'accuracy': 9.8, 'per_class_accuracy': [100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2692906059265137}\n",
            "OCS >> Task 13: {'accuracy': 9.8, 'per_class_accuracy': [100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2666910816192627}\n",
            "OCS >> Task 14: {'accuracy': 9.81, 'per_class_accuracy': [100.0, 0.0881, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.262959365463257}\n",
            "OCS >> Task 15: {'accuracy': 9.8, 'per_class_accuracy': [100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.257865831756592}\n",
            "OCS >> Task 16: {'accuracy': 9.8, 'per_class_accuracy': [100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.255870506286621}\n",
            "OCS >> Task 17: {'accuracy': 10.0, 'per_class_accuracy': [100.0, 1.7621, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.249750707626343}\n",
            "OCS >> (average accuracy): 9.814117647058824\n",
            "OCS >> (Forgetting): 0.856275\n",
            "Maximum per-task accuracies: [95.43]\n",
            "\n",
            "---- Task 18 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 18.77\n",
            "Epoch 0.2 >> (class accuracy): [0.0, 79.207, 51.2597, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 44.4995]\n",
            "Epoch 0.4 >> (per-task accuracy): 23.46\n",
            "Epoch 0.4 >> (class accuracy): [35.3061, 89.9559, 53.3915, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 42.4182]\n",
            "Epoch 0.6 >> (per-task accuracy): 22.41\n",
            "Epoch 0.6 >> (class accuracy): [94.5918, 91.0132, 12.1124, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 15.4609]\n",
            "Epoch 0.8 >> (per-task accuracy): 18.7\n",
            "Epoch 0.8 >> (class accuracy): [99.6939, 77.7974, 0.969, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 1.0 >> (per-task accuracy): 14.77\n",
            "Epoch 1.0 >> (class accuracy): [100.0, 43.7885, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "OCS >> Task 1: {'accuracy': 10.74, 'per_class_accuracy': [100.0, 8.2819, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2864973209381105}\n",
            "OCS >> Task 2: {'accuracy': 10.57, 'per_class_accuracy': [100.0, 6.7841, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2864246910095214}\n",
            "OCS >> Task 3: {'accuracy': 10.84, 'per_class_accuracy': [100.0, 9.163, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.288012272644043}\n",
            "OCS >> Task 4: {'accuracy': 10.05, 'per_class_accuracy': [100.0, 2.2026, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.286501791381836}\n",
            "OCS >> Task 5: {'accuracy': 9.9, 'per_class_accuracy': [99.898, 0.9692, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2870729579925535}\n",
            "OCS >> Task 6: {'accuracy': 9.89, 'per_class_accuracy': [100.0, 0.793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.285817041015625}\n",
            "OCS >> Task 7: {'accuracy': 9.87, 'per_class_accuracy': [100.0, 0.6167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2868161281585695}\n",
            "OCS >> Task 8: {'accuracy': 9.82, 'per_class_accuracy': [100.0, 0.1762, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2841488895416258}\n",
            "OCS >> Task 9: {'accuracy': 9.86, 'per_class_accuracy': [100.0, 0.5286, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.282366912460327}\n",
            "OCS >> Task 10: {'accuracy': 9.82, 'per_class_accuracy': [100.0, 0.1762, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.280958101272583}\n",
            "OCS >> Task 11: {'accuracy': 9.82, 'per_class_accuracy': [100.0, 0.1762, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2803577941894533}\n",
            "OCS >> Task 12: {'accuracy': 9.87, 'per_class_accuracy': [100.0, 0.6167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2804372848510743}\n",
            "OCS >> Task 13: {'accuracy': 9.86, 'per_class_accuracy': [100.0, 0.5286, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2804686317443847}\n",
            "OCS >> Task 14: {'accuracy': 10.0, 'per_class_accuracy': [100.0, 1.7621, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.279183346939087}\n",
            "OCS >> Task 15: {'accuracy': 10.72, 'per_class_accuracy': [100.0, 8.1057, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.275111256790161}\n",
            "OCS >> Task 16: {'accuracy': 11.05, 'per_class_accuracy': [100.0, 11.0132, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2730062984466555}\n",
            "OCS >> Task 17: {'accuracy': 15.38, 'per_class_accuracy': [100.0, 49.163, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2674713886260984}\n",
            "OCS >> Task 18: {'accuracy': 14.77, 'per_class_accuracy': [100.0, 43.7885, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.26596932220459}\n",
            "OCS >> (average accuracy): 10.712777777777779\n",
            "OCS >> (Forgetting): 0.8495588235294117\n",
            "Maximum per-task accuracies: [95.43]\n",
            "\n",
            "---- Task 19 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 17.75\n",
            "Epoch 0.2 >> (class accuracy): [0.0, 95.3304, 37.4031, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.4262]\n",
            "Epoch 0.4 >> (per-task accuracy): 18.58\n",
            "Epoch 0.4 >> (class accuracy): [4.1837, 98.4141, 47.1899, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 21.11]\n",
            "Epoch 0.6 >> (per-task accuracy): 23.11\n",
            "Epoch 0.6 >> (class accuracy): [66.5306, 98.8546, 40.8915, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.3974]\n",
            "Epoch 0.8 >> (per-task accuracy): 22.76\n",
            "Epoch 0.8 >> (class accuracy): [96.3265, 98.4141, 18.0233, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.8741]\n",
            "Epoch 1.0 >> (per-task accuracy): 20.82\n",
            "Epoch 1.0 >> (class accuracy): [99.7959, 94.0088, 3.4884, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0991]\n",
            "OCS >> Task 1: {'accuracy': 20.58, 'per_class_accuracy': [99.2857, 93.4802, 2.3256, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2916624885559083}\n",
            "OCS >> Task 2: {'accuracy': 19.46, 'per_class_accuracy': [99.3878, 82.2026, 3.7791, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.291823458480835}\n",
            "OCS >> Task 3: {'accuracy': 17.37, 'per_class_accuracy': [98.8776, 63.7004, 4.3605, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2931956455230713}\n",
            "OCS >> Task 4: {'accuracy': 15.54, 'per_class_accuracy': [98.8776, 47.2247, 4.7481, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.291972928237915}\n",
            "OCS >> Task 5: {'accuracy': 14.33, 'per_class_accuracy': [98.2653, 36.9163, 4.9419, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2925348472595215}\n",
            "OCS >> Task 6: {'accuracy': 13.3, 'per_class_accuracy': [97.2449, 29.6035, 3.9729, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.291109427642822}\n",
            "OCS >> Task 7: {'accuracy': 13.54, 'per_class_accuracy': [95.4082, 34.4493, 2.7132, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.292121604537964}\n",
            "OCS >> Task 8: {'accuracy': 14.7, 'per_class_accuracy': [94.1837, 43.0837, 5.6202, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2895951137542725}\n",
            "OCS >> Task 9: {'accuracy': 16.3, 'per_class_accuracy': [96.7347, 55.3304, 5.2326, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.288115287399292}\n",
            "OCS >> Task 10: {'accuracy': 14.79, 'per_class_accuracy': [98.6735, 41.9383, 3.4884, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2867988945007323}\n",
            "OCS >> Task 11: {'accuracy': 15.22, 'per_class_accuracy': [99.0816, 45.1982, 3.6822, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2866196807861328}\n",
            "OCS >> Task 12: {'accuracy': 14.27, 'per_class_accuracy': [99.1837, 38.2379, 2.0349, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.287173546600342}\n",
            "OCS >> Task 13: {'accuracy': 11.91, 'per_class_accuracy': [99.5918, 17.533, 1.5504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.288302675247192}\n",
            "OCS >> Task 14: {'accuracy': 12.25, 'per_class_accuracy': [99.5918, 20.0881, 2.0349, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2883276664733887}\n",
            "OCS >> Task 15: {'accuracy': 14.23, 'per_class_accuracy': [99.898, 38.326, 0.8721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2856836902618407}\n",
            "OCS >> Task 16: {'accuracy': 16.1, 'per_class_accuracy': [100.0, 54.7137, 0.7752, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0991], 'loss': 2.2841983936309815}\n",
            "OCS >> Task 17: {'accuracy': 18.5, 'per_class_accuracy': [99.898, 75.0661, 1.8411, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2789756481170653}\n",
            "OCS >> Task 18: {'accuracy': 19.12, 'per_class_accuracy': [99.4898, 80.0881, 2.7132, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.276823119354248}\n",
            "OCS >> Task 19: {'accuracy': 20.82, 'per_class_accuracy': [99.7959, 94.0088, 3.4884, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0991], 'loss': 2.2757315227508546}\n",
            "OCS >> (average accuracy): 15.912105263157894\n",
            "OCS >> (Forgetting): 0.7979055555555558\n",
            "Maximum per-task accuracies: [95.43]\n",
            "\n",
            "---- Task 20 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 16.96\n",
            "Epoch 0.2 >> (class accuracy): [0.0, 96.8282, 24.2248, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 34.3905]\n",
            "Epoch 0.4 >> (per-task accuracy): 17.42\n",
            "Epoch 0.4 >> (class accuracy): [0.5102, 98.6784, 29.7481, 0.0, 0.0, 0.0, 0.0, 0.0973, 0.0, 30.6244]\n",
            "Epoch 0.6 >> (per-task accuracy): 19.5\n",
            "Epoch 0.6 >> (class accuracy): [16.8367, 99.207, 36.4341, 0.0, 0.0, 0.0, 0.0, 0.1946, 0.0, 27.8494]\n",
            "Epoch 0.8 >> (per-task accuracy): 23.31\n",
            "Epoch 0.8 >> (class accuracy): [61.9388, 99.6476, 35.0775, 0.0, 0.0, 0.0, 0.0, 0.1946, 0.0, 22.6957]\n",
            "Epoch 1.0 >> (per-task accuracy): 23.88\n",
            "Epoch 1.0 >> (class accuracy): [90.6122, 99.7357, 26.4535, 0.0, 0.0, 0.0, 0.0, 0.0973, 0.0, 9.3162]\n",
            "OCS >> Task 1: {'accuracy': 21.74, 'per_class_accuracy': [80.8163, 96.2996, 28.0039, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2953522312164307}\n",
            "OCS >> Task 2: {'accuracy': 21.02, 'per_class_accuracy': [73.5714, 87.0485, 38.0814, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2955819366455077}\n",
            "OCS >> Task 3: {'accuracy': 19.73, 'per_class_accuracy': [75.4082, 72.511, 39.7287, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0991], 'loss': 2.2966710552215575}\n",
            "OCS >> Task 4: {'accuracy': 18.53, 'per_class_accuracy': [71.5306, 66.9604, 37.9845, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.295524128341675}\n",
            "OCS >> Task 5: {'accuracy': 18.17, 'per_class_accuracy': [61.9388, 72.2467, 37.6938, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0991], 'loss': 2.2958852760314943}\n",
            "OCS >> Task 6: {'accuracy': 17.45, 'per_class_accuracy': [56.3265, 61.674, 47.7713, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.294362868118286}\n",
            "OCS >> Task 7: {'accuracy': 17.96, 'per_class_accuracy': [46.7347, 73.8326, 48.2558, 0.0, 0.0, 0.0, 0.0, 0.1946, 0.0, 0.0], 'loss': 2.295340634536743}\n",
            "OCS >> Task 8: {'accuracy': 20.41, 'per_class_accuracy': [51.8367, 81.3216, 58.9147, 0.0, 0.0, 0.0, 0.0, 0.1946, 0.0, 0.0], 'loss': 2.292978984069824}\n",
            "OCS >> Task 9: {'accuracy': 22.53, 'per_class_accuracy': [63.4694, 88.8106, 60.2713, 0.0, 0.0, 0.0, 0.0, 0.0973, 0.0, 0.0], 'loss': 2.2917066467285157}\n",
            "OCS >> Task 10: {'accuracy': 21.38, 'per_class_accuracy': [69.6939, 77.8855, 54.9419, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3964], 'loss': 2.290337963104248}\n",
            "OCS >> Task 11: {'accuracy': 21.57, 'per_class_accuracy': [72.6531, 80.1762, 51.7442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0991], 'loss': 2.2904906360626223}\n",
            "OCS >> Task 12: {'accuracy': 20.36, 'per_class_accuracy': [77.2449, 71.8943, 44.6705, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1982], 'loss': 2.291339404296875}\n",
            "OCS >> Task 13: {'accuracy': 17.78, 'per_class_accuracy': [82.1429, 58.7665, 29.6512, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.293124417114258}\n",
            "OCS >> Task 14: {'accuracy': 16.55, 'per_class_accuracy': [86.2245, 41.674, 32.4612, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1982], 'loss': 2.2939397529602052}\n",
            "OCS >> Task 15: {'accuracy': 17.81, 'per_class_accuracy': [86.4286, 57.3568, 25.969, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.4866], 'loss': 2.2923736873626708}\n",
            "OCS >> Task 16: {'accuracy': 18.57, 'per_class_accuracy': [88.7755, 71.7181, 13.469, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.3697], 'loss': 2.2916666858673094}\n",
            "OCS >> Task 17: {'accuracy': 20.32, 'per_class_accuracy': [89.6939, 78.1498, 20.155, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.7483], 'loss': 2.2872582313537597}\n",
            "OCS >> Task 18: {'accuracy': 22.32, 'per_class_accuracy': [87.6531, 85.6388, 27.6163, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.4965], 'loss': 2.284664395904541}\n",
            "OCS >> Task 19: {'accuracy': 23.48, 'per_class_accuracy': [88.1633, 98.9427, 28.7791, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6.3429], 'loss': 2.2835278656005857}\n",
            "OCS >> Task 20: {'accuracy': 23.88, 'per_class_accuracy': [90.6122, 99.7357, 26.4535, 0.0, 0.0, 0.0, 0.0, 0.0973, 0.0, 9.3162], 'loss': 2.28187442817688}\n",
            "OCS >> (average accuracy): 20.077999999999996\n",
            "OCS >> (Forgetting): 0.7555210526315791\n",
            "Maximum per-task accuracies: [95.43]\n",
            "\n",
            "{'num_tasks': 20, 'per_task_rotation': 9, 'memory_size': 200, 'dataset': 'rot-mnist', 'device': 'cuda', 'momentum': 0.8, 'mlp_hiddens': 256, 'dropout': 0.2, 'lr_decay': 0.75, 'n_classes': 10, 'seq_lr': 0.005, 'stream_size': 100, 'ocspick': True, 'batch_size': 20, 'ref_hyp': 10.0, 'n_substeps': 5}\n"
          ]
        }
      ],
      "source": [
        "DATASET = 'rot-mnist'\n",
        "HIDDENS = 256\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "config = {\n",
        "    'num_tasks': 20,\n",
        "    'per_task_rotation': 9,\n",
        "    'memory_size': 200,\n",
        "    'dataset': DATASET,\n",
        "    'device': DEVICE,\n",
        "    'momentum': 0.8,\n",
        "    'mlp_hiddens': HIDDENS,\n",
        "    'dropout': 0.2,\n",
        "    'lr_decay': 0.75 if 'rot-mnist' in DATASET else 0.8,\n",
        "    'n_classes': 10,\n",
        "    'seq_lr': 0.005,\n",
        "    'stream_size': 100,\n",
        "    'ocspick': True,\n",
        "    'batch_size': 20,\n",
        "    # 'tau': 1000.0,\n",
        "    'ref_hyp': 10. if 'rot-mnist' in DATASET else 50\n",
        "}\n",
        "\n",
        "log_dir =  f\"./summery/{config['dataset']}\"\n",
        "summary = SummaryWriter(log_dir)\n",
        "\n",
        "experiment = Experiment(api_key=\"hidden_key\", project_name=\"mnist\", disabled=True)\n",
        "\n",
        "loaders = get_all_loaders(config)\n",
        "\n",
        "\n",
        "def evaluate_model(model, task, loaders, config):\n",
        "    accuracies, losses = [], []\n",
        "    for t in range(1, task + 1):\n",
        "        metrics = eval_single_epoch(model, loaders['sequential'][t]['val'], config)\n",
        "        accuracies.append(metrics['accuracy'])\n",
        "        losses.append(metrics['loss'])\n",
        "        print(f'OCS >> Task {t}: {metrics}')\n",
        "    return accuracies, losses\n",
        "\n",
        "def main():\n",
        "    setup_experiment(experiment, config)\n",
        "\n",
        "    max_accuracies = [0.0] * config['num_tasks']\n",
        "    for task in range(1, config['num_tasks'] + 1):\n",
        "        print(f'---- Task {task} (OCS) ----')\n",
        "        model = train_task_sequentially(task, loaders, config, summary)\n",
        "\n",
        "        accuracies, _ = evaluate_model(model, task, loaders, config)\n",
        "        max_accuracies = [max(acc, max_acc) for acc, max_acc in zip(accuracies, max_accuracies)]\n",
        "\n",
        "        avg_accuracy = np.mean(accuracies)\n",
        "        if task > 1:\n",
        "            forgetting = np.mean(np.array(max_accuracies[:task - 1]) - np.array(accuracies[:task - 1]))/ 100\n",
        "        else:\n",
        "            forgetting = 0.0\n",
        "\n",
        "        print(f\"OCS >> (average accuracy): {avg_accuracy}\")\n",
        "        print(f\"OCS >> (Forgetting): {forgetting}\")\n",
        "        summary.add_scalar('cl_average_accuracy', avg_accuracy, task - 1)\n",
        "        print(f'Maximum per-task accuracies: {max_accuracies}\\n')\n",
        "\n",
        "    print(config)\n",
        "    experiment.end()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeI-dGIw94wg",
        "outputId": "c6cb41ad-e4cf-494b-aaa5-1195c4d31d8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:12<00:00, 13546410.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data/\n",
            "Files already downloaded and verified\n",
            "loading coreset placeholder rot-mnist\n",
            "loading rot-mnist for task 1\n",
            "Rotated MNIST\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:06<00:00, 1480087.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 134850.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:01<00:00, 1268839.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3172444.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "loading rot-mnist for task 2\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 3\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 4\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 5\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 6\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 7\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 8\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 9\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 10\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 11\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 12\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 13\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 14\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 15\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 16\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 17\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 18\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 19\n",
            "Rotated MNIST\n",
            "loading rot-mnist for task 20\n",
            "Rotated MNIST\n",
            "---- Task 1 (OCS) ----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0.05 >> (per-task accuracy): 80.06\n",
            "Epoch 0.05 >> (class accuracy): [96.1224, 99.0308, 67.0543, 79.4059, 92.057, 69.7309, 93.8413, 87.9377, 73.1006, 40.1388]\n",
            "Epoch 0.1 >> (per-task accuracy): 88.04\n",
            "Epoch 0.1 >> (class accuracy): [96.4286, 98.5903, 85.3682, 78.5149, 97.2505, 89.9103, 93.8413, 88.2296, 76.8994, 74.7275]\n",
            "Epoch 0.15 >> (per-task accuracy): 91.8\n",
            "Epoch 0.15 >> (class accuracy): [95.7143, 98.5903, 84.8837, 89.4059, 97.3523, 89.1256, 89.9791, 91.537, 93.3265, 87.3142]\n",
            "Epoch 0.2 >> (per-task accuracy): 93.85\n",
            "Epoch 0.2 >> (class accuracy): [97.2449, 98.7665, 88.469, 92.0792, 98.167, 92.3767, 94.2589, 93.1907, 94.1478, 89.3954]\n",
            "Epoch 0.25 >> (per-task accuracy): 94.45\n",
            "Epoch 0.25 >> (class accuracy): [98.2653, 98.8546, 92.345, 88.0198, 96.7413, 97.3094, 95.3027, 93.9689, 91.4784, 92.1705]\n",
            "Epoch 0.3 >> (per-task accuracy): 94.78\n",
            "Epoch 0.3 >> (class accuracy): [97.7551, 99.0308, 93.8953, 88.3168, 98.167, 98.8789, 95.3027, 94.0661, 93.1211, 89.3954]\n",
            "Epoch 0.35 >> (per-task accuracy): 95.74\n",
            "Epoch 0.35 >> (class accuracy): [98.1633, 98.5022, 94.6705, 90.9901, 97.0468, 98.0942, 94.9896, 94.1634, 97.0226, 93.8553]\n",
            "Epoch 0.4 >> (per-task accuracy): 96.42\n",
            "Epoch 0.4 >> (class accuracy): [98.3673, 99.0308, 95.9302, 95.8416, 95.6212, 97.87, 94.4676, 93.3852, 97.1253, 96.4321]\n",
            "Epoch 0.45 >> (per-task accuracy): 96.75\n",
            "Epoch 0.45 >> (class accuracy): [98.7755, 99.3833, 95.4457, 95.4455, 97.4542, 98.2063, 95.8246, 94.1634, 97.1253, 95.6392]\n",
            "Epoch 0.5 >> (per-task accuracy): 96.59\n",
            "Epoch 0.5 >> (class accuracy): [98.7755, 99.0308, 95.7364, 96.7327, 93.9919, 97.9821, 95.7203, 95.0389, 94.9692, 97.7205]\n",
            "Epoch 0.55 >> (per-task accuracy): 97.25\n",
            "Epoch 0.55 >> (class accuracy): [98.7755, 99.2952, 96.0271, 96.4356, 97.6578, 98.4305, 97.0772, 96.0117, 95.9959, 96.7294]\n",
            "Epoch 0.6 >> (per-task accuracy): 97.48\n",
            "Epoch 0.6 >> (class accuracy): [99.2857, 99.6476, 98.062, 96.9307, 96.5377, 97.87, 97.4948, 96.3035, 94.9692, 97.4232]\n",
            "Epoch 0.65 >> (per-task accuracy): 97.71\n",
            "Epoch 0.65 >> (class accuracy): [99.2857, 99.2952, 97.4806, 97.5248, 97.9633, 96.7489, 98.2255, 96.9844, 97.7413, 95.6392]\n",
            "Epoch 0.7 >> (per-task accuracy): 97.56\n",
            "Epoch 0.7 >> (class accuracy): [98.8776, 99.207, 98.062, 97.4257, 97.3523, 98.0942, 96.5553, 97.9572, 94.4559, 97.3241]\n",
            "Epoch 0.75 >> (per-task accuracy): 97.81\n",
            "Epoch 0.75 >> (class accuracy): [98.8776, 98.7665, 98.1589, 98.1188, 97.3523, 97.7578, 96.8685, 97.2763, 97.4333, 97.3241]\n",
            "Epoch 0.8 >> (per-task accuracy): 97.43\n",
            "Epoch 0.8 >> (class accuracy): [99.2857, 99.1189, 98.062, 97.9208, 94.9084, 97.5336, 96.6597, 96.6926, 95.6879, 98.1169]\n",
            "Epoch 0.85 >> (per-task accuracy): 97.09\n",
            "Epoch 0.85 >> (class accuracy): [99.2857, 99.4714, 98.7403, 93.2673, 96.334, 99.3274, 94.4676, 97.3735, 94.4559, 97.9187]\n",
            "Epoch 0.9 >> (per-task accuracy): 97.82\n",
            "Epoch 0.9 >> (class accuracy): [99.2857, 99.2952, 98.2558, 97.2277, 96.334, 98.4305, 97.286, 97.6654, 96.5092, 97.7205]\n",
            "Epoch 0.95 >> (per-task accuracy): 97.11\n",
            "Epoch 0.95 >> (class accuracy): [98.8776, 99.4714, 97.7713, 94.5545, 96.1303, 99.6637, 97.1816, 97.0817, 93.1211, 97.1259]\n",
            "Epoch 1.0 >> (per-task accuracy): 97.86\n",
            "Epoch 1.0 >> (class accuracy): [99.3878, 98.9427, 97.8682, 97.8218, 96.945, 98.991, 98.0167, 96.4008, 96.9199, 97.3241]\n",
            "OCS >> Task 1: {'accuracy': 97.86, 'per_class_accuracy': [99.3878, 98.9427, 97.8682, 97.8218, 96.945, 98.991, 98.0167, 96.4008, 96.9199, 97.3241], 'loss': 0.06555184647012502}\n",
            "OCS >> (average accuracy): 97.86\n",
            "OCS >> (Forgetting): 0.0\n",
            "Maximum per-task accuracies: [97.86]\n",
            "\n",
            "---- Task 2 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 83.42\n",
            "Epoch 0.05 >> (class accuracy): [96.5306, 95.0661, 74.031, 84.9505, 85.947, 69.843, 91.858, 86.3813, 69.9179, 77.2052]\n",
            "Epoch 0.1 >> (per-task accuracy): 91.27\n",
            "Epoch 0.1 >> (class accuracy): [96.7347, 98.2379, 89.438, 90.297, 92.668, 87.7803, 91.1273, 91.537, 82.4435, 91.0803]\n",
            "Epoch 0.15 >> (per-task accuracy): 93.71\n",
            "Epoch 0.15 >> (class accuracy): [98.5714, 99.0308, 88.8566, 88.8119, 97.556, 94.843, 94.572, 92.9961, 92.5051, 89.1972]\n",
            "Epoch 0.2 >> (per-task accuracy): 94.39\n",
            "Epoch 0.2 >> (class accuracy): [97.1429, 99.2952, 88.6628, 91.7822, 98.0652, 98.3184, 94.2589, 94.2607, 93.2238, 88.999]\n",
            "Epoch 0.25 >> (per-task accuracy): 96.17\n",
            "Epoch 0.25 >> (class accuracy): [98.6735, 99.207, 95.0581, 95.3465, 96.8432, 98.6547, 94.2589, 95.9144, 92.9158, 94.6482]\n",
            "Epoch 0.3 >> (per-task accuracy): 96.6\n",
            "Epoch 0.3 >> (class accuracy): [98.8776, 99.2952, 95.8333, 95.8416, 98.5743, 98.7668, 94.6764, 96.4008, 92.8131, 94.7473]\n",
            "Epoch 0.35 >> (per-task accuracy): 96.68\n",
            "Epoch 0.35 >> (class accuracy): [98.7755, 99.207, 95.3488, 94.7525, 97.556, 98.991, 96.8685, 94.9416, 94.9692, 95.441]\n",
            "Epoch 0.4 >> (per-task accuracy): 97.59\n",
            "Epoch 0.4 >> (class accuracy): [99.2857, 99.3833, 97.3837, 97.8218, 98.2688, 96.3004, 96.5553, 96.1089, 98.2546, 96.2339]\n",
            "Epoch 0.45 >> (per-task accuracy): 97.51\n",
            "Epoch 0.45 >> (class accuracy): [99.0816, 99.1189, 96.9961, 96.9307, 98.0652, 98.4305, 96.0334, 96.3035, 97.9466, 96.1348]\n",
            "Epoch 0.5 >> (per-task accuracy): 97.56\n",
            "Epoch 0.5 >> (class accuracy): [99.3878, 99.3833, 97.093, 97.5248, 98.2688, 97.0852, 95.3027, 96.6926, 97.9466, 96.6303]\n",
            "Epoch 0.55 >> (per-task accuracy): 97.33\n",
            "Epoch 0.55 >> (class accuracy): [99.0816, 99.6476, 97.1899, 95.6436, 98.4725, 99.2152, 96.5553, 96.9844, 94.3532, 96.0357]\n",
            "Epoch 0.6 >> (per-task accuracy): 97.7\n",
            "Epoch 0.6 >> (class accuracy): [98.7755, 99.2952, 97.8682, 97.4257, 98.2688, 98.991, 97.286, 96.8872, 96.9199, 95.2428]\n",
            "Epoch 0.65 >> (per-task accuracy): 97.88\n",
            "Epoch 0.65 >> (class accuracy): [98.9796, 99.207, 98.3527, 97.6238, 98.3707, 97.4215, 97.9123, 96.6926, 97.1253, 96.9277]\n",
            "Epoch 0.7 >> (per-task accuracy): 97.7\n",
            "Epoch 0.7 >> (class accuracy): [99.2857, 99.5595, 98.1589, 98.2178, 98.167, 97.7578, 96.7641, 97.0817, 95.0719, 96.6303]\n",
            "Epoch 0.75 >> (per-task accuracy): 97.49\n",
            "Epoch 0.75 >> (class accuracy): [99.3878, 99.207, 98.062, 96.6337, 98.3707, 98.6547, 95.8246, 97.179, 94.8665, 96.5312]\n",
            "Epoch 0.8 >> (per-task accuracy): 97.9\n",
            "Epoch 0.8 >> (class accuracy): [99.4898, 99.4714, 98.1589, 97.8218, 97.556, 97.7578, 97.3904, 96.4981, 96.9199, 97.7205]\n",
            "Epoch 0.85 >> (per-task accuracy): 98.11\n",
            "Epoch 0.85 >> (class accuracy): [99.3878, 99.4714, 98.1589, 98.3168, 98.3707, 98.2063, 97.4948, 97.0817, 96.8172, 97.6214]\n",
            "Epoch 0.9 >> (per-task accuracy): 98.13\n",
            "Epoch 0.9 >> (class accuracy): [99.2857, 99.4714, 97.8682, 98.3168, 98.3707, 98.0942, 97.9123, 97.2763, 97.3306, 97.225]\n",
            "Epoch 0.95 >> (per-task accuracy): 98.07\n",
            "Epoch 0.95 >> (class accuracy): [99.4898, 99.4714, 97.9651, 98.3168, 97.556, 97.4215, 97.9123, 97.0817, 97.2279, 98.0178]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-b440c4a92a02>:12: RuntimeWarning: divide by zero encountered in scalar floor_divide\n",
            "  if (num_residuals // num_class) > 0:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1.0 >> (per-task accuracy): 98.05\n",
            "Epoch 1.0 >> (class accuracy): [99.2857, 99.4714, 97.9651, 98.0198, 97.9633, 98.3184, 97.9123, 97.5681, 96.4066, 97.4232]\n",
            "OCS >> Task 1: {'accuracy': 96.69, 'per_class_accuracy': [98.9796, 98.4141, 97.093, 97.3267, 96.334, 97.3094, 95.5115, 96.3035, 94.7639, 94.6482], 'loss': 0.134861160296225}\n",
            "OCS >> Task 2: {'accuracy': 98.05, 'per_class_accuracy': [99.2857, 99.4714, 97.9651, 98.0198, 97.9633, 98.3184, 97.9123, 97.5681, 96.4066, 97.4232], 'loss': 0.07530809896745486}\n",
            "OCS >> (average accuracy): 97.37\n",
            "OCS >> (Forgetting): 0.011700000000000018\n",
            "Maximum per-task accuracies: [97.86]\n",
            "\n",
            "---- Task 3 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 82.39\n",
            "Epoch 0.05 >> (class accuracy): [95.0, 97.1806, 77.907, 75.6436, 90.0204, 81.1659, 91.023, 92.0233, 59.0349, 63.0327]\n",
            "Epoch 0.1 >> (per-task accuracy): 88.65\n",
            "Epoch 0.1 >> (class accuracy): [96.5306, 97.0925, 83.5271, 84.6535, 96.334, 90.3587, 89.0397, 87.3541, 75.77, 85.1338]\n",
            "Epoch 0.15 >> (per-task accuracy): 93.07\n",
            "Epoch 0.15 >> (class accuracy): [96.6327, 98.2379, 89.8256, 92.0792, 94.6029, 93.2735, 93.3194, 92.0233, 90.2464, 89.9901]\n",
            "Epoch 0.2 >> (per-task accuracy): 93.29\n",
            "Epoch 0.2 >> (class accuracy): [95.8163, 98.326, 88.7597, 88.6139, 91.9552, 96.861, 91.5449, 92.8988, 94.8665, 93.1615]\n",
            "Epoch 0.25 >> (per-task accuracy): 94.91\n",
            "Epoch 0.25 >> (class accuracy): [97.3469, 98.2379, 93.4109, 91.9802, 94.9084, 96.7489, 93.8413, 92.8988, 97.3306, 92.3687]\n",
            "Epoch 0.3 >> (per-task accuracy): 95.28\n",
            "Epoch 0.3 >> (class accuracy): [97.7551, 99.1189, 94.2829, 91.1881, 95.8248, 98.7668, 94.3633, 93.7743, 93.2238, 94.45]\n",
            "Epoch 0.35 >> (per-task accuracy): 96.02\n",
            "Epoch 0.35 >> (class accuracy): [98.6735, 99.1189, 94.4767, 93.7624, 97.1487, 97.87, 94.3633, 94.0661, 95.7906, 94.8464]\n",
            "Epoch 0.4 >> (per-task accuracy): 96.36\n",
            "Epoch 0.4 >> (class accuracy): [98.3673, 99.207, 95.9302, 94.7525, 97.1487, 98.991, 95.1983, 93.8716, 95.3799, 94.7473]\n",
            "Epoch 0.45 >> (per-task accuracy): 96.83\n",
            "Epoch 0.45 >> (class accuracy): [98.2653, 99.2952, 96.6085, 94.6535, 97.556, 97.7578, 96.7641, 95.5253, 97.1253, 94.6482]\n",
            "Epoch 0.5 >> (per-task accuracy): 96.57\n",
            "Epoch 0.5 >> (class accuracy): [98.7755, 99.207, 96.8992, 93.0693, 95.8248, 98.6547, 96.7641, 96.3035, 93.4292, 96.6303]\n",
            "Epoch 0.55 >> (per-task accuracy): 97.26\n",
            "Epoch 0.55 >> (class accuracy): [98.6735, 99.207, 96.8992, 96.5347, 97.4542, 97.4215, 96.9729, 96.7899, 95.5852, 96.8285]\n",
            "Epoch 0.6 >> (per-task accuracy): 97.61\n",
            "Epoch 0.6 >> (class accuracy): [99.3878, 99.207, 97.5775, 97.8218, 97.4542, 97.3094, 97.4948, 96.3035, 96.2012, 97.1259]\n",
            "Epoch 0.65 >> (per-task accuracy): 97.77\n",
            "Epoch 0.65 >> (class accuracy): [99.0816, 99.4714, 97.7713, 97.6238, 98.2688, 95.5157, 97.286, 97.179, 97.5359, 97.5223]\n",
            "Epoch 0.7 >> (per-task accuracy): 97.61\n",
            "Epoch 0.7 >> (class accuracy): [99.1837, 99.4714, 97.7713, 96.7327, 97.8615, 98.3184, 96.8685, 97.179, 95.1745, 97.3241]\n",
            "Epoch 0.75 >> (per-task accuracy): 97.38\n",
            "Epoch 0.75 >> (class accuracy): [98.6735, 99.6476, 97.7713, 97.9208, 97.7597, 98.0942, 96.3466, 97.179, 92.7105, 97.3241]\n",
            "Epoch 0.8 >> (per-task accuracy): 97.46\n",
            "Epoch 0.8 >> (class accuracy): [98.9796, 99.2952, 97.4806, 96.9307, 98.167, 98.991, 97.4948, 96.9844, 93.1211, 97.0268]\n",
            "Epoch 0.85 >> (per-task accuracy): 97.71\n",
            "Epoch 0.85 >> (class accuracy): [98.9796, 99.2952, 97.3837, 98.4158, 98.3707, 97.5336, 96.6597, 97.4708, 95.8932, 96.8285]\n",
            "Epoch 0.9 >> (per-task accuracy): 97.73\n",
            "Epoch 0.9 >> (class accuracy): [99.0816, 99.5595, 97.6744, 97.9208, 98.4725, 97.87, 97.1816, 96.7899, 94.8665, 97.6214]\n",
            "Epoch 0.95 >> (per-task accuracy): 97.78\n",
            "Epoch 0.95 >> (class accuracy): [99.0816, 99.6476, 97.3837, 98.4158, 98.0652, 97.3094, 97.0772, 97.2763, 95.5852, 97.6214]\n",
            "Epoch 1.0 >> (per-task accuracy): 97.96\n",
            "Epoch 1.0 >> (class accuracy): [99.1837, 99.4714, 97.7713, 98.0198, 98.2688, 97.4215, 98.2255, 97.4708, 96.0986, 97.4232]\n",
            "OCS >> Task 1: {'accuracy': 92.55, 'per_class_accuracy': [98.7755, 95.4185, 93.7984, 96.6337, 92.1589, 92.0404, 92.5887, 87.3541, 89.4251, 87.0168], 'loss': 0.32162896134853364}\n",
            "OCS >> Task 2: {'accuracy': 96.42, 'per_class_accuracy': [99.2857, 98.8546, 96.6085, 98.0198, 96.7413, 94.2825, 97.4948, 94.6498, 94.0452, 93.7562], 'loss': 0.12780935356020928}\n",
            "OCS >> Task 3: {'accuracy': 97.96, 'per_class_accuracy': [99.1837, 99.4714, 97.7713, 98.0198, 98.2688, 97.4215, 98.2255, 97.4708, 96.0986, 97.4232], 'loss': 0.07221267184191384}\n",
            "OCS >> (average accuracy): 95.64333333333333\n",
            "OCS >> (Forgetting): 0.03375\n",
            "Maximum per-task accuracies: [97.86]\n",
            "\n",
            "---- Task 4 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 74.67\n",
            "Epoch 0.05 >> (class accuracy): [90.5102, 97.7093, 78.6822, 56.6337, 67.8208, 62.5561, 75.7829, 75.4864, 48.0493, 88.5035]\n",
            "Epoch 0.1 >> (per-task accuracy): 87.82\n",
            "Epoch 0.1 >> (class accuracy): [97.2449, 96.652, 83.1395, 80.5941, 82.4847, 86.7713, 88.309, 85.7004, 84.3943, 91.8731]\n",
            "Epoch 0.15 >> (per-task accuracy): 90.36\n",
            "Epoch 0.15 >> (class accuracy): [94.3878, 98.5903, 79.2636, 90.0, 87.6782, 89.7982, 90.0835, 89.8833, 89.6304, 93.4589]\n",
            "Epoch 0.2 >> (per-task accuracy): 91.97\n",
            "Epoch 0.2 >> (class accuracy): [93.6735, 98.4141, 86.6279, 81.2871, 93.89, 97.0852, 92.38, 91.4397, 94.4559, 90.5847]\n",
            "Epoch 0.25 >> (per-task accuracy): 93.31\n",
            "Epoch 0.25 >> (class accuracy): [94.4898, 97.9736, 91.8605, 87.3267, 95.9267, 97.87, 94.0501, 91.8288, 92.9158, 88.999]\n",
            "Epoch 0.3 >> (per-task accuracy): 93.9\n",
            "Epoch 0.3 >> (class accuracy): [97.7551, 98.7665, 93.7016, 85.6436, 96.7413, 98.6547, 93.4238, 94.1634, 88.3984, 91.6749]\n",
            "Epoch 0.35 >> (per-task accuracy): 95.64\n",
            "Epoch 0.35 >> (class accuracy): [97.8571, 98.8546, 94.9612, 91.7822, 97.1487, 98.3184, 95.0939, 94.5525, 95.0719, 92.7651]\n",
            "Epoch 0.4 >> (per-task accuracy): 96.46\n",
            "Epoch 0.4 >> (class accuracy): [98.2653, 99.207, 95.2519, 95.3465, 97.3523, 98.2063, 95.929, 95.1362, 95.0719, 94.7473]\n",
            "Epoch 0.45 >> (per-task accuracy): 96.6\n",
            "Epoch 0.45 >> (class accuracy): [97.8571, 99.207, 96.124, 92.9703, 97.4542, 98.7668, 96.1378, 95.7198, 96.2012, 95.5401]\n",
            "Epoch 0.5 >> (per-task accuracy): 97.1\n",
            "Epoch 0.5 >> (class accuracy): [98.2653, 99.207, 96.6085, 96.3366, 98.167, 98.4305, 96.3466, 95.1362, 96.3039, 96.1348]\n",
            "Epoch 0.55 >> (per-task accuracy): 97.38\n",
            "Epoch 0.55 >> (class accuracy): [98.6735, 99.207, 96.7054, 97.4257, 97.8615, 97.1973, 96.9729, 96.1089, 97.4333, 96.0357]\n",
            "Epoch 0.6 >> (per-task accuracy): 97.61\n",
            "Epoch 0.6 >> (class accuracy): [98.9796, 99.2952, 97.3837, 97.0297, 97.8615, 96.6368, 97.5992, 96.4008, 97.7413, 96.9277]\n",
            "Epoch 0.65 >> (per-task accuracy): 97.54\n",
            "Epoch 0.65 >> (class accuracy): [98.8776, 99.3833, 97.3837, 96.4356, 98.0652, 97.87, 96.9729, 95.9144, 97.6386, 96.7294]\n",
            "Epoch 0.7 >> (per-task accuracy): 97.57\n",
            "Epoch 0.7 >> (class accuracy): [99.1837, 99.207, 96.8992, 96.7327, 98.0652, 97.4215, 97.1816, 96.2062, 97.5359, 97.1259]\n",
            "Epoch 0.75 >> (per-task accuracy): 97.7\n",
            "Epoch 0.75 >> (class accuracy): [99.1837, 99.5595, 97.4806, 98.2178, 97.6578, 97.87, 96.1378, 95.5253, 97.2279, 97.9187]\n",
            "Epoch 0.8 >> (per-task accuracy): 97.73\n",
            "Epoch 0.8 >> (class accuracy): [99.0816, 99.3833, 97.7713, 98.2178, 97.6578, 97.1973, 96.5553, 96.1089, 97.2279, 97.8196]\n",
            "Epoch 0.85 >> (per-task accuracy): 97.85\n",
            "Epoch 0.85 >> (class accuracy): [99.1837, 99.3833, 97.8682, 97.9208, 98.3707, 98.5426, 96.5553, 96.6926, 96.4066, 97.4232]\n",
            "Epoch 0.9 >> (per-task accuracy): 97.67\n",
            "Epoch 0.9 >> (class accuracy): [99.1837, 99.4714, 98.2558, 97.3267, 97.2505, 99.2152, 95.7203, 96.6926, 95.5852, 97.8196]\n",
            "Epoch 0.95 >> (per-task accuracy): 97.66\n",
            "Epoch 0.95 >> (class accuracy): [99.0816, 99.1189, 98.3527, 97.0297, 98.0652, 98.991, 97.3904, 96.4981, 95.0719, 96.9277]\n",
            "Epoch 1.0 >> (per-task accuracy): 97.87\n",
            "Epoch 1.0 >> (class accuracy): [99.1837, 99.3833, 98.3527, 97.2277, 98.2688, 98.2063, 97.7035, 96.5953, 96.0986, 97.5223]\n",
            "OCS >> Task 1: {'accuracy': 83.48, 'per_class_accuracy': [97.3469, 88.37, 82.7519, 87.6238, 74.3381, 84.8655, 81.524, 74.6109, 77.2074, 85.7284], 'loss': 0.7030272988319397}\n",
            "OCS >> Task 2: {'accuracy': 92.43, 'per_class_accuracy': [99.0816, 96.1233, 92.2481, 93.3663, 89.5112, 90.9193, 91.858, 91.3424, 89.0144, 90.1883], 'loss': 0.281146355676651}\n",
            "OCS >> Task 3: {'accuracy': 96.31, 'per_class_accuracy': [98.9796, 99.1189, 96.2209, 96.4356, 95.9267, 96.0762, 95.4071, 95.2335, 93.3265, 95.9366], 'loss': 0.12458127521574497}\n",
            "OCS >> Task 4: {'accuracy': 97.87, 'per_class_accuracy': [99.1837, 99.3833, 98.3527, 97.2277, 98.2688, 98.2063, 97.7035, 96.5953, 96.0986, 97.5223], 'loss': 0.07396355776386336}\n",
            "OCS >> (average accuracy): 92.52250000000001\n",
            "OCS >> (Forgetting): 0.07119999999999994\n",
            "Maximum per-task accuracies: [97.86]\n",
            "\n",
            "---- Task 5 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 74.54\n",
            "Epoch 0.05 >> (class accuracy): [96.0204, 96.5639, 64.7287, 87.6238, 59.2668, 52.2422, 70.2505, 78.9883, 59.3429, 74.6283]\n",
            "Epoch 0.1 >> (per-task accuracy): 85.57\n",
            "Epoch 0.1 >> (class accuracy): [94.1837, 97.0925, 80.9109, 76.7327, 91.8534, 85.7623, 90.0835, 86.0895, 81.2115, 70.9613]\n",
            "Epoch 0.15 >> (per-task accuracy): 88.34\n",
            "Epoch 0.15 >> (class accuracy): [93.6735, 96.8282, 80.814, 83.3663, 94.0937, 88.3408, 90.2923, 88.716, 86.345, 80.3766]\n",
            "Epoch 0.2 >> (per-task accuracy): 91.5\n",
            "Epoch 0.2 >> (class accuracy): [95.3061, 97.8855, 87.4031, 84.6535, 93.5845, 93.3857, 92.7975, 90.856, 91.5811, 87.3142]\n",
            "Epoch 0.25 >> (per-task accuracy): 92.98\n",
            "Epoch 0.25 >> (class accuracy): [94.7959, 98.5903, 89.0504, 91.7822, 92.9735, 92.9372, 93.4238, 92.0233, 93.2238, 90.4856]\n",
            "Epoch 0.3 >> (per-task accuracy): 94.31\n",
            "Epoch 0.3 >> (class accuracy): [97.3469, 98.6784, 90.9884, 93.2673, 95.8248, 94.843, 95.5115, 92.7043, 91.9918, 91.6749]\n",
            "Epoch 0.35 >> (per-task accuracy): 94.34\n",
            "Epoch 0.35 >> (class accuracy): [96.7347, 98.5903, 93.1202, 92.9703, 94.1955, 97.0852, 95.7203, 91.6342, 89.0144, 94.1526]\n",
            "Epoch 0.4 >> (per-task accuracy): 95.16\n",
            "Epoch 0.4 >> (class accuracy): [97.8571, 99.0308, 94.9612, 92.3762, 94.9084, 97.6457, 96.9729, 92.9961, 91.9918, 92.7651]\n",
            "Epoch 0.45 >> (per-task accuracy): 95.02\n",
            "Epoch 0.45 >> (class accuracy): [98.2653, 99.1189, 94.3798, 88.0198, 95.8248, 98.6547, 97.0772, 93.1907, 92.5051, 93.2607]\n",
            "Epoch 0.5 >> (per-task accuracy): 96.1\n",
            "Epoch 0.5 >> (class accuracy): [98.5714, 99.207, 94.9612, 94.1584, 97.2505, 98.2063, 97.286, 94.358, 92.9158, 94.0535]\n",
            "Epoch 0.55 >> (per-task accuracy): 96.29\n",
            "Epoch 0.55 >> (class accuracy): [98.4694, 99.2952, 95.6395, 93.8614, 97.2505, 98.4305, 97.0772, 95.2335, 93.5318, 94.0535]\n",
            "Epoch 0.6 >> (per-task accuracy): 96.74\n",
            "Epoch 0.6 >> (class accuracy): [98.4694, 99.2952, 96.3178, 95.1485, 97.0468, 98.0942, 97.1816, 95.8171, 94.8665, 95.0446]\n",
            "Epoch 0.65 >> (per-task accuracy): 96.87\n",
            "Epoch 0.65 >> (class accuracy): [98.1633, 99.2952, 96.7054, 95.4455, 97.556, 98.5426, 97.5992, 96.4008, 93.2238, 95.6392]\n",
            "Epoch 0.7 >> (per-task accuracy): 97.04\n",
            "Epoch 0.7 >> (class accuracy): [98.6735, 99.4714, 96.7054, 96.0396, 96.7413, 98.0942, 97.4948, 96.4981, 94.6612, 95.8375]\n",
            "Epoch 0.75 >> (per-task accuracy): 97.23\n",
            "Epoch 0.75 >> (class accuracy): [98.1633, 99.207, 97.6744, 96.4356, 96.6395, 98.5426, 97.7035, 96.4008, 95.4825, 95.9366]\n",
            "Epoch 0.8 >> (per-task accuracy): 97.51\n",
            "Epoch 0.8 >> (class accuracy): [98.0612, 99.0308, 97.6744, 97.1287, 97.556, 97.3094, 97.8079, 97.3735, 96.9199, 96.0357]\n",
            "Epoch 0.85 >> (per-task accuracy): 97.48\n",
            "Epoch 0.85 >> (class accuracy): [98.5714, 98.9427, 97.4806, 96.8317, 97.6578, 97.9821, 97.5992, 96.9844, 96.2012, 96.4321]\n",
            "Epoch 0.9 >> (per-task accuracy): 97.55\n",
            "Epoch 0.9 >> (class accuracy): [98.5714, 99.1189, 97.7713, 97.1287, 97.7597, 98.2063, 97.3904, 96.3035, 96.3039, 96.8285]\n",
            "Epoch 0.95 >> (per-task accuracy): 97.62\n",
            "Epoch 0.95 >> (class accuracy): [98.7755, 98.8546, 97.5775, 97.3267, 97.8615, 97.9821, 97.8079, 96.4008, 96.9199, 96.6303]\n",
            "Epoch 1.0 >> (per-task accuracy): 97.65\n",
            "Epoch 1.0 >> (class accuracy): [98.7755, 99.207, 97.6744, 97.2277, 97.9633, 98.6547, 98.1211, 96.7899, 94.7639, 97.225]\n",
            "OCS >> Task 1: {'accuracy': 71.03, 'per_class_accuracy': [92.2449, 87.9295, 65.3101, 76.8317, 52.7495, 75.3363, 62.7349, 59.4358, 56.6735, 78.9891], 'loss': 1.1863377885818482}\n",
            "OCS >> Task 2: {'accuracy': 84.73, 'per_class_accuracy': [95.5102, 94.0969, 82.3643, 87.0297, 79.7352, 85.6502, 76.2004, 81.6148, 75.154, 88.4044], 'loss': 0.5512135880470276}\n",
            "OCS >> Task 3: {'accuracy': 92.01, 'per_class_accuracy': [97.2449, 97.0925, 90.6977, 91.6832, 90.835, 94.5067, 88.309, 89.9805, 87.0637, 92.1705], 'loss': 0.2584807307004929}\n",
            "OCS >> Task 4: {'accuracy': 95.91, 'per_class_accuracy': [98.7755, 99.207, 95.2519, 96.3366, 94.7047, 96.7489, 95.8246, 95.1362, 90.4517, 96.2339], 'loss': 0.1327836789906025}\n",
            "OCS >> Task 5: {'accuracy': 97.65, 'per_class_accuracy': [98.7755, 99.207, 97.6744, 97.2277, 97.9633, 98.6547, 98.1211, 96.7899, 94.7639, 97.225], 'loss': 0.08066798886694014}\n",
            "OCS >> (average accuracy): 88.26599999999999\n",
            "OCS >> (Forgetting): 0.11939999999999998\n",
            "Maximum per-task accuracies: [97.86]\n",
            "\n",
            "---- Task 6 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 68.73\n",
            "Epoch 0.05 >> (class accuracy): [91.8367, 95.1542, 49.031, 86.1386, 58.554, 29.8206, 88.2046, 72.8599, 45.8932, 62.9336]\n",
            "Epoch 0.1 >> (per-task accuracy): 80.06\n",
            "Epoch 0.1 >> (class accuracy): [92.1429, 96.652, 67.9264, 73.0693, 86.0489, 70.7399, 90.3967, 84.8249, 82.2382, 54.7076]\n",
            "Epoch 0.15 >> (per-task accuracy): 84.51\n",
            "Epoch 0.15 >> (class accuracy): [93.2653, 94.4493, 77.8101, 77.2277, 92.057, 79.2601, 88.9353, 90.9533, 84.3943, 65.6095]\n",
            "Epoch 0.2 >> (per-task accuracy): 87.79\n",
            "Epoch 0.2 >> (class accuracy): [95.8163, 97.533, 85.562, 79.2079, 90.835, 89.6861, 89.7704, 90.0778, 76.386, 82.0614]\n",
            "Epoch 0.25 >> (per-task accuracy): 90.23\n",
            "Epoch 0.25 >> (class accuracy): [96.5306, 97.1806, 86.531, 85.1485, 95.0102, 90.6951, 89.0397, 90.3696, 86.345, 84.8365]\n",
            "Epoch 0.3 >> (per-task accuracy): 90.97\n",
            "Epoch 0.3 >> (class accuracy): [95.8163, 98.0617, 85.6589, 84.9505, 94.6029, 95.2915, 89.9791, 90.2724, 87.269, 87.6115]\n",
            "Epoch 0.35 >> (per-task accuracy): 93.05\n",
            "Epoch 0.35 >> (class accuracy): [95.6122, 98.5903, 89.9225, 90.495, 94.7047, 96.0762, 92.6931, 91.9261, 88.6037, 91.5758]\n",
            "Epoch 0.4 >> (per-task accuracy): 93.85\n",
            "Epoch 0.4 >> (class accuracy): [96.7347, 98.5903, 91.9574, 89.604, 94.8065, 96.0762, 95.1983, 93.3852, 90.7598, 91.1794]\n",
            "Epoch 0.45 >> (per-task accuracy): 94.19\n",
            "Epoch 0.45 >> (class accuracy): [97.449, 98.4141, 92.0543, 90.8911, 95.6212, 96.7489, 95.6159, 93.2879, 89.117, 92.5669]\n",
            "Epoch 0.5 >> (per-task accuracy): 94.91\n",
            "Epoch 0.5 >> (class accuracy): [97.0408, 98.6784, 93.8953, 91.5842, 95.3157, 97.7578, 95.3027, 94.0661, 92.8131, 92.5669]\n",
            "Epoch 0.55 >> (per-task accuracy): 95.49\n",
            "Epoch 0.55 >> (class accuracy): [97.3469, 98.6784, 94.0891, 93.8614, 95.2138, 98.2063, 95.5115, 94.358, 94.0452, 93.558]\n",
            "Epoch 0.6 >> (per-task accuracy): 95.65\n",
            "Epoch 0.6 >> (class accuracy): [98.0612, 98.6784, 94.6705, 92.3762, 96.5377, 98.5426, 95.8246, 94.9416, 93.6345, 93.2607]\n",
            "Epoch 0.65 >> (per-task accuracy): 95.81\n",
            "Epoch 0.65 >> (class accuracy): [97.8571, 98.8546, 95.4457, 93.5644, 96.7413, 98.6547, 95.7203, 93.8716, 94.3532, 93.0624]\n",
            "Epoch 0.7 >> (per-task accuracy): 96.24\n",
            "Epoch 0.7 >> (class accuracy): [98.5714, 99.1189, 95.9302, 95.0495, 96.334, 98.3184, 96.5553, 94.8444, 92.7105, 94.8464]\n",
            "Epoch 0.75 >> (per-task accuracy): 96.47\n",
            "Epoch 0.75 >> (class accuracy): [98.5714, 98.9427, 97.2868, 95.1485, 96.2322, 98.4305, 96.2422, 95.9144, 93.5318, 94.2517]\n",
            "Epoch 0.8 >> (per-task accuracy): 96.54\n",
            "Epoch 0.8 >> (class accuracy): [98.7755, 99.1189, 96.8992, 95.2475, 95.8248, 98.4305, 96.9729, 96.2062, 92.9158, 94.8464]\n",
            "Epoch 0.85 >> (per-task accuracy): 96.41\n",
            "Epoch 0.85 >> (class accuracy): [98.8776, 99.207, 97.1899, 94.0594, 96.945, 98.8789, 96.6597, 95.9144, 92.2998, 93.9544]\n",
            "Epoch 0.9 >> (per-task accuracy): 96.81\n",
            "Epoch 0.9 >> (class accuracy): [98.8776, 99.207, 96.8023, 96.3366, 97.4542, 98.4305, 96.6597, 96.3035, 93.1211, 94.7473]\n",
            "Epoch 0.95 >> (per-task accuracy): 97.21\n",
            "Epoch 0.95 >> (class accuracy): [98.8776, 99.1189, 97.5775, 97.6238, 97.3523, 97.3094, 98.2255, 96.5953, 93.9425, 95.2428]\n",
            "Epoch 1.0 >> (per-task accuracy): 97.25\n",
            "Epoch 1.0 >> (class accuracy): [99.0816, 99.1189, 97.5775, 97.8218, 96.8432, 97.9821, 97.0772, 95.8171, 95.4825, 95.5401]\n",
            "OCS >> Task 1: {'accuracy': 59.25, 'per_class_accuracy': [96.3265, 90.0441, 49.1279, 63.0693, 35.1324, 58.6323, 57.8288, 30.9339, 38.0903, 69.7721], 'loss': 1.5876279702186584}\n",
            "OCS >> Task 2: {'accuracy': 70.9, 'per_class_accuracy': [97.0408, 94.3612, 65.0194, 74.1584, 58.1466, 65.2466, 65.2401, 52.6265, 58.5216, 75.223], 'loss': 0.9911254692077637}\n",
            "OCS >> Task 3: {'accuracy': 83.13, 'per_class_accuracy': [97.449, 93.6564, 81.4922, 84.6535, 79.0224, 80.7175, 82.2547, 78.0156, 74.23, 78.2953], 'loss': 0.5342617272853851}\n",
            "OCS >> Task 4: {'accuracy': 90.8, 'per_class_accuracy': [98.0612, 94.978, 89.7287, 94.0594, 89.1039, 89.2377, 89.8747, 88.035, 86.4476, 87.8097], 'loss': 0.2898279608249664}\n",
            "OCS >> Task 5: {'accuracy': 95.12, 'per_class_accuracy': [98.2653, 97.9736, 95.7364, 97.1287, 90.7332, 92.4888, 96.1378, 94.7471, 93.7372, 93.558], 'loss': 0.15624907150268555}\n",
            "OCS >> Task 6: {'accuracy': 97.25, 'per_class_accuracy': [99.0816, 99.1189, 97.5775, 97.8218, 96.8432, 97.9821, 97.0772, 95.8171, 95.4825, 95.5401], 'loss': 0.09889115265831351}\n",
            "OCS >> (average accuracy): 82.74166666666666\n",
            "OCS >> (Forgetting): 0.1802\n",
            "Maximum per-task accuracies: [97.86]\n",
            "\n",
            "---- Task 7 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 59.9\n",
            "Epoch 0.05 >> (class accuracy): [97.7551, 94.3612, 19.6705, 51.1881, 2.6477, 47.9821, 73.0689, 73.3463, 60.1643, 74.0337]\n",
            "Epoch 0.1 >> (per-task accuracy): 76.92\n",
            "Epoch 0.1 >> (class accuracy): [96.0204, 93.9207, 58.7209, 78.3168, 69.8574, 62.8924, 77.6618, 85.4086, 81.8275, 61.6452]\n",
            "Epoch 0.15 >> (per-task accuracy): 80.99\n",
            "Epoch 0.15 >> (class accuracy): [92.551, 94.6256, 73.6434, 79.3069, 87.9837, 77.5785, 84.7599, 85.1167, 75.0513, 57.78]\n",
            "Epoch 0.2 >> (per-task accuracy): 85.11\n",
            "Epoch 0.2 >> (class accuracy): [94.4898, 95.4185, 79.7481, 77.8218, 87.78, 78.4753, 88.1002, 85.5058, 86.2423, 76.115]\n",
            "Epoch 0.25 >> (per-task accuracy): 87.56\n",
            "Epoch 0.25 >> (class accuracy): [95.3061, 97.0044, 83.3333, 82.0792, 89.1039, 81.7265, 88.8309, 87.5486, 88.501, 80.773]\n",
            "Epoch 0.3 >> (per-task accuracy): 89.5\n",
            "Epoch 0.3 >> (class accuracy): [95.8163, 97.7093, 84.4961, 87.6238, 87.5764, 86.5471, 89.2484, 87.9377, 88.7064, 88.2061]\n",
            "Epoch 0.35 >> (per-task accuracy): 91.01\n",
            "Epoch 0.35 >> (class accuracy): [95.9184, 97.7974, 86.3372, 91.4851, 90.1222, 89.2377, 90.7098, 89.5914, 88.7064, 89.2963]\n",
            "Epoch 0.4 >> (per-task accuracy): 91.99\n",
            "Epoch 0.4 >> (class accuracy): [96.3265, 98.1498, 87.2093, 90.9901, 92.9735, 92.3767, 90.501, 91.1479, 89.117, 90.4856]\n",
            "Epoch 0.45 >> (per-task accuracy): 92.71\n",
            "Epoch 0.45 >> (class accuracy): [96.3265, 97.7093, 89.8256, 89.703, 93.1772, 95.5157, 91.9624, 92.1206, 90.0411, 90.4856]\n",
            "Epoch 0.5 >> (per-task accuracy): 93.69\n",
            "Epoch 0.5 >> (class accuracy): [96.6327, 97.9736, 92.345, 92.7723, 95.2138, 96.3004, 92.0668, 91.9261, 90.8624, 90.5847]\n",
            "Epoch 0.55 >> (per-task accuracy): 94.07\n",
            "Epoch 0.55 >> (class accuracy): [96.9388, 98.2379, 93.5078, 91.5842, 94.0937, 96.5247, 93.5282, 91.7315, 93.3265, 91.0803]\n",
            "Epoch 0.6 >> (per-task accuracy): 94.66\n",
            "Epoch 0.6 >> (class accuracy): [97.2449, 98.326, 93.8953, 92.3762, 93.6864, 96.861, 93.6326, 92.8988, 93.6345, 93.8553]\n",
            "Epoch 0.65 >> (per-task accuracy): 95.06\n",
            "Epoch 0.65 >> (class accuracy): [97.2449, 98.4141, 94.5736, 94.0594, 96.2322, 96.6368, 94.2589, 92.607, 93.7372, 92.666]\n",
            "Epoch 0.7 >> (per-task accuracy): 95.55\n",
            "Epoch 0.7 >> (class accuracy): [97.9592, 98.8546, 96.2209, 94.3564, 95.723, 96.5247, 94.7808, 93.8716, 93.3265, 93.558]\n",
            "Epoch 0.75 >> (per-task accuracy): 95.86\n",
            "Epoch 0.75 >> (class accuracy): [98.0612, 98.8546, 95.5426, 94.2574, 96.334, 96.7489, 94.9896, 94.1634, 94.7639, 94.6482]\n",
            "Epoch 0.8 >> (per-task accuracy): 96.14\n",
            "Epoch 0.8 >> (class accuracy): [98.5714, 98.9427, 96.4147, 95.4455, 96.6395, 97.0852, 95.0939, 94.7471, 93.4292, 94.7473]\n",
            "Epoch 0.85 >> (per-task accuracy): 96.27\n",
            "Epoch 0.85 >> (class accuracy): [98.1633, 98.9427, 96.8023, 95.2475, 95.9267, 97.5336, 95.929, 94.5525, 93.9425, 95.441]\n",
            "Epoch 0.9 >> (per-task accuracy): 96.54\n",
            "Epoch 0.9 >> (class accuracy): [98.5714, 98.9427, 96.9961, 96.3366, 96.945, 96.861, 96.4509, 94.0661, 94.4559, 95.5401]\n",
            "Epoch 0.95 >> (per-task accuracy): 96.68\n",
            "Epoch 0.95 >> (class accuracy): [98.5714, 99.207, 96.5116, 97.1287, 97.8615, 96.7489, 96.1378, 94.5525, 94.5585, 95.2428]\n",
            "Epoch 1.0 >> (per-task accuracy): 96.81\n",
            "Epoch 1.0 >> (class accuracy): [98.6735, 99.1189, 97.2868, 95.8416, 96.4358, 97.7578, 96.4509, 95.7198, 95.2772, 95.3419]\n",
            "OCS >> Task 1: {'accuracy': 50.32, 'per_class_accuracy': [91.1224, 85.9912, 24.4186, 39.505, 34.0122, 40.9193, 46.9729, 41.6342, 33.7782, 60.0595], 'loss': 1.8771477966308594}\n",
            "OCS >> Task 2: {'accuracy': 60.73, 'per_class_accuracy': [93.1633, 94.4493, 33.0426, 55.8416, 45.9267, 45.0673, 56.8894, 61.5759, 51.0267, 64.9158], 'loss': 1.3748370653152466}\n",
            "OCS >> Task 3: {'accuracy': 73.52, 'per_class_accuracy': [93.6735, 93.0396, 55.3295, 69.0099, 70.0611, 66.704, 67.0146, 74.8054, 69.4045, 73.3399], 'loss': 0.8485950293540955}\n",
            "OCS >> Task 4: {'accuracy': 82.24, 'per_class_accuracy': [94.7959, 92.9515, 71.5116, 84.8515, 82.7902, 78.6996, 80.0626, 74.2218, 79.5688, 81.7641], 'loss': 0.5595291718959808}\n",
            "OCS >> Task 5: {'accuracy': 90.83, 'per_class_accuracy': [96.4286, 97.0925, 90.2132, 93.7624, 83.7067, 88.0045, 88.5177, 90.0778, 90.2464, 88.999], 'loss': 0.29941865031719206}\n",
            "OCS >> Task 6: {'accuracy': 94.27, 'per_class_accuracy': [97.1429, 99.0308, 94.9612, 93.8614, 94.7047, 95.852, 95.6159, 92.4125, 92.8131, 86.0258], 'loss': 0.1896863977253437}\n",
            "OCS >> Task 7: {'accuracy': 96.81, 'per_class_accuracy': [98.6735, 99.1189, 97.2868, 95.8416, 96.4358, 97.7578, 96.4509, 95.7198, 95.2772, 95.3419], 'loss': 0.12509541233479976}\n",
            "OCS >> (average accuracy): 78.38857142857144\n",
            "OCS >> (Forgetting): 0.2254166666666667\n",
            "Maximum per-task accuracies: [97.86]\n",
            "\n",
            "---- Task 8 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 43.26\n",
            "Epoch 0.05 >> (class accuracy): [96.8367, 62.7313, 0.0969, 81.7822, 1.7312, 0.0, 86.3257, 84.6304, 12.731, 0.0]\n",
            "Epoch 0.1 >> (per-task accuracy): 71.0\n",
            "Epoch 0.1 >> (class accuracy): [94.4898, 94.2731, 54.845, 80.8911, 70.9776, 37.7803, 88.4134, 83.3658, 64.6817, 34.9851]\n",
            "Epoch 0.15 >> (per-task accuracy): 78.64\n",
            "Epoch 0.15 >> (class accuracy): [93.0612, 96.2115, 63.9535, 81.5842, 77.6986, 59.417, 86.3257, 83.2685, 78.9528, 62.5372]\n",
            "Epoch 0.2 >> (per-task accuracy): 81.68\n",
            "Epoch 0.2 >> (class accuracy): [93.6735, 97.0925, 72.7713, 85.9406, 82.4847, 72.4215, 88.9353, 83.6576, 73.5113, 63.9247]\n",
            "Epoch 0.25 >> (per-task accuracy): 83.64\n",
            "Epoch 0.25 >> (class accuracy): [91.2245, 96.3877, 77.7132, 87.0297, 83.6049, 70.6278, 88.4134, 84.5331, 80.9035, 73.3399]\n",
            "Epoch 0.3 >> (per-task accuracy): 85.77\n",
            "Epoch 0.3 >> (class accuracy): [94.898, 96.652, 78.7791, 86.0396, 85.7434, 81.5022, 87.4739, 84.8249, 80.3901, 79.8811]\n",
            "Epoch 0.35 >> (per-task accuracy): 87.57\n",
            "Epoch 0.35 >> (class accuracy): [95.102, 97.3568, 77.1318, 90.495, 87.5764, 84.3049, 88.6221, 86.6732, 81.9302, 85.2329]\n",
            "Epoch 0.4 >> (per-task accuracy): 89.02\n",
            "Epoch 0.4 >> (class accuracy): [94.3878, 97.3568, 83.8178, 91.2871, 90.835, 89.4619, 90.7098, 87.8405, 79.6715, 83.9445]\n",
            "Epoch 0.45 >> (per-task accuracy): 90.46\n",
            "Epoch 0.45 >> (class accuracy): [95.6122, 97.8855, 82.1705, 89.3069, 90.224, 91.704, 92.0668, 89.3969, 87.577, 88.2061]\n",
            "Epoch 0.5 >> (per-task accuracy): 91.91\n",
            "Epoch 0.5 >> (class accuracy): [95.9184, 98.1498, 86.7248, 93.1683, 90.835, 90.9193, 92.5887, 90.0778, 88.809, 91.1794]\n",
            "Epoch 0.55 >> (per-task accuracy): 92.42\n",
            "Epoch 0.55 >> (class accuracy): [96.1224, 97.4449, 88.2752, 91.6832, 92.9735, 93.6099, 94.2589, 91.0506, 89.3224, 89.1972]\n",
            "Epoch 0.6 >> (per-task accuracy): 93.43\n",
            "Epoch 0.6 >> (class accuracy): [96.5306, 97.6211, 91.1822, 92.7723, 94.1955, 93.9462, 94.1545, 91.8288, 91.2731, 90.4856]\n",
            "Epoch 0.65 >> (per-task accuracy): 93.7\n",
            "Epoch 0.65 >> (class accuracy): [97.0408, 97.533, 92.7326, 93.4653, 93.4827, 94.843, 94.2589, 91.6342, 90.3491, 91.3776]\n",
            "Epoch 0.7 >> (per-task accuracy): 94.12\n",
            "Epoch 0.7 >> (class accuracy): [96.8367, 97.7093, 92.4419, 92.9703, 94.7047, 96.1883, 93.5282, 92.7043, 91.9918, 91.9722]\n",
            "Epoch 0.75 >> (per-task accuracy): 94.65\n",
            "Epoch 0.75 >> (class accuracy): [97.7551, 97.7093, 94.0891, 92.8713, 95.6212, 96.1883, 94.2589, 92.3152, 92.9158, 92.666]\n",
            "Epoch 0.8 >> (per-task accuracy): 95.07\n",
            "Epoch 0.8 >> (class accuracy): [97.9592, 98.1498, 94.3798, 94.8515, 95.4175, 96.1883, 94.7808, 93.4825, 92.1971, 93.0624]\n",
            "Epoch 0.85 >> (per-task accuracy): 95.37\n",
            "Epoch 0.85 >> (class accuracy): [98.2653, 98.2379, 94.2829, 94.9505, 95.8248, 96.1883, 94.8852, 93.1907, 94.0452, 93.6571]\n",
            "Epoch 0.9 >> (per-task accuracy): 95.63\n",
            "Epoch 0.9 >> (class accuracy): [98.3673, 98.326, 95.155, 95.0495, 96.334, 96.5247, 95.3027, 93.677, 94.1478, 93.2607]\n",
            "Epoch 0.95 >> (per-task accuracy): 95.94\n",
            "Epoch 0.95 >> (class accuracy): [98.2653, 98.4141, 95.3488, 95.9406, 96.334, 96.3004, 95.4071, 93.8716, 94.2505, 95.0446]\n",
            "Epoch 1.0 >> (per-task accuracy): 96.16\n",
            "Epoch 1.0 >> (class accuracy): [98.8776, 98.5903, 96.4147, 95.9406, 96.334, 97.1973, 95.4071, 93.8716, 93.5318, 95.2428]\n",
            "OCS >> Task 1: {'accuracy': 44.08, 'per_class_accuracy': [89.5918, 64.141, 15.407, 31.8812, 40.9369, 29.0359, 67.9541, 33.1712, 30.6982, 36.5709], 'loss': 2.020583715057373}\n",
            "OCS >> Task 2: {'accuracy': 50.2, 'per_class_accuracy': [91.9388, 75.7709, 15.407, 44.2574, 48.3707, 27.2422, 69.3111, 46.9844, 45.6879, 33.9941], 'loss': 1.723692911720276}\n",
            "OCS >> Task 3: {'accuracy': 59.62, 'per_class_accuracy': [93.7755, 74.8018, 27.2287, 60.0, 65.4786, 33.0717, 74.3215, 57.1012, 54.7228, 53.221], 'loss': 1.2851397254943848}\n",
            "OCS >> Task 4: {'accuracy': 67.55, 'per_class_accuracy': [94.6939, 74.5374, 38.0814, 77.5248, 72.7088, 51.2332, 79.2276, 56.5175, 66.8378, 63.7265], 'loss': 0.9949216128349304}\n",
            "OCS >> Task 5: {'accuracy': 80.25, 'per_class_accuracy': [95.6122, 82.3789, 66.0853, 89.4059, 80.2444, 72.1973, 81.3152, 74.2218, 77.5154, 83.1516], 'loss': 0.5990013388633728}\n",
            "OCS >> Task 6: {'accuracy': 87.25, 'per_class_accuracy': [97.551, 96.1233, 86.3372, 92.4752, 91.1405, 84.9776, 91.3361, 80.3502, 84.9076, 66.6006], 'loss': 0.3849944004058838}\n",
            "OCS >> Task 7: {'accuracy': 93.06, 'per_class_accuracy': [97.7551, 98.6784, 93.5078, 95.2475, 92.057, 91.704, 94.572, 88.4241, 89.0144, 88.8999], 'loss': 0.24680617082118989}\n",
            "OCS >> Task 8: {'accuracy': 96.16, 'per_class_accuracy': [98.8776, 98.5903, 96.4147, 95.9406, 96.334, 97.1973, 95.4071, 93.8716, 93.5318, 95.2428], 'loss': 0.17628266899585723}\n",
            "OCS >> (average accuracy): 72.27125000000001\n",
            "OCS >> (Forgetting): 0.2900142857142858\n",
            "Maximum per-task accuracies: [97.86]\n",
            "\n",
            "---- Task 9 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 25.62\n",
            "Epoch 0.05 >> (class accuracy): [41.5306, 2.3789, 0.0, 94.4554, 0.0, 0.0, 16.9102, 98.4436, 0.0, 0.0]\n",
            "Epoch 0.1 >> (per-task accuracy): 56.91\n",
            "Epoch 0.1 >> (class accuracy): [96.2245, 89.5154, 42.5388, 89.604, 4.7862, 0.2242, 88.6221, 93.2879, 42.2998, 11.7939]\n",
            "Epoch 0.15 >> (per-task accuracy): 74.34\n",
            "Epoch 0.15 >> (class accuracy): [93.7755, 97.4449, 63.8566, 77.9208, 66.0896, 40.583, 89.5616, 87.9377, 64.8871, 55.3023]\n",
            "Epoch 0.2 >> (per-task accuracy): 78.47\n",
            "Epoch 0.2 >> (class accuracy): [95.5102, 96.9163, 69.9612, 84.2574, 68.8391, 54.3722, 86.6388, 84.6304, 70.1232, 68.781]\n",
            "Epoch 0.25 >> (per-task accuracy): 80.76\n",
            "Epoch 0.25 >> (class accuracy): [95.2041, 97.3568, 73.5465, 82.3762, 68.2281, 64.4619, 87.9958, 83.8521, 72.9979, 77.8989]\n",
            "Epoch 0.3 >> (per-task accuracy): 81.97\n",
            "Epoch 0.3 >> (class accuracy): [94.6939, 96.4758, 72.1899, 84.3564, 74.7454, 73.5426, 87.3695, 90.2724, 72.2793, 71.1596]\n",
            "Epoch 0.35 >> (per-task accuracy): 84.26\n",
            "Epoch 0.35 >> (class accuracy): [94.0816, 95.4185, 79.845, 82.3762, 77.3931, 76.6816, 88.309, 90.2724, 78.5421, 77.5025]\n",
            "Epoch 0.4 >> (per-task accuracy): 85.6\n",
            "Epoch 0.4 >> (class accuracy): [93.8776, 94.8899, 79.2636, 83.6634, 80.6517, 80.0448, 89.5616, 89.5914, 81.3142, 81.5659]\n",
            "Epoch 0.45 >> (per-task accuracy): 86.94\n",
            "Epoch 0.45 >> (class accuracy): [95.102, 96.0352, 79.845, 88.1188, 81.0591, 83.296, 90.0835, 91.1479, 78.6448, 84.5391]\n",
            "Epoch 0.5 >> (per-task accuracy): 88.2\n",
            "Epoch 0.5 >> (class accuracy): [95.5102, 95.859, 81.686, 87.0297, 83.6049, 85.5381, 90.1879, 92.2179, 83.7782, 85.4311]\n",
            "Epoch 0.55 >> (per-task accuracy): 88.72\n",
            "Epoch 0.55 >> (class accuracy): [94.3878, 95.2423, 84.3023, 84.4554, 87.9837, 91.3677, 91.5449, 92.4125, 78.9528, 86.0258]\n",
            "Epoch 0.6 >> (per-task accuracy): 89.86\n",
            "Epoch 0.6 >> (class accuracy): [94.4898, 96.9163, 86.3372, 87.0297, 87.2709, 91.704, 91.7537, 91.2451, 81.1088, 89.9901]\n",
            "Epoch 0.65 >> (per-task accuracy): 90.58\n",
            "Epoch 0.65 >> (class accuracy): [94.6939, 96.652, 87.1124, 85.7426, 88.391, 92.0404, 92.0668, 91.8288, 85.8316, 90.8821]\n",
            "Epoch 0.7 >> (per-task accuracy): 91.28\n",
            "Epoch 0.7 >> (class accuracy): [95.7143, 96.9163, 88.8566, 88.3168, 87.6782, 92.4888, 92.4843, 92.5097, 85.0103, 92.1705]\n",
            "Epoch 0.75 >> (per-task accuracy): 92.06\n",
            "Epoch 0.75 >> (class accuracy): [95.2041, 97.3568, 89.3411, 89.802, 90.1222, 93.2735, 92.38, 92.9961, 87.0637, 92.4678]\n",
            "Epoch 0.8 >> (per-task accuracy): 92.84\n",
            "Epoch 0.8 >> (class accuracy): [96.2245, 97.533, 91.2791, 89.3069, 91.9552, 94.5067, 93.0063, 93.2879, 88.9117, 91.9722]\n",
            "Epoch 0.85 >> (per-task accuracy): 93.32\n",
            "Epoch 0.85 >> (class accuracy): [95.5102, 97.9736, 91.2791, 91.7822, 93.9919, 94.6188, 94.1545, 93.0934, 88.809, 91.5758]\n",
            "Epoch 0.9 >> (per-task accuracy): 93.82\n",
            "Epoch 0.9 >> (class accuracy): [96.7347, 97.7974, 91.6667, 91.1881, 93.5845, 95.6278, 94.7808, 93.3852, 90.0411, 93.1615]\n",
            "Epoch 0.95 >> (per-task accuracy): 94.21\n",
            "Epoch 0.95 >> (class accuracy): [97.551, 98.0617, 92.5388, 92.6733, 92.5662, 95.9641, 94.9896, 93.1907, 91.4784, 92.8642]\n",
            "Epoch 1.0 >> (per-task accuracy): 94.66\n",
            "Epoch 1.0 >> (class accuracy): [97.551, 97.7093, 92.9264, 93.1683, 93.4827, 96.0762, 95.8246, 94.2607, 91.2731, 94.1526]\n",
            "OCS >> Task 1: {'accuracy': 42.42, 'per_class_accuracy': [72.7551, 58.2379, 12.8876, 36.0396, 54.6843, 33.1839, 48.9562, 45.8171, 20.9446, 39.0486], 'loss': 1.930221872138977}\n",
            "OCS >> Task 2: {'accuracy': 47.9, 'per_class_accuracy': [76.1224, 65.022, 16.7636, 49.3069, 59.8778, 32.7354, 56.3674, 54.0856, 30.5955, 35.778], 'loss': 1.6951489469528198}\n",
            "OCS >> Task 3: {'accuracy': 56.09, 'per_class_accuracy': [79.3878, 69.0749, 19.7674, 62.5743, 67.0061, 34.8655, 71.7119, 62.0623, 38.2957, 53.9148], 'loss': 1.4086173473358155}\n",
            "OCS >> Task 4: {'accuracy': 60.9, 'per_class_accuracy': [83.3673, 64.7577, 30.814, 72.1782, 71.3849, 44.6188, 77.8706, 60.7004, 46.9199, 55.996], 'loss': 1.2050865942955018}\n",
            "OCS >> Task 5: {'accuracy': 69.31, 'per_class_accuracy': [82.551, 71.1894, 49.6124, 82.5743, 70.9776, 50.7848, 79.5407, 72.9572, 58.2136, 73.2408], 'loss': 0.9042622709751129}\n",
            "OCS >> Task 6: {'accuracy': 78.74, 'per_class_accuracy': [90.6122, 92.2467, 66.8605, 82.9703, 86.3544, 66.0314, 85.6994, 77.6265, 72.3819, 64.4202], 'loss': 0.6220305374145508}\n",
            "OCS >> Task 7: {'accuracy': 86.02, 'per_class_accuracy': [94.3878, 94.978, 82.7519, 91.0891, 78.5132, 80.3812, 89.3528, 86.0895, 78.9528, 81.9623], 'loss': 0.4527112298488617}\n",
            "OCS >> Task 8: {'accuracy': 90.43, 'per_class_accuracy': [93.1633, 95.6828, 87.1124, 92.1782, 87.8819, 90.3587, 94.3633, 93.677, 84.2916, 84.9356], 'loss': 0.33787152891159056}\n",
            "OCS >> Task 9: {'accuracy': 94.66, 'per_class_accuracy': [97.551, 97.7093, 92.9264, 93.1683, 93.4827, 96.0762, 95.8246, 94.2607, 91.2731, 94.1526], 'loss': 0.24391351362466812}\n",
            "OCS >> (average accuracy): 69.60777777777777\n",
            "OCS >> (Forgetting): 0.3138375\n",
            "Maximum per-task accuracies: [97.86]\n",
            "\n",
            "---- Task 10 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 30.65\n",
            "Epoch 0.05 >> (class accuracy): [0.0, 99.3833, 0.0, 80.8911, 0.0, 0.0, 0.0, 90.3696, 0.0, 18.9296]\n",
            "Epoch 0.1 >> (per-task accuracy): 53.1\n",
            "Epoch 0.1 >> (class accuracy): [97.6531, 93.4802, 22.1899, 89.703, 0.0, 0.8969, 51.357, 85.1167, 18.1725, 59.9604]\n",
            "Epoch 0.15 >> (per-task accuracy): 66.21\n",
            "Epoch 0.15 >> (class accuracy): [96.1224, 97.0044, 58.2364, 78.0198, 18.6354, 9.6413, 87.5783, 84.4358, 65.6057, 56.888]\n",
            "Epoch 0.2 >> (per-task accuracy): 73.28\n",
            "Epoch 0.2 >> (class accuracy): [93.7755, 96.1233, 72.8682, 63.4653, 50.7128, 48.8789, 88.4134, 79.1829, 63.2444, 70.7631]\n",
            "Epoch 0.25 >> (per-task accuracy): 75.97\n",
            "Epoch 0.25 >> (class accuracy): [90.0, 94.5374, 72.7713, 74.0594, 58.1466, 56.278, 86.8476, 79.7665, 69.3018, 73.6373]\n",
            "Epoch 0.3 >> (per-task accuracy): 78.24\n",
            "Epoch 0.3 >> (class accuracy): [92.449, 93.9207, 68.2171, 74.2574, 60.5906, 65.9193, 86.952, 77.821, 75.462, 83.9445]\n",
            "Epoch 0.35 >> (per-task accuracy): 81.0\n",
            "Epoch 0.35 >> (class accuracy): [92.6531, 94.8899, 74.5155, 72.7723, 74.2363, 72.0852, 88.8309, 83.8521, 75.0513, 78.7909]\n",
            "Epoch 0.4 >> (per-task accuracy): 83.4\n",
            "Epoch 0.4 >> (class accuracy): [92.6531, 95.6828, 79.9419, 76.3366, 76.4766, 78.6996, 88.4134, 86.6732, 77.9261, 79.2864]\n",
            "Epoch 0.45 >> (per-task accuracy): 84.58\n",
            "Epoch 0.45 >> (class accuracy): [93.5714, 97.4449, 81.1047, 82.1782, 79.3279, 75.7848, 89.4572, 87.1595, 76.7967, 80.4757]\n",
            "Epoch 0.5 >> (per-task accuracy): 85.54\n",
            "Epoch 0.5 >> (class accuracy): [91.2245, 96.3877, 81.3953, 77.1287, 80.8554, 86.5471, 90.501, 88.2296, 78.8501, 83.1516]\n",
            "Epoch 0.55 >> (per-task accuracy): 87.02\n",
            "Epoch 0.55 >> (class accuracy): [92.551, 97.533, 83.3333, 80.099, 83.8086, 86.6592, 90.3967, 89.3969, 81.7248, 83.449]\n",
            "Epoch 0.6 >> (per-task accuracy): 87.91\n",
            "Epoch 0.6 >> (class accuracy): [93.2653, 97.0044, 83.8178, 79.2079, 84.9287, 89.7982, 89.3528, 89.4942, 85.2156, 86.224]\n",
            "Epoch 0.65 >> (per-task accuracy): 88.88\n",
            "Epoch 0.65 >> (class accuracy): [93.9796, 97.4449, 82.3643, 83.3663, 84.1141, 91.0314, 91.023, 89.2023, 86.653, 88.999]\n",
            "Epoch 0.7 >> (per-task accuracy): 90.08\n",
            "Epoch 0.7 >> (class accuracy): [95.102, 97.3568, 85.8527, 86.3366, 86.4562, 89.4619, 90.9186, 90.5642, 87.885, 89.9901]\n",
            "Epoch 0.75 >> (per-task accuracy): 90.79\n",
            "Epoch 0.75 >> (class accuracy): [94.6939, 97.7093, 88.5659, 86.5347, 89.3075, 91.4798, 91.3361, 90.1751, 89.117, 88.3053]\n",
            "Epoch 0.8 >> (per-task accuracy): 91.23\n",
            "Epoch 0.8 >> (class accuracy): [95.6122, 97.7093, 87.6938, 86.8317, 88.0855, 91.2556, 92.2756, 90.3696, 89.7331, 92.0714]\n",
            "Epoch 0.85 >> (per-task accuracy): 91.91\n",
            "Epoch 0.85 >> (class accuracy): [95.6122, 97.8855, 89.3411, 87.4257, 89.5112, 92.2646, 93.737, 91.3424, 89.6304, 91.774]\n",
            "Epoch 0.9 >> (per-task accuracy): 92.39\n",
            "Epoch 0.9 >> (class accuracy): [96.5306, 97.6211, 89.5349, 88.6139, 90.4277, 92.9372, 93.737, 91.537, 90.2464, 92.2696]\n",
            "Epoch 0.95 >> (per-task accuracy): 92.94\n",
            "Epoch 0.95 >> (class accuracy): [96.6327, 97.8855, 90.6977, 89.703, 91.2424, 92.8251, 94.4676, 92.2179, 90.5544, 92.666]\n",
            "Epoch 1.0 >> (per-task accuracy): 93.32\n",
            "Epoch 1.0 >> (class accuracy): [96.9388, 98.1498, 91.6667, 90.198, 91.9552, 93.3857, 94.4676, 92.9961, 90.2464, 92.666]\n",
            "OCS >> Task 1: {'accuracy': 40.71, 'per_class_accuracy': [64.2857, 65.1101, 15.5039, 27.9208, 45.5193, 33.4081, 57.9332, 36.4786, 26.2834, 32.6065], 'loss': 1.967282162094116}\n",
            "OCS >> Task 2: {'accuracy': 46.28, 'per_class_accuracy': [70.8163, 82.2907, 18.5078, 31.2871, 49.2872, 37.1076, 60.5428, 47.9572, 34.0862, 27.0565], 'loss': 1.7724909837722778}\n",
            "OCS >> Task 3: {'accuracy': 50.98, 'per_class_accuracy': [68.5714, 78.326, 27.0349, 39.901, 54.5825, 39.0135, 65.6576, 50.5837, 37.7823, 44.995], 'loss': 1.5857520574569701}\n",
            "OCS >> Task 4: {'accuracy': 54.67, 'per_class_accuracy': [75.4082, 80.3524, 27.1318, 53.2673, 54.1752, 43.2735, 72.6514, 57.1984, 37.885, 42.3191], 'loss': 1.4671975244522095}\n",
            "OCS >> Task 5: {'accuracy': 60.96, 'per_class_accuracy': [70.4082, 87.4009, 41.2791, 61.3861, 60.2851, 49.4395, 65.762, 71.2062, 42.0945, 55.7978], 'loss': 1.2239005158424376}\n",
            "OCS >> Task 6: {'accuracy': 69.07, 'per_class_accuracy': [80.9184, 96.9163, 51.2597, 76.8317, 75.9674, 57.9596, 73.382, 73.249, 55.0308, 44.995], 'loss': 0.958653727722168}\n",
            "OCS >> Task 7: {'accuracy': 76.11, 'per_class_accuracy': [85.9184, 97.9736, 65.6008, 82.6733, 70.4684, 69.1704, 78.0793, 80.4475, 67.6591, 59.663], 'loss': 0.7255960218429566}\n",
            "OCS >> Task 8: {'accuracy': 84.21, 'per_class_accuracy': [86.2245, 97.7974, 81.0078, 86.6337, 80.4481, 79.8206, 87.0564, 91.6342, 78.8501, 70.2676], 'loss': 0.4992664452552795}\n",
            "OCS >> Task 9: {'accuracy': 89.37, 'per_class_accuracy': [91.8367, 98.5903, 88.3721, 86.5347, 82.7902, 86.0987, 93.5282, 93.8716, 84.2916, 86.1249], 'loss': 0.36923584728240966}\n",
            "OCS >> Task 10: {'accuracy': 93.32, 'per_class_accuracy': [96.9388, 98.1498, 91.6667, 90.198, 91.9552, 93.3857, 94.4676, 92.9961, 90.2464, 92.666], 'loss': 0.28813269407749176}\n",
            "OCS >> (average accuracy): 66.56800000000001\n",
            "OCS >> (Forgetting): 0.3426444444444444\n",
            "Maximum per-task accuracies: [97.86]\n",
            "\n",
            "---- Task 11 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 34.72\n",
            "Epoch 0.05 >> (class accuracy): [73.3673, 66.9604, 5.9109, 98.8119, 0.0, 0.6726, 0.6263, 77.821, 9.9589, 2.4777]\n",
            "Epoch 0.1 >> (per-task accuracy): 24.46\n",
            "Epoch 0.1 >> (class accuracy): [99.898, 25.4626, 2.8101, 67.4257, 0.0, 0.0, 4.071, 24.8054, 4.3121, 13.0823]\n",
            "Epoch 0.15 >> (per-task accuracy): 58.19\n",
            "Epoch 0.15 >> (class accuracy): [97.6531, 76.8282, 47.9651, 91.3861, 0.0, 0.0, 74.8434, 70.9144, 43.6345, 69.4747]\n",
            "Epoch 0.2 >> (per-task accuracy): 66.68\n",
            "Epoch 0.2 >> (class accuracy): [95.8163, 93.5683, 64.9225, 86.6337, 5.0916, 11.5471, 86.4301, 81.6148, 69.1992, 62.2398]\n",
            "Epoch 0.25 >> (per-task accuracy): 73.13\n",
            "Epoch 0.25 >> (class accuracy): [94.0816, 97.2687, 74.9031, 76.3366, 38.4929, 37.8924, 88.9353, 83.0739, 69.0965, 64.222]\n",
            "Epoch 0.3 >> (per-task accuracy): 75.44\n",
            "Epoch 0.3 >> (class accuracy): [94.2857, 96.4758, 74.031, 78.0198, 48.8798, 47.7578, 87.7871, 81.7121, 68.7885, 70.9613]\n",
            "Epoch 0.35 >> (per-task accuracy): 77.78\n",
            "Epoch 0.35 >> (class accuracy): [92.9592, 95.9471, 78.7791, 75.0495, 58.8595, 57.8475, 87.4739, 81.6148, 72.7926, 72.0515]\n",
            "Epoch 0.4 >> (per-task accuracy): 79.17\n",
            "Epoch 0.4 >> (class accuracy): [92.3469, 95.7709, 77.1318, 77.5248, 67.8208, 57.287, 87.7871, 84.3385, 75.5647, 71.8533]\n",
            "Epoch 0.45 >> (per-task accuracy): 80.27\n",
            "Epoch 0.45 >> (class accuracy): [92.9592, 95.3304, 78.876, 81.1881, 72.7088, 62.8924, 85.9081, 86.284, 72.1766, 70.5649]\n",
            "Epoch 0.5 >> (per-task accuracy): 81.38\n",
            "Epoch 0.5 >> (class accuracy): [92.8571, 95.3304, 80.0388, 79.505, 72.0978, 68.2735, 86.6388, 85.1167, 74.23, 76.5114]\n",
            "Epoch 0.55 >> (per-task accuracy): 82.24\n",
            "Epoch 0.55 >> (class accuracy): [91.2245, 95.5947, 79.4574, 80.7921, 75.8656, 73.4305, 86.5344, 86.284, 77.2074, 73.439]\n",
            "Epoch 0.6 >> (per-task accuracy): 82.92\n",
            "Epoch 0.6 >> (class accuracy): [91.5306, 95.6828, 81.2984, 78.4158, 77.5967, 75.3363, 86.7432, 89.5914, 78.3368, 72.2498]\n",
            "Epoch 0.65 >> (per-task accuracy): 83.65\n",
            "Epoch 0.65 >> (class accuracy): [92.449, 96.4758, 80.1357, 79.802, 78.6151, 78.2511, 87.3695, 87.3541, 77.0021, 76.9078]\n",
            "Epoch 0.7 >> (per-task accuracy): 84.34\n",
            "Epoch 0.7 >> (class accuracy): [91.2245, 96.2996, 80.7171, 75.3465, 81.9756, 84.1928, 87.5783, 89.786, 79.6715, 75.223]\n",
            "Epoch 0.75 >> (per-task accuracy): 85.29\n",
            "Epoch 0.75 >> (class accuracy): [90.7143, 96.8282, 81.686, 79.4059, 78.9206, 83.8565, 88.309, 88.3268, 80.8008, 82.4579]\n",
            "Epoch 0.8 >> (per-task accuracy): 86.48\n",
            "Epoch 0.8 >> (class accuracy): [91.8367, 96.8282, 82.655, 80.099, 82.9939, 87.3318, 87.7871, 89.9805, 81.2115, 82.8543]\n",
            "Epoch 0.85 >> (per-task accuracy): 87.28\n",
            "Epoch 0.85 >> (class accuracy): [93.1633, 96.8282, 82.5581, 82.0792, 82.6884, 87.3318, 89.8747, 89.1051, 82.9569, 85.1338]\n",
            "Epoch 0.9 >> (per-task accuracy): 87.96\n",
            "Epoch 0.9 >> (class accuracy): [92.449, 97.1806, 84.1085, 82.9703, 83.2994, 88.1166, 90.7098, 89.3969, 83.4702, 86.8186]\n",
            "Epoch 0.95 >> (per-task accuracy): 88.66\n",
            "Epoch 0.95 >> (class accuracy): [93.3673, 97.1806, 85.8527, 83.4653, 85.4379, 88.7892, 90.6054, 90.1751, 85.0103, 85.7284]\n",
            "Epoch 1.0 >> (per-task accuracy): 89.09\n",
            "Epoch 1.0 >> (class accuracy): [92.9592, 97.0044, 85.3682, 81.6832, 86.3544, 90.3587, 91.4405, 90.9533, 86.037, 88.0079]\n",
            "OCS >> Task 1: {'accuracy': 39.81, 'per_class_accuracy': [59.898, 74.5374, 10.7558, 22.3762, 32.7902, 27.3543, 55.428, 30.8366, 33.2649, 46.8781], 'loss': 1.9735629642486572}\n",
            "OCS >> Task 2: {'accuracy': 43.68, 'per_class_accuracy': [65.6122, 76.1233, 8.9147, 20.495, 38.0855, 31.6143, 62.1086, 42.8988, 42.4025, 45.2924], 'loss': 1.8595209186553956}\n",
            "OCS >> Task 3: {'accuracy': 49.23, 'per_class_accuracy': [65.3061, 80.9692, 9.7868, 30.396, 43.0754, 35.2018, 63.8831, 50.6809, 49.5893, 59.7621], 'loss': 1.6724087259292602}\n",
            "OCS >> Task 4: {'accuracy': 49.99, 'per_class_accuracy': [69.5918, 76.5639, 14.438, 36.5347, 39.4094, 41.3677, 65.6576, 52.821, 43.1211, 57.6809], 'loss': 1.6398734197616578}\n",
            "OCS >> Task 5: {'accuracy': 53.86, 'per_class_accuracy': [66.7347, 86.8722, 26.1628, 42.7723, 43.3809, 40.9193, 61.691, 65.7588, 40.9651, 58.1764], 'loss': 1.4652074244499207}\n",
            "OCS >> Task 6: {'accuracy': 58.98, 'per_class_accuracy': [75.8163, 95.5947, 30.9109, 50.7921, 49.7963, 47.0852, 67.5365, 70.1362, 49.076, 47.8692], 'loss': 1.26038182220459}\n",
            "OCS >> Task 7: {'accuracy': 64.59, 'per_class_accuracy': [82.2449, 97.1806, 43.5078, 60.198, 50.3055, 59.6413, 68.9979, 76.8482, 56.2628, 46.3826], 'loss': 1.0540123637199401}\n",
            "OCS >> Task 8: {'accuracy': 73.71, 'per_class_accuracy': [78.7755, 96.5639, 63.469, 70.495, 61.0998, 73.7668, 79.2276, 84.7276, 69.0965, 56.9871], 'loss': 0.793598392868042}\n",
            "OCS >> Task 9: {'accuracy': 81.36, 'per_class_accuracy': [87.3469, 97.8855, 76.2597, 76.2376, 72.0978, 79.4843, 86.4301, 89.3969, 77.1047, 69.0783], 'loss': 0.588241548538208}\n",
            "OCS >> Task 10: {'accuracy': 87.19, 'per_class_accuracy': [92.1429, 96.8282, 82.8488, 82.0792, 81.5682, 87.5561, 90.9186, 90.4669, 84.7023, 81.7641], 'loss': 0.45341082038879393}\n",
            "OCS >> Task 11: {'accuracy': 89.09, 'per_class_accuracy': [92.9592, 97.0044, 85.3682, 81.6832, 86.3544, 90.3587, 91.4405, 90.9533, 86.037, 88.0079], 'loss': 0.407164866399765}\n",
            "OCS >> (average accuracy): 62.862727272727284\n",
            "OCS >> (Forgetting): 0.3762\n",
            "Maximum per-task accuracies: [97.86]\n",
            "\n",
            "---- Task 12 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 20.87\n",
            "Epoch 0.05 >> (class accuracy): [68.5714, 0.0, 3.2946, 99.4059, 0.0, 0.0, 22.2338, 15.0778, 0.0, 0.892]\n",
            "Epoch 0.1 >> (per-task accuracy): 13.96\n",
            "Epoch 0.1 >> (class accuracy): [100.0, 0.0, 0.0, 39.703, 0.0, 0.0, 1.2526, 0.0973, 0.0, 0.1982]\n",
            "Epoch 0.15 >> (per-task accuracy): 42.41\n",
            "Epoch 0.15 >> (class accuracy): [98.8776, 10.837, 29.4574, 85.0495, 0.0, 0.0, 58.6639, 57.6848, 25.77, 57.4827]\n",
            "Epoch 0.2 >> (per-task accuracy): 60.42\n",
            "Epoch 0.2 >> (class accuracy): [96.7347, 70.9251, 54.4574, 86.9307, 8.9613, 0.0, 83.9248, 65.5642, 56.5708, 72.5471]\n",
            "Epoch 0.25 >> (per-task accuracy): 68.97\n",
            "Epoch 0.25 >> (class accuracy): [94.898, 88.8987, 63.9535, 79.4059, 38.4929, 15.9193, 89.3528, 71.3035, 69.4045, 70.4658]\n",
            "Epoch 0.3 >> (per-task accuracy): 72.31\n",
            "Epoch 0.3 >> (class accuracy): [94.2857, 93.5683, 69.7674, 78.6139, 45.5193, 33.7444, 87.5783, 79.1829, 69.0965, 65.114]\n",
            "Epoch 0.35 >> (per-task accuracy): 74.39\n",
            "Epoch 0.35 >> (class accuracy): [94.6939, 94.6256, 75.3876, 75.8416, 45.8248, 43.722, 85.0731, 78.5992, 71.7659, 72.448]\n",
            "Epoch 0.4 >> (per-task accuracy): 77.59\n",
            "Epoch 0.4 >> (class accuracy): [93.6735, 94.978, 77.0349, 76.8317, 61.4053, 56.3901, 87.8914, 82.4903, 69.7125, 71.0605]\n",
            "Epoch 0.45 >> (per-task accuracy): 77.5\n",
            "Epoch 0.45 >> (class accuracy): [93.6735, 95.7709, 76.4535, 75.7426, 51.5275, 62.5561, 87.4739, 81.5175, 71.1499, 75.1239]\n",
            "Epoch 0.5 >> (per-task accuracy): 78.79\n",
            "Epoch 0.5 >> (class accuracy): [94.2857, 95.9471, 75.969, 80.9901, 55.9063, 61.0987, 89.0397, 81.6148, 72.5873, 76.4123]\n",
            "Epoch 0.55 >> (per-task accuracy): 80.14\n",
            "Epoch 0.55 >> (class accuracy): [93.6735, 95.7709, 76.938, 78.4158, 63.9511, 66.1435, 87.2651, 81.9066, 74.9487, 78.9891]\n",
            "Epoch 0.6 >> (per-task accuracy): 81.65\n",
            "Epoch 0.6 >> (class accuracy): [93.7755, 96.2115, 78.4884, 79.4059, 70.8758, 69.843, 87.9958, 85.4086, 74.846, 76.6105]\n",
            "Epoch 0.65 >> (per-task accuracy): 82.63\n",
            "Epoch 0.65 >> (class accuracy): [92.7551, 96.1233, 79.5543, 80.0, 74.2363, 74.2152, 87.3695, 85.0195, 78.4394, 76.115]\n",
            "Epoch 0.7 >> (per-task accuracy): 83.46\n",
            "Epoch 0.7 >> (class accuracy): [92.551, 96.2115, 78.6822, 82.2772, 74.8473, 76.1211, 88.5177, 87.1595, 76.5914, 79.2864]\n",
            "Epoch 0.75 >> (per-task accuracy): 84.35\n",
            "Epoch 0.75 >> (class accuracy): [92.551, 96.4758, 80.7171, 83.3663, 76.8839, 76.009, 87.8914, 87.9377, 78.4394, 80.773]\n",
            "Epoch 0.8 >> (per-task accuracy): 85.29\n",
            "Epoch 0.8 >> (class accuracy): [93.7755, 96.3877, 80.814, 83.5644, 78.7169, 76.9058, 88.7265, 88.9105, 81.0062, 81.8632]\n",
            "Epoch 0.85 >> (per-task accuracy): 85.68\n",
            "Epoch 0.85 >> (class accuracy): [93.4694, 95.859, 80.1357, 85.1485, 81.0591, 79.2601, 87.8914, 88.5214, 81.2115, 82.3588]\n",
            "Epoch 0.9 >> (per-task accuracy): 86.7\n",
            "Epoch 0.9 >> (class accuracy): [93.4694, 96.4758, 82.2674, 85.4455, 83.0957, 81.5022, 89.1441, 89.5914, 80.5955, 83.6472]\n",
            "Epoch 0.95 >> (per-task accuracy): 87.43\n",
            "Epoch 0.95 >> (class accuracy): [93.8776, 96.2115, 82.8488, 85.4455, 84.9287, 83.7444, 89.7704, 89.9805, 82.0329, 84.0436]\n",
            "Epoch 1.0 >> (per-task accuracy): 88.06\n",
            "Epoch 1.0 >> (class accuracy): [94.1837, 96.1233, 83.8178, 85.6436, 86.558, 84.5291, 90.1879, 91.1479, 82.9569, 84.1427]\n",
            "OCS >> Task 1: {'accuracy': 43.16, 'per_class_accuracy': [68.4694, 84.7577, 10.1744, 41.2871, 41.1405, 7.5112, 57.8288, 46.7899, 27.8234, 38.0575], 'loss': 1.9169556282043456}\n",
            "OCS >> Task 2: {'accuracy': 44.49, 'per_class_accuracy': [70.9184, 82.3789, 9.8837, 31.5842, 40.1222, 6.1659, 61.1691, 51.9455, 41.3758, 42.22], 'loss': 1.876648010635376}\n",
            "OCS >> Task 3: {'accuracy': 49.77, 'per_class_accuracy': [70.2041, 90.4846, 11.3372, 39.0099, 44.1955, 9.3049, 66.1795, 53.8911, 50.1027, 55.3023], 'loss': 1.6906695169448853}\n",
            "OCS >> Task 4: {'accuracy': 48.72, 'per_class_accuracy': [69.4898, 77.8855, 13.469, 40.6931, 39.613, 10.426, 70.3549, 52.5292, 53.4908, 53.5183], 'loss': 1.7222841009140015}\n",
            "OCS >> Task 5: {'accuracy': 52.35, 'per_class_accuracy': [71.3265, 90.1322, 22.2868, 48.0198, 42.1589, 11.8834, 64.7182, 63.2296, 54.6201, 47.1754], 'loss': 1.580217148399353}\n",
            "OCS >> Task 6: {'accuracy': 55.47, 'per_class_accuracy': [75.8163, 95.3304, 27.8101, 55.3465, 46.0285, 18.3857, 69.3111, 67.607, 49.692, 41.328], 'loss': 1.4232975855827332}\n",
            "OCS >> Task 7: {'accuracy': 58.9, 'per_class_accuracy': [85.8163, 97.2687, 36.0465, 61.7822, 43.3809, 25.3363, 70.7724, 76.751, 52.8747, 31.219], 'loss': 1.2623751668930054}\n",
            "OCS >> Task 8: {'accuracy': 65.44, 'per_class_accuracy': [86.7347, 97.0925, 52.0349, 73.1683, 49.7963, 36.3229, 76.7223, 80.7393, 57.2895, 37.6611], 'loss': 1.0263098838806153}\n",
            "OCS >> Task 9: {'accuracy': 72.46, 'per_class_accuracy': [91.3265, 97.4449, 62.2093, 77.1287, 62.3218, 48.3184, 83.0898, 85.7977, 60.5749, 50.8424], 'loss': 0.8416326667785644}\n",
            "OCS >> Task 10: {'accuracy': 78.43, 'per_class_accuracy': [91.7347, 97.1806, 73.062, 81.3861, 70.2648, 56.8386, 86.952, 88.2296, 71.0472, 63.0327], 'loss': 0.6709229592323304}\n",
            "OCS >> Task 11: {'accuracy': 85.79, 'per_class_accuracy': [92.3469, 97.2687, 82.9457, 85.3465, 81.4664, 75.4484, 90.3967, 92.2179, 79.0554, 78.7909], 'loss': 0.5083328696250915}\n",
            "OCS >> Task 12: {'accuracy': 88.06, 'per_class_accuracy': [94.1837, 96.1233, 83.8178, 85.6436, 86.558, 84.5291, 90.1879, 91.1479, 82.9569, 84.1427], 'loss': 0.46023737454414365}\n",
            "OCS >> (average accuracy): 61.919999999999995\n",
            "OCS >> (Forgetting): 0.3831636363636364\n",
            "Maximum per-task accuracies: [97.86]\n",
            "\n",
            "---- Task 13 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 21.53\n",
            "Epoch 0.05 >> (class accuracy): [97.1429, 41.9383, 70.0581, 0.0, 0.2037, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.1 >> (per-task accuracy): 25.8\n",
            "Epoch 0.1 >> (class accuracy): [92.9592, 10.6608, 23.3527, 98.6139, 0.0, 0.0, 10.4384, 0.0, 0.0, 20.9118]\n",
            "Epoch 0.15 >> (per-task accuracy): 47.81\n",
            "Epoch 0.15 >> (class accuracy): [94.3878, 67.6652, 53.3915, 84.8515, 6.9246, 0.0, 79.4363, 0.0, 64.8871, 21.7047]\n",
            "Epoch 0.2 >> (per-task accuracy): 46.9\n",
            "Epoch 0.2 >> (class accuracy): [90.0, 8.6344, 58.7209, 85.3465, 90.5295, 9.6413, 41.3361, 62.1595, 23.8193, 0.0]\n",
            "Epoch 0.25 >> (per-task accuracy): 57.44\n",
            "Epoch 0.25 >> (class accuracy): [85.9184, 93.5683, 66.376, 71.4851, 10.1833, 18.4978, 77.7662, 17.1206, 48.9733, 76.3132]\n",
            "Epoch 0.3 >> (per-task accuracy): 67.86\n",
            "Epoch 0.3 >> (class accuracy): [82.9592, 93.3921, 71.4147, 60.099, 61.0998, 39.0135, 84.1336, 77.5292, 68.5832, 34.6878]\n",
            "Epoch 0.35 >> (per-task accuracy): 70.0\n",
            "Epoch 0.35 >> (class accuracy): [80.2041, 96.2996, 69.186, 59.505, 51.3238, 54.148, 88.7265, 66.3424, 60.5749, 69.0783]\n",
            "Epoch 0.4 >> (per-task accuracy): 73.26\n",
            "Epoch 0.4 >> (class accuracy): [88.4694, 94.8018, 65.5039, 75.7426, 76.8839, 49.3274, 82.7766, 89.2996, 64.4764, 40.4361]\n",
            "Epoch 0.45 >> (per-task accuracy): 76.4\n",
            "Epoch 0.45 >> (class accuracy): [91.4286, 94.2731, 68.1202, 73.3663, 71.0794, 57.9596, 87.9958, 81.9066, 70.8419, 63.4291]\n",
            "Epoch 0.5 >> (per-task accuracy): 79.48\n",
            "Epoch 0.5 >> (class accuracy): [93.4694, 95.7709, 75.2907, 78.0198, 74.7454, 62.6682, 87.9958, 80.642, 75.154, 67.6908]\n",
            "Epoch 0.55 >> (per-task accuracy): 80.17\n",
            "Epoch 0.55 >> (class accuracy): [94.1837, 96.652, 75.6783, 77.5248, 79.3279, 63.9013, 84.0292, 83.9494, 75.6674, 67.2944]\n",
            "Epoch 0.6 >> (per-task accuracy): 81.82\n",
            "Epoch 0.6 >> (class accuracy): [93.8776, 96.5639, 74.5155, 80.297, 81.8737, 66.8161, 86.5344, 83.3658, 79.1581, 72.2498]\n",
            "Epoch 0.65 >> (per-task accuracy): 82.91\n",
            "Epoch 0.65 >> (class accuracy): [92.6531, 97.0044, 76.4535, 80.9901, 80.2444, 69.2825, 88.309, 85.3113, 77.8234, 78.0971]\n",
            "Epoch 0.7 >> (per-task accuracy): 83.24\n",
            "Epoch 0.7 >> (class accuracy): [93.9796, 95.0661, 76.8411, 81.8812, 78.3096, 71.1883, 88.1002, 84.6304, 78.8501, 81.0704]\n",
            "Epoch 0.75 >> (per-task accuracy): 84.54\n",
            "Epoch 0.75 >> (class accuracy): [94.1837, 96.2996, 77.5194, 82.2772, 84.0122, 76.2332, 88.4134, 88.035, 76.694, 79.5837]\n",
            "Epoch 0.8 >> (per-task accuracy): 85.28\n",
            "Epoch 0.8 >> (class accuracy): [93.4694, 96.4758, 79.3605, 86.4356, 85.2342, 77.6906, 87.6827, 89.786, 75.3593, 79.0882]\n",
            "Epoch 0.85 >> (per-task accuracy): 86.12\n",
            "Epoch 0.85 >> (class accuracy): [93.4694, 96.9163, 79.1667, 81.5842, 86.9654, 80.3812, 89.7704, 90.2724, 82.3409, 78.6918]\n",
            "Epoch 0.9 >> (per-task accuracy): 86.76\n",
            "Epoch 0.9 >> (class accuracy): [93.0612, 96.7401, 82.8488, 83.1683, 86.7617, 81.7265, 89.1441, 89.4942, 82.1355, 80.8722]\n",
            "Epoch 0.95 >> (per-task accuracy): 87.34\n",
            "Epoch 0.95 >> (class accuracy): [93.7755, 97.0925, 82.0736, 84.7525, 86.6599, 80.6054, 89.8747, 89.8833, 82.3409, 84.5391]\n",
            "Epoch 1.0 >> (per-task accuracy): 88.34\n",
            "Epoch 1.0 >> (class accuracy): [95.102, 97.2687, 83.0426, 85.6436, 87.169, 84.417, 91.858, 90.2724, 82.7515, 84.5391]\n",
            "OCS >> Task 1: {'accuracy': 36.45, 'per_class_accuracy': [79.0816, 75.5066, 18.6047, 12.8713, 47.556, 5.7175, 40.9186, 41.1479, 24.1273, 12.1903], 'loss': 2.1891187759399413}\n",
            "OCS >> Task 2: {'accuracy': 41.04, 'per_class_accuracy': [76.6327, 84.4053, 23.2558, 10.297, 46.334, 5.8296, 54.6973, 52.3346, 34.1889, 14.7671], 'loss': 2.0763467056274414}\n",
            "OCS >> Task 3: {'accuracy': 42.4, 'per_class_accuracy': [69.7959, 78.9427, 17.4419, 15.0495, 48.4725, 7.5112, 60.0209, 50.6809, 45.5852, 24.2815], 'loss': 1.9716190509796143}\n",
            "OCS >> Task 4: {'accuracy': 43.39, 'per_class_accuracy': [71.2245, 67.9295, 20.3488, 24.3564, 44.501, 10.426, 65.6576, 50.1946, 52.3614, 22.6957], 'loss': 1.9407662801742553}\n",
            "OCS >> Task 5: {'accuracy': 46.76, 'per_class_accuracy': [72.6531, 84.7577, 27.1318, 32.7723, 41.446, 13.2287, 60.4384, 57.1012, 47.5359, 23.4886], 'loss': 1.7848527647018433}\n",
            "OCS >> Task 6: {'accuracy': 50.26, 'per_class_accuracy': [78.3673, 93.3921, 32.655, 42.6733, 45.112, 18.6099, 65.3445, 57.9767, 37.9877, 22.6957], 'loss': 1.6568182344436646}\n",
            "OCS >> Task 7: {'accuracy': 52.67, 'per_class_accuracy': [86.9388, 96.652, 37.0155, 51.8812, 42.3625, 20.5157, 61.0647, 67.7043, 35.5236, 18.4341], 'loss': 1.5118884000778199}\n",
            "OCS >> Task 8: {'accuracy': 57.85, 'per_class_accuracy': [90.3061, 95.4185, 48.7403, 65.1485, 41.6497, 25.8969, 71.3987, 68.7743, 36.037, 27.1556], 'loss': 1.2961757590293885}\n",
            "OCS >> Task 9: {'accuracy': 62.61, 'per_class_accuracy': [92.2449, 97.6211, 55.9109, 73.0693, 41.9552, 27.8027, 72.7557, 78.6965, 36.4476, 40.9316], 'loss': 1.140921712589264}\n",
            "OCS >> Task 10: {'accuracy': 70.29, 'per_class_accuracy': [93.3673, 97.7093, 65.8915, 79.505, 53.8697, 33.8565, 81.3152, 80.3502, 54.6201, 54.9058], 'loss': 0.9059092376708985}\n",
            "OCS >> Task 11: {'accuracy': 79.3, 'per_class_accuracy': [94.5918, 96.0352, 79.1667, 83.4653, 63.0346, 60.6502, 87.3695, 86.5759, 66.9405, 70.7631], 'loss': 0.6676520588874817}\n",
            "OCS >> Task 12: {'accuracy': 84.66, 'per_class_accuracy': [94.898, 96.0352, 84.6899, 83.4653, 74.7454, 73.7668, 90.9186, 88.2296, 76.5914, 80.5748], 'loss': 0.5459977560043335}\n",
            "OCS >> Task 13: {'accuracy': 88.34, 'per_class_accuracy': [95.102, 97.2687, 83.0426, 85.6436, 87.169, 84.417, 91.858, 90.2724, 82.7515, 84.5391], 'loss': 0.4597363293170929}\n",
            "OCS >> (average accuracy): 58.15538461538462\n",
            "OCS >> (Forgetting): 0.4221999999999999\n",
            "Maximum per-task accuracies: [97.86]\n",
            "\n",
            "---- Task 14 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 22.97\n",
            "Epoch 0.05 >> (class accuracy): [28.2653, 63.0837, 30.7171, 74.1584, 0.0, 2.2422, 0.0, 17.3152, 4.0041, 0.0991]\n",
            "Epoch 0.1 >> (per-task accuracy): 9.84\n",
            "Epoch 0.1 >> (class accuracy): [100.0, 0.0, 0.2907, 0.099, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.15 >> (per-task accuracy): 10.57\n",
            "Epoch 0.15 >> (class accuracy): [100.0, 0.0, 1.2597, 6.3366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.2 >> (per-task accuracy): 16.71\n",
            "Epoch 0.2 >> (class accuracy): [99.7959, 0.0, 18.2171, 45.8416, 0.0, 0.0, 2.8184, 0.0, 1.232, 0.2973]\n",
            "Epoch 0.25 >> (per-task accuracy): 29.03\n",
            "Epoch 0.25 >> (class accuracy): [98.7755, 0.0, 45.5426, 77.4257, 0.0, 0.0, 34.0292, 0.4864, 8.8296, 26.3627]\n",
            "Epoch 0.3 >> (per-task accuracy): 37.49\n",
            "Epoch 0.3 >> (class accuracy): [97.7551, 0.0, 56.1047, 90.6931, 0.0, 0.0, 61.4823, 4.6693, 14.271, 51.5362]\n",
            "Epoch 0.35 >> (per-task accuracy): 44.44\n",
            "Epoch 0.35 >> (class accuracy): [96.9388, 2.467, 61.6279, 91.8812, 0.0, 0.0, 77.6618, 20.2335, 32.1355, 63.1318]\n",
            "Epoch 0.4 >> (per-task accuracy): 53.99\n",
            "Epoch 0.4 >> (class accuracy): [96.2245, 31.9824, 64.5349, 88.8119, 2.9532, 2.4664, 84.3424, 38.6187, 56.5708, 71.6551]\n",
            "Epoch 0.45 >> (per-task accuracy): 64.55\n",
            "Epoch 0.45 >> (class accuracy): [95.8163, 75.0661, 69.2829, 84.0594, 20.7739, 14.4619, 86.5344, 54.2802, 67.4538, 71.6551]\n",
            "Epoch 0.5 >> (per-task accuracy): 70.6\n",
            "Epoch 0.5 >> (class accuracy): [95.2041, 88.4581, 71.8992, 79.604, 39.8167, 30.3812, 87.1608, 67.3152, 71.5606, 68.4836]\n",
            "Epoch 0.55 >> (per-task accuracy): 73.07\n",
            "Epoch 0.55 >> (class accuracy): [94.898, 93.5683, 73.2558, 76.0396, 51.222, 39.4619, 87.0564, 74.1245, 72.4846, 62.8345]\n",
            "Epoch 0.6 >> (per-task accuracy): 74.3\n",
            "Epoch 0.6 >> (class accuracy): [94.898, 95.2423, 74.5155, 72.8713, 55.3971, 46.5247, 87.4739, 76.6537, 72.4846, 61.7443]\n",
            "Epoch 0.65 >> (per-task accuracy): 75.52\n",
            "Epoch 0.65 >> (class accuracy): [94.898, 96.5639, 75.6783, 72.2772, 62.3218, 50.7848, 86.6388, 77.0428, 70.8419, 63.2309]\n",
            "Epoch 0.7 >> (per-task accuracy): 76.4\n",
            "Epoch 0.7 >> (class accuracy): [94.7959, 96.9163, 75.2907, 72.0792, 64.5621, 56.0538, 86.7432, 77.4319, 71.3552, 64.4202]\n",
            "Epoch 0.75 >> (per-task accuracy): 77.6\n",
            "Epoch 0.75 >> (class accuracy): [94.6939, 97.7093, 77.5194, 73.4653, 67.6171, 59.417, 86.7432, 79.7665, 70.8419, 64.0238]\n",
            "Epoch 0.8 >> (per-task accuracy): 78.47\n",
            "Epoch 0.8 >> (class accuracy): [94.7959, 97.8855, 75.969, 75.8416, 70.2648, 61.6592, 87.7871, 80.8366, 70.5339, 65.2131]\n",
            "Epoch 0.85 >> (per-task accuracy): 79.08\n",
            "Epoch 0.85 >> (class accuracy): [95.102, 98.0617, 75.969, 77.3267, 74.2363, 63.7892, 86.5344, 81.8093, 69.6099, 64.6184]\n",
            "Epoch 0.9 >> (per-task accuracy): 79.93\n",
            "Epoch 0.9 >> (class accuracy): [94.2857, 98.2379, 77.7132, 77.2277, 75.9674, 64.7982, 89.5616, 81.5175, 70.6366, 65.8077]\n",
            "Epoch 0.95 >> (per-task accuracy): 80.68\n",
            "Epoch 0.95 >> (class accuracy): [94.7959, 98.1498, 76.7442, 76.8317, 75.8656, 69.6188, 89.2484, 80.2529, 71.7659, 70.5649]\n",
            "Epoch 1.0 >> (per-task accuracy): 81.31\n",
            "Epoch 1.0 >> (class accuracy): [95.6122, 98.0617, 78.1008, 79.1089, 75.9674, 70.1794, 87.6827, 81.9066, 71.6632, 71.7542]\n",
            "OCS >> Task 1: {'accuracy': 38.82, 'per_class_accuracy': [81.7347, 94.8018, 2.8101, 20.297, 46.4358, 4.9327, 40.3967, 53.8911, 26.5914, 7.0367], 'loss': 1.8694653240203858}\n",
            "OCS >> Task 2: {'accuracy': 41.59, 'per_class_accuracy': [80.5102, 95.0661, 4.0698, 17.2277, 53.4623, 5.8296, 53.2359, 58.463, 30.4928, 8.9197], 'loss': 1.80094074344635}\n",
            "OCS >> Task 3: {'accuracy': 45.01, 'per_class_accuracy': [75.5102, 96.2996, 7.655, 24.3564, 63.2383, 4.8206, 57.2025, 59.2412, 38.6037, 14.4698], 'loss': 1.6763017213821412}\n",
            "OCS >> Task 4: {'accuracy': 44.87, 'per_class_accuracy': [72.6531, 84.141, 12.7907, 30.297, 60.998, 3.6996, 63.8831, 54.0856, 43.3265, 15.8573], 'loss': 1.6758445793151855}\n",
            "OCS >> Task 5: {'accuracy': 48.84, 'per_class_accuracy': [72.3469, 86.1674, 27.8101, 37.9208, 61.609, 2.6906, 54.8017, 60.214, 48.3573, 28.0476], 'loss': 1.5797536058425903}\n",
            "OCS >> Task 6: {'accuracy': 50.61, 'per_class_accuracy': [74.7959, 89.0749, 35.0775, 44.9505, 60.2851, 3.2511, 55.3236, 56.0311, 46.0986, 32.2101], 'loss': 1.5294181077957154}\n",
            "OCS >> Task 7: {'accuracy': 52.65, 'per_class_accuracy': [81.1224, 89.2511, 39.2442, 51.3861, 54.1752, 5.9417, 56.785, 57.6848, 44.6612, 37.2646], 'loss': 1.463488014984131}\n",
            "OCS >> Task 8: {'accuracy': 54.72, 'per_class_accuracy': [82.6531, 84.0529, 49.9031, 58.9109, 49.2872, 10.8744, 66.2839, 50.5837, 40.6571, 46.1843], 'loss': 1.3858013122558595}\n",
            "OCS >> Task 9: {'accuracy': 58.97, 'per_class_accuracy': [86.0204, 95.3304, 54.3605, 67.4257, 46.7413, 17.4888, 71.9207, 56.8093, 37.4743, 47.2745], 'loss': 1.2566421335220337}\n",
            "OCS >> Task 10: {'accuracy': 63.93, 'per_class_accuracy': [89.3878, 97.7093, 61.0465, 74.0594, 49.0835, 25.2242, 76.9311, 62.2568, 43.3265, 51.9326], 'loss': 1.1414485342025757}\n",
            "OCS >> Task 11: {'accuracy': 70.79, 'per_class_accuracy': [91.9388, 97.9736, 69.2829, 78.4158, 48.4725, 42.4888, 83.4029, 71.0117, 54.0041, 64.3211], 'loss': 0.9773338691711426}\n",
            "OCS >> Task 12: {'accuracy': 76.17, 'per_class_accuracy': [94.898, 98.2379, 76.1628, 79.4059, 57.6375, 55.157, 85.8038, 73.7354, 64.3737, 71.2587], 'loss': 0.8522375961303711}\n",
            "OCS >> Task 13: {'accuracy': 80.06, 'per_class_accuracy': [95.3061, 98.4141, 79.4574, 77.9208, 67.3116, 64.6861, 88.5177, 80.1556, 69.1992, 75.7185], 'loss': 0.7700734498977662}\n",
            "OCS >> Task 14: {'accuracy': 81.31, 'per_class_accuracy': [95.6122, 98.0617, 78.1008, 79.1089, 75.9674, 70.1794, 87.6827, 81.9066, 71.6632, 71.7542], 'loss': 0.7568544942855835}\n",
            "OCS >> (average accuracy): 57.738571428571426\n",
            "OCS >> (Forgetting): 0.41934615384615376\n",
            "Maximum per-task accuracies: [97.86]\n",
            "\n",
            "---- Task 15 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 18.75\n",
            "Epoch 0.05 >> (class accuracy): [2.6531, 94.185, 0.4845, 34.0594, 0.0, 32.3991, 0.0, 7.0039, 7.1869, 0.0]\n",
            "Epoch 0.1 >> (per-task accuracy): 10.93\n",
            "Epoch 0.1 >> (class accuracy): [100.0, 0.6167, 0.6783, 9.703, 0.0, 0.0, 0.0, 0.0, 0.1027, 0.0]\n",
            "Epoch 0.15 >> (per-task accuracy): 9.84\n",
            "Epoch 0.15 >> (class accuracy): [100.0, 0.0, 0.0, 0.396, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.2 >> (per-task accuracy): 10.81\n",
            "Epoch 0.2 >> (class accuracy): [100.0, 0.0, 1.4535, 8.5149, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.25 >> (per-task accuracy): 15.39\n",
            "Epoch 0.25 >> (class accuracy): [99.898, 0.0, 10.0775, 33.4653, 0.0, 0.0, 1.4614, 0.0, 9.3429, 1.2884]\n",
            "Epoch 0.3 >> (per-task accuracy): 27.55\n",
            "Epoch 0.3 >> (class accuracy): [99.2857, 0.0881, 26.1628, 70.297, 0.0, 0.0, 23.904, 10.9922, 24.0246, 22.2993]\n",
            "Epoch 0.35 >> (per-task accuracy): 35.74\n",
            "Epoch 0.35 >> (class accuracy): [98.6735, 0.4405, 41.6667, 86.2376, 0.0, 0.0, 44.9896, 20.1362, 32.6489, 34.1923]\n",
            "Epoch 0.4 >> (per-task accuracy): 42.27\n",
            "Epoch 0.4 >> (class accuracy): [98.3673, 4.7577, 50.8721, 88.4158, 0.0, 0.0, 57.7244, 31.323, 39.8357, 52.329]\n",
            "Epoch 0.45 >> (per-task accuracy): 50.07\n",
            "Epoch 0.45 >> (class accuracy): [97.449, 35.7709, 55.1357, 90.495, 0.0, 0.1121, 70.1461, 45.8171, 42.2998, 60.1586]\n",
            "Epoch 0.5 >> (per-task accuracy): 58.79\n",
            "Epoch 0.5 >> (class accuracy): [96.6327, 73.5683, 58.8178, 88.6139, 3.7678, 1.6816, 78.6013, 63.8132, 53.4908, 60.7532]\n",
            "Epoch 0.55 >> (per-task accuracy): 64.48\n",
            "Epoch 0.55 >> (class accuracy): [96.7347, 89.4273, 62.4031, 85.0495, 15.2749, 9.5291, 81.7328, 67.4125, 61.9097, 66.2042]\n",
            "Epoch 0.6 >> (per-task accuracy): 68.07\n",
            "Epoch 0.6 >> (class accuracy): [96.6327, 93.7445, 66.7636, 82.2772, 33.2994, 22.0852, 84.3424, 74.3191, 62.9363, 56.1943]\n",
            "Epoch 0.65 >> (per-task accuracy): 70.49\n",
            "Epoch 0.65 >> (class accuracy): [96.4286, 94.5374, 69.0891, 77.9208, 40.5295, 31.9507, 86.0125, 73.4436, 65.9138, 62.1407]\n",
            "Epoch 0.7 >> (per-task accuracy): 71.49\n",
            "Epoch 0.7 >> (class accuracy): [96.3265, 95.5947, 71.6085, 74.9505, 49.6945, 38.2287, 85.6994, 77.6265, 65.8111, 53.0228]\n",
            "Epoch 0.75 >> (per-task accuracy): 72.18\n",
            "Epoch 0.75 >> (class accuracy): [96.3265, 96.0352, 72.7713, 73.1683, 52.3422, 40.6951, 86.1169, 78.5992, 68.1725, 51.5362]\n",
            "Epoch 0.8 >> (per-task accuracy): 73.34\n",
            "Epoch 0.8 >> (class accuracy): [95.8163, 96.5639, 73.3527, 72.6733, 56.0081, 48.0942, 86.5344, 77.5292, 67.0431, 54.5094]\n",
            "Epoch 0.85 >> (per-task accuracy): 73.56\n",
            "Epoch 0.85 >> (class accuracy): [96.0204, 96.9163, 72.3837, 72.4752, 58.1466, 50.8969, 85.9081, 77.0428, 65.8111, 55.005]\n",
            "Epoch 0.9 >> (per-task accuracy): 74.51\n",
            "Epoch 0.9 >> (class accuracy): [95.5102, 97.6211, 72.2868, 72.9703, 62.0163, 54.2601, 86.7432, 78.3074, 66.1191, 54.6085]\n",
            "Epoch 0.95 >> (per-task accuracy): 75.48\n",
            "Epoch 0.95 >> (class accuracy): [95.4082, 97.6211, 73.9341, 73.1683, 62.6273, 57.9596, 86.952, 78.9883, 66.0164, 57.78]\n",
            "Epoch 1.0 >> (per-task accuracy): 76.01\n",
            "Epoch 1.0 >> (class accuracy): [95.0, 97.9736, 72.9651, 72.8713, 64.4603, 59.7534, 87.0564, 78.7938, 66.2218, 60.8523]\n",
            "OCS >> Task 1: {'accuracy': 38.17, 'per_class_accuracy': [77.7551, 94.2731, 3.2946, 15.3465, 44.8065, 7.9596, 43.0063, 46.9844, 32.8542, 6.9376], 'loss': 1.844853757095337}\n",
            "OCS >> Task 2: {'accuracy': 41.21, 'per_class_accuracy': [76.5306, 95.1542, 4.2636, 15.0495, 49.2872, 9.5291, 57.62, 53.4047, 33.7782, 9.5144], 'loss': 1.7888811876296997}\n",
            "OCS >> Task 3: {'accuracy': 43.54, 'per_class_accuracy': [72.9592, 95.3304, 6.2016, 18.8119, 60.6925, 9.1928, 59.7077, 52.9183, 40.0411, 11.7939], 'loss': 1.6887579467773437}\n",
            "OCS >> Task 4: {'accuracy': 45.33, 'per_class_accuracy': [76.6327, 90.5727, 13.9535, 24.3564, 63.9511, 7.6233, 65.6576, 44.6498, 45.7906, 13.2805], 'loss': 1.6653362215042113}\n",
            "OCS >> Task 5: {'accuracy': 47.97, 'per_class_accuracy': [74.0816, 92.7753, 24.031, 32.0792, 64.053, 4.9327, 58.8727, 52.9183, 49.076, 18.5332], 'loss': 1.6157055736541748}\n",
            "OCS >> Task 6: {'accuracy': 49.16, 'per_class_accuracy': [76.3265, 93.5683, 31.2016, 38.4158, 62.3218, 3.4753, 56.8894, 50.2918, 48.3573, 21.8038], 'loss': 1.588051672744751}\n",
            "OCS >> Task 7: {'accuracy': 50.74, 'per_class_accuracy': [81.4286, 94.2731, 35.3682, 46.6337, 56.11, 3.139, 58.2463, 48.3463, 47.9466, 26.6601], 'loss': 1.556970435142517}\n",
            "OCS >> Task 8: {'accuracy': 52.26, 'per_class_accuracy': [82.0408, 93.6564, 43.5078, 55.2475, 50.0, 6.3901, 62.5261, 40.5642, 43.0185, 36.5709], 'loss': 1.5127075096130371}\n",
            "OCS >> Task 9: {'accuracy': 54.01, 'per_class_accuracy': [84.7959, 97.0044, 48.3527, 61.9802, 41.6497, 10.8744, 66.3883, 40.6615, 37.6797, 41.328], 'loss': 1.4279849956512451}\n",
            "OCS >> Task 10: {'accuracy': 57.42, 'per_class_accuracy': [88.9796, 98.326, 53.6822, 67.0297, 44.8065, 18.6099, 72.1294, 43.5798, 35.8316, 42.5173], 'loss': 1.3433813102722167}\n",
            "OCS >> Task 11: {'accuracy': 62.54, 'per_class_accuracy': [90.5102, 98.4141, 60.562, 73.3663, 45.0102, 27.2422, 79.4363, 48.0545, 44.5585, 50.5451], 'loss': 1.2201278778076172}\n",
            "OCS >> Task 12: {'accuracy': 67.53, 'per_class_accuracy': [95.0, 98.5022, 68.4109, 76.0396, 45.6212, 38.2287, 81.524, 55.0584, 54.8255, 55.5005], 'loss': 1.1041517822265625}\n",
            "OCS >> Task 13: {'accuracy': 72.7, 'per_class_accuracy': [95.5102, 98.5022, 74.7093, 73.5644, 51.5275, 47.6457, 85.6994, 70.2335, 61.191, 62.6363], 'loss': 0.9910796208381653}\n",
            "OCS >> Task 14: {'accuracy': 76.73, 'per_class_accuracy': [95.9184, 98.326, 78.3915, 74.0594, 60.6925, 57.1749, 88.6221, 73.5409, 67.7618, 68.2854], 'loss': 0.9431294310569763}\n",
            "OCS >> Task 15: {'accuracy': 76.01, 'per_class_accuracy': [95.0, 97.9736, 72.9651, 72.8713, 64.4603, 59.7534, 87.0564, 78.7938, 66.2218, 60.8523], 'loss': 0.9399657609939576}\n",
            "OCS >> (average accuracy): 55.688\n",
            "OCS >> (Forgetting): 0.4362357142857143\n",
            "Maximum per-task accuracies: [97.86]\n",
            "\n",
            "---- Task 16 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 15.72\n",
            "Epoch 0.05 >> (class accuracy): [0.0, 50.1322, 0.3876, 28.8119, 0.0, 69.9552, 0.0, 8.1712, 0.0, 0.0]\n",
            "Epoch 0.1 >> (per-task accuracy): 13.33\n",
            "Epoch 0.1 >> (class accuracy): [99.7959, 3.348, 2.8101, 28.2178, 0.0, 0.0, 0.0, 0.0, 0.308, 0.0]\n",
            "Epoch 0.15 >> (per-task accuracy): 9.83\n",
            "Epoch 0.15 >> (class accuracy): [100.0, 0.0, 0.0969, 0.198, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.2 >> (per-task accuracy): 9.83\n",
            "Epoch 0.2 >> (class accuracy): [100.0, 0.0, 0.0969, 0.198, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.25 >> (per-task accuracy): 10.26\n",
            "Epoch 0.25 >> (class accuracy): [100.0, 0.0, 0.7752, 3.7624, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.3 >> (per-task accuracy): 15.31\n",
            "Epoch 0.3 >> (class accuracy): [100.0, 0.0, 9.4961, 28.9109, 0.0, 0.0, 0.6263, 0.0, 15.9138, 0.0]\n",
            "Epoch 0.35 >> (per-task accuracy): 22.38\n",
            "Epoch 0.35 >> (class accuracy): [99.6939, 0.0, 24.1279, 50.6931, 0.0, 0.0, 13.8831, 0.0, 37.6797, 0.0]\n",
            "Epoch 0.4 >> (per-task accuracy): 28.08\n",
            "Epoch 0.4 >> (class accuracy): [98.7755, 0.0, 37.1124, 72.4752, 0.0, 0.0, 23.2777, 0.1946, 43.1211, 7.9286]\n",
            "Epoch 0.45 >> (per-task accuracy): 33.3\n",
            "Epoch 0.45 >> (class accuracy): [98.1633, 0.0881, 48.8372, 85.3465, 0.0, 0.4484, 43.215, 0.0, 35.4209, 23.5877]\n",
            "Epoch 0.5 >> (per-task accuracy): 38.01\n",
            "Epoch 0.5 >> (class accuracy): [97.7551, 4.0529, 52.2287, 87.6238, 0.0, 0.1121, 55.428, 0.3891, 45.2772, 39.2468]\n",
            "Epoch 0.55 >> (per-task accuracy): 40.85\n",
            "Epoch 0.55 >> (class accuracy): [97.449, 9.2511, 53.0039, 88.7129, 0.3055, 1.1211, 65.762, 4.1829, 48.768, 41.7245]\n",
            "Epoch 0.6 >> (per-task accuracy): 49.62\n",
            "Epoch 0.6 >> (class accuracy): [96.4286, 28.2819, 59.1085, 87.5248, 16.7006, 6.9507, 74.8434, 26.9455, 58.2136, 41.1298]\n",
            "Epoch 0.65 >> (per-task accuracy): 52.63\n",
            "Epoch 0.65 >> (class accuracy): [95.2041, 29.6916, 61.9186, 86.4356, 20.1629, 14.1256, 83.9248, 21.7899, 61.6016, 52.4281]\n",
            "Epoch 0.7 >> (per-task accuracy): 57.37\n",
            "Epoch 0.7 >> (class accuracy): [95.102, 43.348, 62.7907, 84.5545, 37.9837, 19.843, 83.5073, 37.1595, 67.1458, 42.1209]\n",
            "Epoch 0.75 >> (per-task accuracy): 65.12\n",
            "Epoch 0.75 >> (class accuracy): [95.6122, 77.6211, 67.2481, 81.9802, 47.3523, 26.7937, 80.6889, 58.3658, 68.7885, 42.1209]\n",
            "Epoch 0.8 >> (per-task accuracy): 69.68\n",
            "Epoch 0.8 >> (class accuracy): [94.898, 86.6079, 71.0271, 77.5248, 49.4908, 45.9641, 84.3424, 67.8988, 64.5791, 50.3469]\n",
            "Epoch 0.85 >> (per-task accuracy): 69.94\n",
            "Epoch 0.85 >> (class accuracy): [94.3878, 91.0132, 72.093, 78.5149, 54.0733, 37.5561, 84.6555, 70.1362, 67.3511, 44.1031]\n",
            "Epoch 0.9 >> (per-task accuracy): 70.69\n",
            "Epoch 0.9 >> (class accuracy): [95.0, 92.5991, 72.093, 76.1386, 59.2668, 42.6009, 83.5073, 74.3191, 65.1951, 40.9316]\n",
            "Epoch 0.95 >> (per-task accuracy): 71.16\n",
            "Epoch 0.95 >> (class accuracy): [94.7959, 95.0661, 73.062, 74.4554, 58.4521, 44.9552, 83.6117, 75.8755, 64.7844, 41.1298]\n",
            "Epoch 1.0 >> (per-task accuracy): 71.94\n",
            "Epoch 1.0 >> (class accuracy): [94.4898, 94.8899, 71.124, 73.3663, 64.053, 47.5336, 84.8643, 77.9183, 66.7351, 39.5441]\n",
            "OCS >> Task 1: {'accuracy': 34.7, 'per_class_accuracy': [71.6327, 90.6608, 2.907, 9.901, 50.0, 6.8386, 43.1106, 8.0739, 48.6653, 8.6224], 'loss': 1.9339127689361573}\n",
            "OCS >> Task 2: {'accuracy': 37.14, 'per_class_accuracy': [72.7551, 91.8943, 3.7791, 7.1287, 48.9817, 8.7444, 55.2192, 17.607, 49.384, 9.6135], 'loss': 1.887852982521057}\n",
            "OCS >> Task 3: {'accuracy': 39.74, 'per_class_accuracy': [72.551, 93.304, 5.6202, 11.3861, 59.5723, 9.9776, 57.9332, 17.3152, 56.3655, 7.4331], 'loss': 1.8034637525558472}\n",
            "OCS >> Task 4: {'accuracy': 41.01, 'per_class_accuracy': [75.7143, 88.9868, 9.8837, 12.9703, 63.0346, 9.6413, 66.8058, 14.786, 57.0842, 6.2438], 'loss': 1.773767219543457}\n",
            "OCS >> Task 5: {'accuracy': 44.44, 'per_class_accuracy': [74.4898, 92.5991, 25.0, 17.7228, 63.5438, 7.1749, 64.405, 24.4163, 58.2136, 10.2081], 'loss': 1.7155936101913452}\n",
            "OCS >> Task 6: {'accuracy': 45.43, 'per_class_accuracy': [73.2653, 90.5727, 33.8178, 22.2772, 60.8961, 5.0448, 65.3445, 28.2101, 58.5216, 9.3162], 'loss': 1.7005872903823853}\n",
            "OCS >> Task 7: {'accuracy': 48.77, 'per_class_accuracy': [78.2653, 91.1013, 41.5698, 31.2871, 59.165, 4.5964, 67.3278, 34.0467, 58.4189, 14.3707], 'loss': 1.6675426874160766}\n",
            "OCS >> Task 8: {'accuracy': 49.78, 'per_class_accuracy': [79.0816, 86.7841, 49.6124, 39.505, 54.7862, 6.9507, 69.1023, 27.4319, 54.4148, 23.0922], 'loss': 1.6382063495635986}\n",
            "OCS >> Task 9: {'accuracy': 51.61, 'per_class_accuracy': [81.6327, 93.8326, 52.4225, 47.6238, 48.8798, 10.7623, 71.0856, 25.0973, 46.5092, 30.3271], 'loss': 1.5854841594696045}\n",
            "OCS >> Task 10: {'accuracy': 54.09, 'per_class_accuracy': [85.2041, 96.4758, 55.3295, 52.3762, 50.8147, 18.4978, 73.2777, 25.2918, 38.3984, 37.5619], 'loss': 1.5368503261566162}\n",
            "OCS >> Task 11: {'accuracy': 56.8, 'per_class_accuracy': [87.0408, 97.1806, 56.4922, 59.3069, 50.4073, 23.991, 77.2443, 26.8482, 39.8357, 42.5173], 'loss': 1.4623539573669433}\n",
            "OCS >> Task 12: {'accuracy': 61.08, 'per_class_accuracy': [90.9184, 97.8855, 60.3682, 65.6436, 52.1385, 31.8386, 77.6618, 37.9377, 44.7639, 44.995], 'loss': 1.3852806259155273}\n",
            "OCS >> Task 13: {'accuracy': 66.58, 'per_class_accuracy': [92.0408, 98.2379, 66.8605, 69.505, 57.8411, 38.4529, 79.5407, 59.2412, 50.2053, 47.3736], 'loss': 1.2800845680236816}\n",
            "OCS >> Task 14: {'accuracy': 70.95, 'per_class_accuracy': [93.4694, 98.2379, 73.5465, 73.8614, 64.5621, 44.5067, 82.6722, 66.1479, 58.3162, 48.3647], 'loss': 1.2098834257125854}\n",
            "OCS >> Task 15: {'accuracy': 72.34, 'per_class_accuracy': [94.0816, 97.7974, 74.6124, 73.1683, 63.2383, 47.7578, 84.9687, 73.4436, 63.4497, 45.4906], 'loss': 1.1663581399917602}\n",
            "OCS >> Task 16: {'accuracy': 71.94, 'per_class_accuracy': [94.4898, 94.8899, 71.124, 73.3663, 64.053, 47.5336, 84.8643, 77.9183, 66.7351, 39.5441], 'loss': 1.165296312713623}\n",
            "OCS >> (average accuracy): 52.900000000000006\n",
            "OCS >> (Forgetting): 0.4622933333333333\n",
            "Maximum per-task accuracies: [97.86]\n",
            "\n",
            "---- Task 17 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 10.22\n",
            "Epoch 0.05 >> (class accuracy): [0.5102, 3.2599, 2.5194, 9.1089, 0.0, 92.1525, 0.0, 2.821, 1.1294, 0.0]\n",
            "Epoch 0.1 >> (per-task accuracy): 12.97\n",
            "Epoch 0.1 >> (class accuracy): [99.1837, 0.0, 0.2907, 30.9901, 0.0, 0.5605, 0.0, 0.0, 0.4107, 0.0]\n",
            "Epoch 0.15 >> (per-task accuracy): 10.03\n",
            "Epoch 0.15 >> (class accuracy): [100.0, 0.0, 0.0, 2.2772, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.2 >> (per-task accuracy): 9.8\n",
            "Epoch 0.2 >> (class accuracy): [100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.25 >> (per-task accuracy): 9.88\n",
            "Epoch 0.25 >> (class accuracy): [100.0, 0.0, 0.0, 0.6931, 0.0, 0.0, 0.0, 0.0, 0.1027, 0.0]\n",
            "Epoch 0.3 >> (per-task accuracy): 11.24\n",
            "Epoch 0.3 >> (class accuracy): [100.0, 0.0, 0.3876, 13.7624, 0.0, 0.0, 0.0, 0.0, 0.1027, 0.0]\n",
            "Epoch 0.35 >> (per-task accuracy): 14.02\n",
            "Epoch 0.35 >> (class accuracy): [99.898, 0.0, 7.1705, 26.1386, 0.2037, 0.0, 0.0, 0.0, 8.1109, 0.3964]\n",
            "Epoch 0.4 >> (per-task accuracy): 20.49\n",
            "Epoch 0.4 >> (class accuracy): [99.7959, 0.0881, 24.3217, 31.7822, 0.0, 0.0, 7.8288, 0.0, 42.4025, 0.9911]\n",
            "Epoch 0.45 >> (per-task accuracy): 26.84\n",
            "Epoch 0.45 >> (class accuracy): [98.9796, 0.2643, 30.3295, 61.7822, 0.2037, 1.009, 32.3591, 0.0, 23.1006, 22.5966]\n",
            "Epoch 0.5 >> (per-task accuracy): 34.1\n",
            "Epoch 0.5 >> (class accuracy): [98.2653, 16.9163, 28.4884, 77.9208, 0.0, 3.3632, 43.9457, 0.0, 16.4271, 55.7978]\n",
            "Epoch 0.55 >> (per-task accuracy): 35.89\n",
            "Epoch 0.55 >> (class accuracy): [97.6531, 0.4405, 51.1628, 85.6436, 5.6008, 6.5022, 45.1983, 0.0973, 9.4456, 58.9693]\n",
            "Epoch 0.6 >> (per-task accuracy): 41.41\n",
            "Epoch 0.6 >> (class accuracy): [97.3469, 5.9031, 42.5388, 83.2673, 10.998, 10.5381, 65.8664, 14.8833, 26.2834, 59.2666]\n",
            "Epoch 0.65 >> (per-task accuracy): 48.9\n",
            "Epoch 0.65 >> (class accuracy): [97.7551, 14.185, 52.3256, 82.8713, 34.3177, 17.3767, 64.9269, 38.9105, 35.2156, 53.221]\n",
            "Epoch 0.7 >> (per-task accuracy): 50.21\n",
            "Epoch 0.7 >> (class accuracy): [96.8367, 2.9956, 48.6434, 80.099, 26.0692, 10.426, 72.4426, 58.1712, 72.3819, 37.7602]\n",
            "Epoch 0.75 >> (per-task accuracy): 59.31\n",
            "Epoch 0.75 >> (class accuracy): [95.5102, 57.0044, 53.876, 84.6535, 72.6069, 16.0314, 75.6785, 43.7743, 57.5975, 34.1923]\n",
            "Epoch 0.8 >> (per-task accuracy): 60.51\n",
            "Epoch 0.8 >> (class accuracy): [96.6327, 62.2026, 56.686, 80.297, 59.4705, 20.2915, 73.5908, 37.8405, 69.7125, 45.9861]\n",
            "Epoch 0.85 >> (per-task accuracy): 66.78\n",
            "Epoch 0.85 >> (class accuracy): [96.1224, 83.2599, 60.562, 81.5842, 48.3707, 29.9327, 76.4092, 66.4397, 62.5257, 57.0862]\n",
            "Epoch 0.9 >> (per-task accuracy): 69.64\n",
            "Epoch 0.9 >> (class accuracy): [94.6939, 89.4273, 59.7868, 77.0297, 70.2648, 45.0673, 80.6889, 82.4903, 64.271, 28.444]\n",
            "Epoch 0.95 >> (per-task accuracy): 69.86\n",
            "Epoch 0.95 >> (class accuracy): [95.2041, 88.37, 63.2752, 78.5149, 70.2648, 36.435, 81.3152, 75.4864, 70.0205, 34.886]\n",
            "Epoch 1.0 >> (per-task accuracy): 71.05\n",
            "Epoch 1.0 >> (class accuracy): [93.2653, 96.2115, 63.8566, 76.6337, 64.1548, 53.5874, 84.3424, 80.642, 56.9815, 36.2735]\n",
            "OCS >> Task 1: {'accuracy': 31.45, 'per_class_accuracy': [62.3469, 91.2775, 9.3992, 19.0099, 37.8819, 6.1659, 52.0877, 12.7432, 11.0883, 4.3608], 'loss': 1.9464867185592651}\n",
            "OCS >> Task 2: {'accuracy': 34.02, 'per_class_accuracy': [64.2857, 91.9824, 10.3682, 18.3168, 39.613, 8.0717, 61.2735, 22.2763, 12.115, 4.0634], 'loss': 1.9060983371734619}\n",
            "OCS >> Task 3: {'accuracy': 37.01, 'per_class_accuracy': [59.0816, 92.4229, 13.3721, 22.3762, 51.833, 7.7354, 66.7015, 27.9183, 18.1725, 2.775], 'loss': 1.8374003608703613}\n",
            "OCS >> Task 4: {'accuracy': 38.0, 'per_class_accuracy': [56.6327, 86.9604, 18.5078, 23.2673, 58.1466, 7.8475, 73.0689, 25.7782, 20.3285, 2.775], 'loss': 1.818162639427185}\n",
            "OCS >> Task 5: {'accuracy': 42.02, 'per_class_accuracy': [55.4082, 91.1894, 31.1047, 26.3366, 57.7393, 7.287, 73.5908, 34.9222, 30.0821, 4.7572], 'loss': 1.7720671852111816}\n",
            "OCS >> Task 6: {'accuracy': 43.27, 'per_class_accuracy': [58.7755, 91.0132, 37.7907, 30.198, 55.9063, 8.5202, 73.2777, 35.1167, 29.9795, 4.2616], 'loss': 1.754579642868042}\n",
            "OCS >> Task 7: {'accuracy': 44.7, 'per_class_accuracy': [60.8163, 93.9207, 42.0543, 37.2277, 50.3055, 11.3229, 70.9812, 36.4786, 29.9795, 5.55], 'loss': 1.738152936553955}\n",
            "OCS >> Task 8: {'accuracy': 45.63, 'per_class_accuracy': [68.1633, 93.1278, 44.3798, 44.4554, 47.556, 16.8161, 67.8497, 25.2918, 28.6448, 12.3885], 'loss': 1.7224232526779175}\n",
            "OCS >> Task 9: {'accuracy': 46.62, 'per_class_accuracy': [73.9796, 95.2423, 41.6667, 51.1881, 40.835, 23.8789, 65.8664, 25.1946, 21.3552, 19.5243], 'loss': 1.6952481212615966}\n",
            "OCS >> Task 10: {'accuracy': 47.86, 'per_class_accuracy': [81.2245, 98.0617, 36.0465, 55.2475, 38.6965, 34.0807, 62.0042, 27.7237, 16.0164, 22.5966], 'loss': 1.6680829027175903}\n",
            "OCS >> Task 11: {'accuracy': 51.22, 'per_class_accuracy': [85.7143, 97.8855, 33.624, 59.4059, 36.4562, 48.0942, 64.6138, 32.393, 19.0965, 29.6333], 'loss': 1.6214384098052979}\n",
            "OCS >> Task 12: {'accuracy': 54.3, 'per_class_accuracy': [89.4898, 97.9736, 34.7868, 59.604, 35.0305, 58.296, 62.9436, 43.1907, 24.0246, 33.2012], 'loss': 1.569545586013794}\n",
            "OCS >> Task 13: {'accuracy': 61.12, 'per_class_accuracy': [91.0204, 98.4141, 40.3101, 62.6733, 44.3992, 63.565, 74.2171, 59.4358, 33.9836, 39.445], 'loss': 1.4745854551315307}\n",
            "OCS >> Task 14: {'accuracy': 66.7, 'per_class_accuracy': [93.2653, 98.4141, 49.4186, 68.6139, 53.8697, 64.3498, 78.6013, 71.0117, 45.5852, 40.2379], 'loss': 1.405009736442566}\n",
            "OCS >> Task 15: {'accuracy': 70.65, 'per_class_accuracy': [94.2857, 98.0617, 59.593, 71.2871, 59.8778, 63.3408, 81.2109, 78.5019, 54.3121, 42.22], 'loss': 1.337686643600464}\n",
            "OCS >> Task 16: {'accuracy': 71.7, 'per_class_accuracy': [93.9796, 97.1806, 63.1783, 75.2475, 64.1548, 59.1928, 83.7161, 82.393, 58.2136, 35.6789], 'loss': 1.3167722400665283}\n",
            "OCS >> Task 17: {'accuracy': 71.05, 'per_class_accuracy': [93.2653, 96.2115, 63.8566, 76.6337, 64.1548, 53.5874, 84.3424, 80.642, 56.9815, 36.2735], 'loss': 1.3221286848068237}\n",
            "OCS >> (average accuracy): 50.43058823529412\n",
            "OCS >> (Forgetting): 0.48718125\n",
            "Maximum per-task accuracies: [97.86]\n",
            "\n",
            "---- Task 18 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 10.32\n",
            "Epoch 0.05 >> (class accuracy): [0.0, 0.0, 100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.1 >> (per-task accuracy): 15.04\n",
            "Epoch 0.1 >> (class accuracy): [0.0, 0.0, 66.7636, 75.5446, 0.0, 0.0, 0.0, 0.0, 0.0, 5.1536]\n",
            "Epoch 0.15 >> (per-task accuracy): 11.14\n",
            "Epoch 0.15 >> (class accuracy): [0.0, 0.0, 13.8566, 0.0, 0.0, 0.0, 0.0, 0.0, 99.692, 0.0]\n",
            "Epoch 0.2 >> (per-task accuracy): 25.1\n",
            "Epoch 0.2 >> (class accuracy): [53.6735, 0.0, 80.5233, 30.0, 0.0, 0.0, 0.0, 0.0, 0.4107, 83.8454]\n",
            "Epoch 0.25 >> (per-task accuracy): 10.73\n",
            "Epoch 0.25 >> (class accuracy): [100.0, 0.0, 0.3876, 4.6535, 0.0, 0.0, 0.0, 0.0, 0.5133, 3.667]\n",
            "Epoch 0.3 >> (per-task accuracy): 17.42\n",
            "Epoch 0.3 >> (class accuracy): [98.6735, 0.0, 0.2907, 0.297, 0.0, 0.0, 0.0, 0.0, 78.9528, 0.0]\n",
            "Epoch 0.35 >> (per-task accuracy): 18.38\n",
            "Epoch 0.35 >> (class accuracy): [99.7959, 0.0, 26.3566, 29.1089, 0.0, 0.0, 0.0, 0.0, 0.7187, 28.444]\n",
            "Epoch 0.4 >> (per-task accuracy): 25.44\n",
            "Epoch 0.4 >> (class accuracy): [99.5918, 0.0, 30.4264, 47.5248, 0.0, 1.009, 30.5846, 42.607, 0.4107, 2.9732]\n",
            "Epoch 0.45 >> (per-task accuracy): 25.85\n",
            "Epoch 0.45 >> (class accuracy): [97.7551, 0.7048, 81.9767, 61.4851, 0.0, 0.0, 13.2568, 2.1401, 0.0, 0.2973]\n",
            "Epoch 0.5 >> (per-task accuracy): 23.8\n",
            "Epoch 0.5 >> (class accuracy): [83.5714, 0.0, 96.124, 32.2772, 0.0, 0.0, 0.0, 0.0, 0.0, 24.0833]\n",
            "Epoch 0.55 >> (per-task accuracy): 25.57\n",
            "Epoch 0.55 >> (class accuracy): [97.551, 0.0, 22.2868, 41.6832, 0.0, 0.0, 0.0, 0.0, 3.2854, 90.9812]\n",
            "Epoch 0.6 >> (per-task accuracy): 36.21\n",
            "Epoch 0.6 >> (class accuracy): [97.551, 0.0881, 39.7287, 31.8812, 0.0, 0.0, 29.9582, 0.0, 78.3368, 87.4133]\n",
            "Epoch 0.65 >> (per-task accuracy): 39.48\n",
            "Epoch 0.65 >> (class accuracy): [93.5714, 3.9648, 54.6512, 77.4257, 0.0, 7.3991, 57.2025, 0.0, 18.6858, 83.6472]\n",
            "Epoch 0.7 >> (per-task accuracy): 46.92\n",
            "Epoch 0.7 >> (class accuracy): [91.2245, 41.5859, 62.5, 70.297, 0.4073, 11.9955, 49.6868, 0.0, 58.7269, 80.4757]\n",
            "Epoch 0.75 >> (per-task accuracy): 35.32\n",
            "Epoch 0.75 >> (class accuracy): [87.0408, 13.6564, 47.1899, 24.6535, 0.1018, 80.6054, 19.3111, 0.0, 1.1294, 86.4222]\n",
            "Epoch 0.8 >> (per-task accuracy): 51.0\n",
            "Epoch 0.8 >> (class accuracy): [94.4898, 68.1938, 55.0388, 31.3861, 15.8859, 2.4664, 60.0209, 36.5759, 91.1704, 49.3558]\n",
            "Epoch 0.85 >> (per-task accuracy): 52.89\n",
            "Epoch 0.85 >> (class accuracy): [94.5918, 60.6167, 52.3256, 83.3663, 69.3483, 13.1166, 62.5261, 15.6615, 38.6037, 35.4807]\n",
            "Epoch 0.9 >> (per-task accuracy): 57.33\n",
            "Epoch 0.9 >> (class accuracy): [87.8571, 47.0485, 56.1047, 61.1881, 19.2464, 37.1076, 71.2944, 63.8132, 71.0472, 58.4737]\n",
            "Epoch 0.95 >> (per-task accuracy): 54.27\n",
            "Epoch 0.95 >> (class accuracy): [88.4694, 53.3921, 56.0078, 65.3465, 1.6293, 21.5247, 79.0188, 81.0311, 70.2259, 23.1913]\n",
            "Epoch 1.0 >> (per-task accuracy): 58.13\n",
            "Epoch 1.0 >> (class accuracy): [86.5306, 63.1718, 63.2752, 63.6634, 69.7556, 28.6996, 61.8998, 25.0973, 74.6407, 42.9138]\n",
            "OCS >> Task 1: {'accuracy': 19.29, 'per_class_accuracy': [31.2245, 20.7048, 11.531, 17.5248, 52.8513, 1.1211, 7.5157, 0.3891, 38.809, 10.8028], 'loss': 2.0825927780151368}\n",
            "OCS >> Task 2: {'accuracy': 19.48, 'per_class_accuracy': [32.6531, 8.37, 15.3101, 16.5347, 46.8432, 1.5695, 8.4551, 0.8755, 49.076, 16.4519], 'loss': 2.0270353031158446}\n",
            "OCS >> Task 3: {'accuracy': 22.9, 'per_class_accuracy': [28.1633, 0.3524, 21.7054, 25.0495, 60.5906, 2.13, 6.8894, 1.6537, 64.271, 20.8127], 'loss': 1.9261061418533325}\n",
            "OCS >> Task 4: {'accuracy': 25.73, 'per_class_accuracy': [31.4286, 0.1762, 34.0116, 30.297, 58.0448, 1.6816, 12.6305, 1.6537, 64.7844, 24.9752], 'loss': 1.8758396675109863}\n",
            "OCS >> Task 5: {'accuracy': 30.02, 'per_class_accuracy': [27.551, 0.2643, 48.3527, 39.901, 56.5173, 1.6816, 13.7787, 6.323, 68.7885, 38.6521], 'loss': 1.800423168373108}\n",
            "OCS >> Task 6: {'accuracy': 33.21, 'per_class_accuracy': [27.0408, 0.1762, 58.7209, 48.3168, 50.4073, 1.1211, 23.5908, 8.8521, 68.2752, 46.8781], 'loss': 1.758661616897583}\n",
            "OCS >> Task 7: {'accuracy': 37.01, 'per_class_accuracy': [28.3673, 0.1762, 64.2442, 58.0198, 49.4908, 1.1211, 34.3424, 18.8716, 69.1992, 47.4727], 'loss': 1.7171875831604004}\n",
            "OCS >> Task 8: {'accuracy': 39.98, 'per_class_accuracy': [32.7551, 0.4405, 66.6667, 63.2673, 46.1303, 1.2332, 45.0939, 19.2607, 68.1725, 58.1764], 'loss': 1.6864351718902588}\n",
            "OCS >> Task 9: {'accuracy': 40.89, 'per_class_accuracy': [33.7755, 1.7621, 61.9186, 67.8218, 40.5295, 1.7937, 49.3737, 22.5681, 70.3285, 60.4559], 'loss': 1.6554403926849366}\n",
            "OCS >> Task 10: {'accuracy': 40.44, 'per_class_accuracy': [39.898, 4.3172, 52.6163, 73.3663, 41.6497, 4.148, 42.9019, 16.8288, 67.2485, 62.9336], 'loss': 1.635662229347229}\n",
            "OCS >> Task 11: {'accuracy': 39.81, 'per_class_accuracy': [43.5714, 2.2907, 47.4806, 73.2673, 40.4277, 9.7534, 41.1273, 12.6459, 64.4764, 65.6095], 'loss': 1.6344663551330567}\n",
            "OCS >> Task 12: {'accuracy': 39.84, 'per_class_accuracy': [51.5306, 0.7048, 42.1512, 68.8119, 45.723, 15.1345, 36.2213, 17.8016, 58.5216, 65.114], 'loss': 1.6243200664520263}\n",
            "OCS >> Task 13: {'accuracy': 44.45, 'per_class_accuracy': [58.9796, 2.5551, 38.7597, 66.4356, 60.4888, 19.9552, 47.1816, 30.5447, 58.6242, 65.2131], 'loss': 1.5525423288345337}\n",
            "OCS >> Task 14: {'accuracy': 48.94, 'per_class_accuracy': [69.6939, 12.2467, 41.6667, 63.0693, 66.5988, 23.8789, 58.2463, 34.8249, 58.2136, 64.9158], 'loss': 1.4932759677886962}\n",
            "OCS >> Task 15: {'accuracy': 55.4, 'per_class_accuracy': [78.4694, 29.9559, 50.0, 62.7723, 71.8941, 32.6233, 63.1524, 44.9416, 63.655, 59.1675], 'loss': 1.4110958864212035}\n",
            "OCS >> Task 16: {'accuracy': 58.91, 'per_class_accuracy': [84.4898, 39.4714, 59.3992, 65.4455, 72.3014, 34.8655, 67.7453, 45.3307, 67.8645, 53.9148], 'loss': 1.350876360321045}\n",
            "OCS >> Task 17: {'accuracy': 61.13, 'per_class_accuracy': [87.8571, 60.2643, 64.5349, 65.5446, 73.8289, 34.417, 66.1795, 36.7704, 69.6099, 51.338], 'loss': 1.3216640607833863}\n",
            "OCS >> Task 18: {'accuracy': 58.13, 'per_class_accuracy': [86.5306, 63.1718, 63.2752, 63.6634, 69.7556, 28.6996, 61.8998, 25.0973, 74.6407, 42.9138], 'loss': 1.3550608926773071}\n",
            "OCS >> (average accuracy): 39.75333333333333\n",
            "OCS >> (Forgetting): 0.5918764705882353\n",
            "Maximum per-task accuracies: [97.86]\n",
            "\n",
            "---- Task 19 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 10.68\n",
            "Epoch 0.05 >> (class accuracy): [0.0, 17.6211, 24.1279, 0.0, 0.0, 0.2242, 0.0, 0.0, 63.347, 0.0]\n",
            "Epoch 0.1 >> (per-task accuracy): 19.01\n",
            "Epoch 0.1 >> (class accuracy): [57.3469, 0.2643, 22.8682, 63.4653, 10.0815, 0.0, 0.0, 2.1401, 34.5996, 0.0991]\n",
            "Epoch 0.15 >> (per-task accuracy): 15.79\n",
            "Epoch 0.15 >> (class accuracy): [59.2857, 0.0, 0.0, 98.8119, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.2 >> (per-task accuracy): 17.54\n",
            "Epoch 0.2 >> (class accuracy): [87.8571, 0.0, 0.0, 87.3267, 0.0, 0.0, 0.0, 1.07, 0.0, 0.0]\n",
            "Epoch 0.25 >> (per-task accuracy): 10.67\n",
            "Epoch 0.25 >> (class accuracy): [100.0, 0.0, 0.2907, 5.6436, 0.3055, 2.6906, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.3 >> (per-task accuracy): 10.54\n",
            "Epoch 0.3 >> (class accuracy): [99.898, 0.0, 0.0969, 7.3267, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.35 >> (per-task accuracy): 9.91\n",
            "Epoch 0.35 >> (class accuracy): [100.0, 0.0, 0.2907, 0.7921, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.4 >> (per-task accuracy): 15.3\n",
            "Epoch 0.4 >> (class accuracy): [99.7959, 4.3172, 2.6163, 12.4752, 0.0, 0.0, 6.1587, 3.3074, 26.386, 0.0]\n",
            "Epoch 0.45 >> (per-task accuracy): 14.25\n",
            "Epoch 0.45 >> (class accuracy): [99.7959, 1.4978, 0.1938, 25.8416, 0.1018, 0.0, 0.0, 0.0, 17.0431, 0.0]\n",
            "Epoch 0.5 >> (per-task accuracy): 13.63\n",
            "Epoch 0.5 >> (class accuracy): [99.7959, 4.141, 0.0, 26.2376, 0.0, 0.0, 7.4113, 0.0, 0.2053, 0.0]\n",
            "Epoch 0.55 >> (per-task accuracy): 19.82\n",
            "Epoch 0.55 >> (class accuracy): [99.7959, 3.2599, 0.0969, 44.2574, 0.0, 0.0, 18.8935, 0.0, 34.7023, 0.0]\n",
            "Epoch 0.6 >> (per-task accuracy): 24.02\n",
            "Epoch 0.6 >> (class accuracy): [99.1837, 13.7445, 1.1628, 13.7624, 0.0, 11.2108, 30.3758, 0.0, 75.154, 0.0]\n",
            "Epoch 0.65 >> (per-task accuracy): 24.36\n",
            "Epoch 0.65 >> (class accuracy): [99.2857, 0.0, 20.0581, 26.8317, 0.0, 16.4798, 13.5699, 0.0, 64.7844, 7.6313]\n",
            "Epoch 0.7 >> (per-task accuracy): 28.58\n",
            "Epoch 0.7 >> (class accuracy): [98.6735, 5.3744, 8.2364, 18.2178, 0.0, 1.9058, 4.6973, 0.0, 84.3943, 67.0961]\n",
            "Epoch 0.75 >> (per-task accuracy): 44.86\n",
            "Epoch 0.75 >> (class accuracy): [98.7755, 84.9339, 40.8915, 52.4752, 0.0, 9.5291, 13.7787, 0.0, 61.3963, 77.998]\n",
            "Epoch 0.8 >> (per-task accuracy): 43.1\n",
            "Epoch 0.8 >> (class accuracy): [95.5102, 73.4802, 58.624, 82.9703, 0.0, 39.3498, 22.9645, 0.0, 2.0534, 50.1487]\n",
            "Epoch 0.85 >> (per-task accuracy): 41.41\n",
            "Epoch 0.85 >> (class accuracy): [94.6939, 57.6211, 43.5078, 37.0297, 0.4073, 60.3139, 48.2255, 0.0, 21.9713, 51.338]\n",
            "Epoch 0.9 >> (per-task accuracy): 46.14\n",
            "Epoch 0.9 >> (class accuracy): [96.2245, 51.7181, 28.4884, 80.099, 0.1018, 49.6637, 51.0438, 2.821, 41.4784, 60.9514]\n",
            "Epoch 0.95 >> (per-task accuracy): 46.45\n",
            "Epoch 0.95 >> (class accuracy): [96.4286, 73.4802, 37.1124, 45.0495, 0.2037, 3.2511, 40.1879, 0.0, 80.4928, 82.0614]\n",
            "Epoch 1.0 >> (per-task accuracy): 45.68\n",
            "Epoch 1.0 >> (class accuracy): [97.7551, 75.3304, 19.8643, 61.8812, 0.0, 0.0, 47.3904, 0.0, 71.7659, 76.5114]\n",
            "OCS >> Task 1: {'accuracy': 27.85, 'per_class_accuracy': [97.1429, 71.63, 0.0969, 5.4455, 0.0, 0.0, 26.618, 0.0, 35.9343, 35.5798], 'loss': 2.0782452796936037}\n",
            "OCS >> Task 2: {'accuracy': 30.13, 'per_class_accuracy': [97.1429, 69.3392, 0.0969, 3.7624, 0.0, 0.1121, 39.8747, 0.0, 41.5811, 44.3013], 'loss': 2.0534886444091796}\n",
            "OCS >> Task 3: {'accuracy': 33.02, 'per_class_accuracy': [97.449, 61.8502, 0.8721, 4.2574, 0.0, 0.2242, 43.737, 0.0, 57.5975, 60.555], 'loss': 2.0017931186676026}\n",
            "OCS >> Task 4: {'accuracy': 34.26, 'per_class_accuracy': [96.3265, 52.2467, 1.2597, 4.5545, 0.0, 0.0, 53.0271, 0.0, 63.039, 70.1685], 'loss': 1.9818205856323243}\n",
            "OCS >> Task 5: {'accuracy': 36.98, 'per_class_accuracy': [95.4082, 64.2291, 2.5194, 5.1485, 0.0, 0.2242, 52.5052, 0.0, 71.5606, 74.7275], 'loss': 1.9441171356201172}\n",
            "OCS >> Task 6: {'accuracy': 38.26, 'per_class_accuracy': [94.7959, 67.489, 4.6512, 4.5545, 0.0, 0.1121, 56.1587, 0.0, 74.3326, 76.7096], 'loss': 1.9227957773208617}\n",
            "OCS >> Task 7: {'accuracy': 38.92, 'per_class_accuracy': [95.3061, 70.2203, 8.7209, 6.8317, 0.0, 0.0, 53.3403, 0.0, 75.154, 75.223], 'loss': 1.90230417804718}\n",
            "OCS >> Task 8: {'accuracy': 39.14, 'per_class_accuracy': [94.5918, 64.9339, 17.345, 9.3069, 0.0, 0.0, 50.8351, 0.0, 74.9487, 75.3221], 'loss': 1.8882563079833985}\n",
            "OCS >> Task 9: {'accuracy': 39.65, 'per_class_accuracy': [95.2041, 71.1894, 21.124, 8.3168, 0.0, 0.0, 46.4509, 0.0, 76.1807, 72.8444], 'loss': 1.871978865814209}\n",
            "OCS >> Task 10: {'accuracy': 42.02, 'per_class_accuracy': [96.2245, 87.5771, 26.2597, 11.9802, 0.0, 0.0, 45.0939, 0.0, 73.306, 72.0515], 'loss': 1.8608796585083007}\n",
            "OCS >> Task 11: {'accuracy': 40.6, 'per_class_accuracy': [95.7143, 79.207, 23.2558, 11.5842, 0.0, 0.0, 36.2213, 0.0, 75.462, 77.7007], 'loss': 1.8552816358566284}\n",
            "OCS >> Task 12: {'accuracy': 38.15, 'per_class_accuracy': [96.9388, 63.8767, 16.2791, 13.8614, 0.0, 0.2242, 28.6013, 0.0, 76.2834, 80.5748], 'loss': 1.852668016433716}\n",
            "OCS >> Task 13: {'accuracy': 38.83, 'per_class_accuracy': [97.3469, 63.8767, 11.7248, 19.802, 0.0, 0.1121, 33.9248, 0.0, 72.6899, 84.1427], 'loss': 1.8275618225097656}\n",
            "OCS >> Task 14: {'accuracy': 38.31, 'per_class_accuracy': [97.1429, 51.8062, 10.8527, 26.6337, 0.0, 0.1121, 34.9687, 0.0, 71.1499, 87.3142], 'loss': 1.806384147644043}\n",
            "OCS >> Task 15: {'accuracy': 38.41, 'per_class_accuracy': [97.6531, 49.0749, 12.1124, 30.8911, 0.0, 0.1121, 36.1169, 0.0, 69.9179, 85.4311], 'loss': 1.771855909538269}\n",
            "OCS >> Task 16: {'accuracy': 39.68, 'per_class_accuracy': [98.2653, 45.815, 16.0853, 39.1089, 0.0, 0.5605, 39.666, 0.0973, 70.2259, 84.6383], 'loss': 1.7417049974441527}\n",
            "OCS >> Task 17: {'accuracy': 43.18, 'per_class_accuracy': [98.4694, 62.467, 18.6047, 49.901, 0.0, 0.6726, 40.8142, 0.0, 73.5113, 82.7552], 'loss': 1.7122707025527955}\n",
            "OCS >> Task 18: {'accuracy': 45.34, 'per_class_accuracy': [97.9592, 71.5419, 22.4806, 55.9406, 0.0, 0.2242, 45.6159, 0.0, 74.3326, 79.4846], 'loss': 1.7147921628952025}\n",
            "OCS >> Task 19: {'accuracy': 45.68, 'per_class_accuracy': [97.7551, 75.3304, 19.8643, 61.8812, 0.0, 0.0, 47.3904, 0.0, 71.7659, 76.5114], 'loss': 1.7259181777954102}\n",
            "OCS >> (average accuracy): 38.33736842105263\n",
            "OCS >> (Forgetting): 0.5993055555555555\n",
            "Maximum per-task accuracies: [97.86]\n",
            "\n",
            "---- Task 20 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 15.24\n",
            "Epoch 0.05 >> (class accuracy): [0.0, 76.7401, 0.0, 32.6733, 0.0, 16.9283, 0.0, 16.6342, 0.1027, 0.0]\n",
            "Epoch 0.1 >> (per-task accuracy): 18.49\n",
            "Epoch 0.1 >> (class accuracy): [0.8163, 74.4493, 0.2907, 68.3168, 0.0, 9.417, 0.0, 20.2335, 1.1294, 0.0]\n",
            "Epoch 0.15 >> (per-task accuracy): 22.72\n",
            "Epoch 0.15 >> (class accuracy): [50.0, 57.3568, 2.1318, 81.6832, 0.0, 1.7937, 0.0, 21.0117, 5.3388, 0.0]\n",
            "Epoch 0.2 >> (per-task accuracy): 22.36\n",
            "Epoch 0.2 >> (class accuracy): [91.0204, 39.0308, 4.5543, 72.0792, 0.0, 0.1121, 0.0, 6.4202, 6.0575, 0.0]\n",
            "Epoch 0.25 >> (per-task accuracy): 17.14\n",
            "Epoch 0.25 >> (class accuracy): [98.8776, 15.5947, 5.1357, 47.0297, 0.0, 0.0, 0.0, 0.2918, 3.7988, 0.0]\n",
            "Epoch 0.3 >> (per-task accuracy): 12.85\n",
            "Epoch 0.3 >> (class accuracy): [100.0, 5.1982, 2.3256, 21.1881, 0.0, 0.0, 0.0, 0.0, 0.8214, 0.0]\n",
            "Epoch 0.35 >> (per-task accuracy): 10.87\n",
            "Epoch 0.35 >> (class accuracy): [100.0, 1.674, 1.0659, 7.5248, 0.0, 0.0, 0.0, 0.0, 0.1027, 0.0]\n",
            "Epoch 0.4 >> (per-task accuracy): 10.04\n",
            "Epoch 0.4 >> (class accuracy): [100.0, 0.0881, 0.5814, 1.6832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.45 >> (per-task accuracy): 9.89\n",
            "Epoch 0.45 >> (class accuracy): [100.0, 0.0, 0.1938, 0.6931, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.5 >> (per-task accuracy): 9.83\n",
            "Epoch 0.5 >> (class accuracy): [100.0, 0.0, 0.0969, 0.198, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.55 >> (per-task accuracy): 9.83\n",
            "Epoch 0.55 >> (class accuracy): [100.0, 0.0, 0.0969, 0.198, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.6 >> (per-task accuracy): 9.86\n",
            "Epoch 0.6 >> (class accuracy): [100.0, 0.0, 0.1938, 0.396, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.65 >> (per-task accuracy): 9.91\n",
            "Epoch 0.65 >> (class accuracy): [100.0, 0.0, 0.2907, 0.6931, 0.0, 0.0, 0.0, 0.0, 0.1027, 0.0]\n",
            "Epoch 0.7 >> (per-task accuracy): 10.08\n",
            "Epoch 0.7 >> (class accuracy): [100.0, 0.0881, 0.4845, 1.5842, 0.0, 0.0, 0.0, 0.0, 0.616, 0.0]\n",
            "Epoch 0.75 >> (per-task accuracy): 10.59\n",
            "Epoch 0.75 >> (class accuracy): [100.0, 0.0881, 1.1628, 4.0594, 0.0, 0.0, 0.0, 0.0, 2.5667, 0.0]\n",
            "Epoch 0.8 >> (per-task accuracy): 11.59\n",
            "Epoch 0.8 >> (class accuracy): [100.0, 0.6167, 2.2287, 7.8218, 0.0, 0.0, 0.0, 0.0, 7.1869, 0.0]\n",
            "Epoch 0.85 >> (per-task accuracy): 13.1\n",
            "Epoch 0.85 >> (class accuracy): [100.0, 1.2335, 3.6822, 12.0792, 0.0, 0.0, 0.0, 0.0, 16.0164, 0.0]\n",
            "Epoch 0.9 >> (per-task accuracy): 15.31\n",
            "Epoch 0.9 >> (class accuracy): [100.0, 2.6432, 6.8798, 18.5149, 0.0, 0.0, 0.0, 0.0, 27.0021, 0.0]\n",
            "Epoch 0.95 >> (per-task accuracy): 17.9\n",
            "Epoch 0.95 >> (class accuracy): [99.898, 5.1982, 11.2403, 26.6337, 0.0, 0.0, 0.1044, 0.0, 37.577, 0.0]\n",
            "Epoch 1.0 >> (per-task accuracy): 20.28\n",
            "Epoch 1.0 >> (class accuracy): [99.6939, 8.9868, 15.407, 32.1782, 0.0, 0.0, 0.1044, 0.0, 47.6386, 0.0]\n",
            "OCS >> Task 1: {'accuracy': 12.69, 'per_class_accuracy': [99.3878, 5.1982, 1.0659, 13.9604, 0.0, 0.0, 0.0, 0.0, 8.6242, 0.0], 'loss': 2.2281780658721924}\n",
            "OCS >> Task 2: {'accuracy': 12.76, 'per_class_accuracy': [99.4898, 0.9692, 1.1628, 13.7624, 0.0, 0.0, 0.0, 0.0, 14.271, 0.0], 'loss': 2.228613575363159}\n",
            "OCS >> Task 3: {'accuracy': 14.36, 'per_class_accuracy': [99.4898, 0.1762, 0.4845, 15.5446, 0.0, 0.0, 0.0, 0.0, 30.4928, 0.0], 'loss': 2.2254727237701415}\n",
            "OCS >> Task 4: {'accuracy': 14.58, 'per_class_accuracy': [99.5918, 0.2643, 0.1938, 12.1782, 0.0, 0.0, 0.0, 0.0, 36.345, 0.0], 'loss': 2.2281336486816405}\n",
            "OCS >> Task 5: {'accuracy': 15.71, 'per_class_accuracy': [99.3878, 0.793, 0.7752, 10.495, 0.0, 0.0, 0.0, 0.0, 48.6653, 0.0], 'loss': 2.2236831413269043}\n",
            "OCS >> Task 6: {'accuracy': 14.98, 'per_class_accuracy': [99.5918, 0.3524, 0.4845, 8.0198, 0.0, 0.0, 0.0, 0.1946, 44.1478, 0.0], 'loss': 2.2223480972290037}\n",
            "OCS >> Task 7: {'accuracy': 14.5, 'per_class_accuracy': [100.0, 1.0573, 0.3876, 9.505, 0.0, 0.0, 0.0, 0.0, 36.7556, 0.0], 'loss': 2.2232793838500977}\n",
            "OCS >> Task 8: {'accuracy': 13.45, 'per_class_accuracy': [100.0, 1.0573, 0.6783, 9.604, 0.0, 0.0, 0.0, 0.0, 25.5647, 0.0], 'loss': 2.221255773162842}\n",
            "OCS >> Task 9: {'accuracy': 12.49, 'per_class_accuracy': [100.0, 1.2335, 0.2907, 10.099, 0.0, 0.0, 0.0, 0.0, 15.4004, 0.0], 'loss': 2.2207025123596194}\n",
            "OCS >> Task 10: {'accuracy': 13.77, 'per_class_accuracy': [99.7959, 13.1278, 0.0969, 11.3861, 0.0, 0.0, 0.0, 0.0, 13.7577, 0.0], 'loss': 2.2125041557312013}\n",
            "OCS >> Task 11: {'accuracy': 14.3, 'per_class_accuracy': [99.7959, 16.0352, 0.0969, 14.4554, 0.0, 0.0, 0.0, 0.0, 12.6283, 0.0], 'loss': 2.210189594268799}\n",
            "OCS >> Task 12: {'accuracy': 13.61, 'per_class_accuracy': [99.898, 9.6035, 0.2907, 16.5347, 0.0, 0.0, 0.0, 0.0, 10.5749, 0.0], 'loss': 2.2090549201965333}\n",
            "OCS >> Task 13: {'accuracy': 15.1, 'per_class_accuracy': [100.0, 15.7709, 0.8721, 15.5446, 0.0, 0.0, 0.0, 0.0, 18.9938, 0.0], 'loss': 2.199309817123413}\n",
            "OCS >> Task 14: {'accuracy': 13.53, 'per_class_accuracy': [100.0, 5.3744, 2.8101, 13.6634, 0.0, 0.0, 0.0, 0.0, 14.8871, 0.0], 'loss': 2.1912263153076172}\n",
            "OCS >> Task 15: {'accuracy': 12.76, 'per_class_accuracy': [99.898, 0.2643, 4.3605, 9.2079, 0.0, 0.0, 0.0, 0.0, 16.0164, 0.0], 'loss': 2.1790715156555174}\n",
            "OCS >> Task 16: {'accuracy': 13.39, 'per_class_accuracy': [99.898, 1.1454, 8.3333, 8.7129, 0.0, 0.0, 0.2088, 0.0, 17.5565, 0.0], 'loss': 2.1651086013793943}\n",
            "OCS >> Task 17: {'accuracy': 15.04, 'per_class_accuracy': [99.898, 1.3216, 12.3062, 13.9604, 0.0, 0.0, 0.7307, 0.0, 24.1273, 0.0], 'loss': 2.149891346359253}\n",
            "OCS >> Task 18: {'accuracy': 16.46, 'per_class_accuracy': [99.898, 4.0529, 16.7636, 17.6238, 0.0, 0.0, 0.3132, 0.0, 27.4127, 0.0], 'loss': 2.13751633644104}\n",
            "OCS >> Task 19: {'accuracy': 18.97, 'per_class_accuracy': [99.7959, 7.9295, 16.2791, 28.2178, 0.0, 0.0, 0.0, 0.0, 38.6037, 0.0], 'loss': 2.1306513118743897}\n",
            "OCS >> Task 20: {'accuracy': 20.28, 'per_class_accuracy': [99.6939, 8.9868, 15.407, 32.1782, 0.0, 0.0, 0.1044, 0.0, 47.6386, 0.0], 'loss': 2.1260689277648925}\n",
            "OCS >> (average accuracy): 14.636500000000002\n",
            "OCS >> (Forgetting): 0.835205263157895\n",
            "Maximum per-task accuracies: [97.86]\n",
            "\n",
            "{'num_tasks': 20, 'per_task_rotation': 9, 'memory_size': 200, 'dataset': 'rot-mnist', 'device': 'cuda', 'momentum': 0.75, 'mlp_hiddens': 256, 'dropout': 0.2, 'lr_decay': 0.75, 'n_classes': 10, 'seq_lr': 0.006, 'stream_size': 100, 'ocspick': True, 'batch_size': 5, 'tau': 900.0, 'ref_hyp': 9.0, 'n_substeps': 20}\n"
          ]
        }
      ],
      "source": [
        "# NEW\n",
        "DATASET = 'rot-mnist'\n",
        "HIDDENS = 256\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "config = {\n",
        "    'num_tasks': 20,\n",
        "    'per_task_rotation': 9,\n",
        "    'memory_size': 200,\n",
        "    'dataset': DATASET,\n",
        "    'device': DEVICE,\n",
        "    'momentum': 0.7,\n",
        "    'mlp_hiddens': HIDDENS,\n",
        "    'dropout': 0.15,\n",
        "    'lr_decay': 0.7 if 'rot-mnist' in DATASET else 0.76,\n",
        "    'n_classes': 10,\n",
        "    'seq_lr': 0.006,\n",
        "    'stream_size': 100,\n",
        "    'ocspick': True,\n",
        "    'batch_size': 5,\n",
        "    #  'tau': 950.0,\n",
        "    'ref_hyp': 10. if 'rot-mnist' in DATASET else 50\n",
        "}\n",
        "\n",
        "log_dir =  f\"./summery/{config['dataset']}\"\n",
        "summary = SummaryWriter(log_dir)\n",
        "\n",
        "experiment = Experiment(api_key=\"hidden_key\", project_name=\"mnist\", disabled=True)\n",
        "\n",
        "loaders = get_all_loaders(config)\n",
        "\n",
        "\n",
        "def evaluate_model(model, task, loaders, config):\n",
        "    accuracies, losses = [], []\n",
        "    for t in range(1, task + 1):\n",
        "        metrics = eval_single_epoch(model, loaders['sequential'][t]['val'], config)\n",
        "        accuracies.append(metrics['accuracy'])\n",
        "        losses.append(metrics['loss'])\n",
        "        print(f'OCS >> Task {t}: {metrics}')\n",
        "    return accuracies, losses\n",
        "\n",
        "def main():\n",
        "    setup_experiment(experiment, config)\n",
        "\n",
        "    max_accuracies = [0.0] * config['num_tasks']\n",
        "    for task in range(1, config['num_tasks'] + 1):\n",
        "        print(f'---- Task {task} (OCS) ----')\n",
        "        model = train_task_sequentially(task, loaders, config, summary)\n",
        "\n",
        "        accuracies, _ = evaluate_model(model, task, loaders, config)\n",
        "        max_accuracies = [max(acc, max_acc) for acc, max_acc in zip(accuracies, max_accuracies)]\n",
        "\n",
        "        avg_accuracy = np.mean(accuracies)\n",
        "        if task > 1:\n",
        "            forgetting = np.mean(np.array(max_accuracies[:task - 1]) - np.array(accuracies[:task - 1]))/ 100\n",
        "        else:\n",
        "            forgetting = 0.0\n",
        "\n",
        "        print(f\"OCS >> (average accuracy): {avg_accuracy}\")\n",
        "        print(f\"OCS >> (Forgetting): {forgetting}\")\n",
        "        summary.add_scalar('cl_average_accuracy', avg_accuracy, task - 1)\n",
        "        print(f'Maximum per-task accuracies: {max_accuracies}\\n')\n",
        "\n",
        "    print(config)\n",
        "    experiment.end()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogU8kdShsqi6",
        "outputId": "520816f7-0ce9-4d0c-97f2-8abfe5f18e37"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "loading coreset placeholder noisy-rot-mnist\n",
            "loading noisy-rot-mnist for task 1\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 2\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 3\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 4\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 5\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 6\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 7\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 8\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 9\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 10\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 11\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 12\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 13\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 14\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 15\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 16\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 17\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 18\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 19\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 20\n",
            "Noisy Rotated MNIST\n",
            "---- Task 1 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 69.86\n",
            "Epoch 0.2 >> (class accuracy): [95.9184, 98.8546, 53.6822, 87.6238, 26.1711, 0.8969, 94.2589, 80.8366, 64.6817, 84.8365]\n",
            "Epoch 0.4 >> (per-task accuracy): 87.25\n",
            "Epoch 0.4 >> (class accuracy): [93.7755, 98.4141, 76.6473, 86.4356, 93.6864, 74.7758, 93.3194, 86.284, 86.345, 80.8722]\n",
            "Epoch 0.6 >> (per-task accuracy): 90.37\n",
            "Epoch 0.6 >> (class accuracy): [95.5102, 98.1498, 83.4302, 87.6238, 94.3992, 83.7444, 95.4071, 88.6187, 88.6037, 87.116]\n",
            "Epoch 0.8 >> (per-task accuracy): 92.03\n",
            "Epoch 0.8 >> (class accuracy): [96.3265, 97.6211, 85.562, 88.5149, 94.8065, 90.3587, 95.1983, 90.3696, 90.7598, 90.3865]\n",
            "Epoch 1.0 >> (per-task accuracy): 93.06\n",
            "Epoch 1.0 >> (class accuracy): [96.9388, 97.8855, 87.5969, 90.5941, 95.0102, 92.6009, 94.9896, 91.7315, 91.8891, 91.0803]\n",
            "OCS >> Task 1: {'accuracy': 93.06, 'per_class_accuracy': [96.9388, 97.8855, 87.5969, 90.5941, 95.0102, 92.6009, 94.9896, 91.7315, 91.8891, 91.0803], 'loss': 0.2877178560972214}\n",
            "OCS >> (average accuracy): 93.06\n",
            "OCS >> (Forgetting): 0.0\n",
            "Maximum per-task accuracies: [93.06]\n",
            "\n",
            "---- Task 2 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 74.54\n",
            "Epoch 0.2 >> (class accuracy): [97.6531, 95.4185, 63.2752, 80.9901, 89.4094, 6.7265, 83.4029, 89.4942, 77.8234, 52.329]\n",
            "Epoch 0.4 >> (per-task accuracy): 86.66\n",
            "Epoch 0.4 >> (class accuracy): [97.3469, 96.4758, 72.8682, 86.3366, 91.0387, 74.7758, 91.2317, 85.7977, 85.5236, 83.5481]\n",
            "Epoch 0.6 >> (per-task accuracy): 89.41\n",
            "Epoch 0.6 >> (class accuracy): [97.551, 97.7974, 78.4884, 87.9208, 91.3442, 84.0807, 92.2756, 90.856, 84.2916, 88.3053]\n",
            "Epoch 0.8 >> (per-task accuracy): 91.23\n",
            "Epoch 0.8 >> (class accuracy): [97.9592, 97.9736, 82.1705, 88.8119, 92.668, 87.7803, 93.9457, 91.7315, 87.9877, 90.4856]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_2118/1459498902.py:12: RuntimeWarning: divide by zero encountered in scalar floor_divide\n",
            "  if (num_residuals // num_class) > 0:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1.0 >> (per-task accuracy): 92.67\n",
            "Epoch 1.0 >> (class accuracy): [98.3673, 98.7665, 85.3682, 90.6931, 93.89, 91.0314, 94.2589, 92.5097, 89.7331, 91.4767]\n",
            "OCS >> Task 1: {'accuracy': 90.16, 'per_class_accuracy': [98.1633, 93.2159, 81.7829, 89.505, 91.9552, 88.1166, 90.7098, 92.0233, 87.0637, 88.8008], 'loss': 0.36457865376472476}\n",
            "OCS >> Task 2: {'accuracy': 92.67, 'per_class_accuracy': [98.3673, 98.7665, 85.3682, 90.6931, 93.89, 91.0314, 94.2589, 92.5097, 89.7331, 91.4767], 'loss': 0.2990399155855179}\n",
            "OCS >> (average accuracy): 91.41499999999999\n",
            "OCS >> (Forgetting): 0.029000000000000057\n",
            "Maximum per-task accuracies: [93.06]\n",
            "\n",
            "---- Task 3 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 54.44\n",
            "Epoch 0.2 >> (class accuracy): [95.3061, 18.6784, 21.2209, 76.7327, 61.0998, 0.0, 71.9207, 55.8366, 91.1704, 54.8067]\n",
            "Epoch 0.4 >> (per-task accuracy): 82.6\n",
            "Epoch 0.4 >> (class accuracy): [94.4898, 93.9207, 71.4147, 82.3762, 91.8534, 70.2915, 90.6054, 87.6459, 83.3676, 58.3746]\n",
            "Epoch 0.6 >> (per-task accuracy): 87.22\n",
            "Epoch 0.6 >> (class accuracy): [96.6327, 95.4185, 76.938, 86.5347, 92.7699, 78.139, 91.023, 89.3969, 85.1129, 78.89]\n",
            "Epoch 0.8 >> (per-task accuracy): 88.89\n",
            "Epoch 0.8 >> (class accuracy): [97.7551, 96.2115, 76.4535, 87.8218, 93.4827, 83.8565, 91.6493, 90.5642, 86.8583, 83.449]\n",
            "Epoch 1.0 >> (per-task accuracy): 90.5\n",
            "Epoch 1.0 >> (class accuracy): [97.551, 97.533, 80.3295, 88.3168, 93.5845, 88.1166, 93.1106, 90.2724, 88.7064, 86.9177]\n",
            "OCS >> Task 1: {'accuracy': 80.55, 'per_class_accuracy': [96.6327, 76.3877, 66.4729, 82.4752, 81.5682, 67.0404, 88.309, 83.0739, 84.9076, 78.89], 'loss': 0.6502226016044617}\n",
            "OCS >> Task 2: {'accuracy': 88.11, 'per_class_accuracy': [96.3265, 92.9515, 79.2636, 87.5248, 87.9837, 78.8117, 92.6931, 89.2023, 88.3984, 86.9177], 'loss': 0.4696975363254547}\n",
            "OCS >> Task 3: {'accuracy': 90.5, 'per_class_accuracy': [97.551, 97.533, 80.3295, 88.3168, 93.5845, 88.1166, 93.1106, 90.2724, 88.7064, 86.9177], 'loss': 0.39954949603080747}\n",
            "OCS >> (average accuracy): 86.38666666666666\n",
            "OCS >> (Forgetting): 0.08730000000000004\n",
            "Maximum per-task accuracies: [93.06]\n",
            "\n",
            "---- Task 4 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 42.61\n",
            "Epoch 0.2 >> (class accuracy): [97.551, 10.1322, 58.624, 0.0, 69.6538, 6.9507, 77.6618, 20.8171, 90.4517, 0.0]\n",
            "Epoch 0.4 >> (per-task accuracy): 80.94\n",
            "Epoch 0.4 >> (class accuracy): [95.102, 92.1586, 74.031, 78.5149, 90.4277, 45.7399, 90.3967, 80.4475, 84.7023, 73.8355]\n",
            "Epoch 0.6 >> (per-task accuracy): 86.0\n",
            "Epoch 0.6 >> (class accuracy): [96.4286, 95.6828, 80.5233, 85.4455, 88.2892, 68.2735, 91.023, 82.7821, 84.7023, 84.3409]\n",
            "Epoch 0.8 >> (per-task accuracy): 88.73\n",
            "Epoch 0.8 >> (class accuracy): [96.5306, 96.3877, 82.1705, 87.3267, 90.224, 81.3901, 93.8413, 87.2568, 86.2423, 84.7374]\n",
            "Epoch 1.0 >> (per-task accuracy): 90.33\n",
            "Epoch 1.0 >> (class accuracy): [97.2449, 96.9163, 85.6589, 89.505, 89.9185, 88.2287, 93.5282, 87.6459, 86.4476, 87.5124]\n",
            "OCS >> Task 1: {'accuracy': 68.89, 'per_class_accuracy': [93.7755, 50.837, 57.5581, 77.0297, 65.5804, 37.7803, 81.7328, 70.5253, 72.5873, 81.7641], 'loss': 0.9996314757347107}\n",
            "OCS >> Task 2: {'accuracy': 79.78, 'per_class_accuracy': [95.102, 75.3304, 72.5775, 86.2376, 72.5051, 59.5291, 86.8476, 78.9883, 83.2649, 86.5213], 'loss': 0.67998657913208}\n",
            "OCS >> Task 3: {'accuracy': 85.82, 'per_class_accuracy': [96.0204, 84.4934, 79.2636, 87.2277, 86.0489, 76.009, 91.6493, 81.9066, 88.501, 87.0168], 'loss': 0.5165616138458252}\n",
            "OCS >> Task 4: {'accuracy': 90.33, 'per_class_accuracy': [97.2449, 96.9163, 85.6589, 89.505, 89.9185, 88.2287, 93.5282, 87.6459, 86.4476, 87.5124], 'loss': 0.4234623016834259}\n",
            "OCS >> (average accuracy): 81.205\n",
            "OCS >> (Forgetting): 0.1489666666666667\n",
            "Maximum per-task accuracies: [93.06]\n",
            "\n",
            "---- Task 5 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 31.76\n",
            "Epoch 0.2 >> (class accuracy): [91.6327, 0.0, 95.9302, 0.0, 68.1263, 0.0, 0.1044, 21.4981, 40.7598, 0.0]\n",
            "Epoch 0.4 >> (per-task accuracy): 75.69\n",
            "Epoch 0.4 >> (class accuracy): [96.8367, 88.1938, 60.8527, 83.1683, 91.446, 25.2242, 87.7871, 76.1673, 80.3901, 61.5461]\n",
            "Epoch 0.6 >> (per-task accuracy): 82.69\n",
            "Epoch 0.6 >> (class accuracy): [96.2245, 93.4802, 68.8953, 85.1485, 88.7984, 68.1614, 91.1273, 82.2957, 76.8994, 73.9346]\n",
            "Epoch 0.8 >> (per-task accuracy): 86.08\n",
            "Epoch 0.8 >> (class accuracy): [96.4286, 94.7137, 78.2946, 83.6634, 87.8819, 75.8969, 92.6931, 87.4514, 81.3142, 80.8722]\n",
            "Epoch 1.0 >> (per-task accuracy): 86.63\n",
            "Epoch 1.0 >> (class accuracy): [97.1429, 95.7709, 77.8101, 82.6733, 89.002, 76.3453, 92.2756, 87.0623, 82.5462, 84.0436]\n",
            "OCS >> Task 1: {'accuracy': 54.26, 'per_class_accuracy': [91.8367, 28.4581, 22.1899, 62.2772, 71.1813, 18.2735, 78.4969, 54.3774, 41.2731, 76.3132], 'loss': 1.4707731842041016}\n",
            "OCS >> Task 2: {'accuracy': 65.86, 'per_class_accuracy': [92.9592, 45.6388, 36.9186, 74.7525, 79.0224, 35.426, 80.4802, 68.7743, 62.8337, 83.1516], 'loss': 1.1031665378570557}\n",
            "OCS >> Task 3: {'accuracy': 74.84, 'per_class_accuracy': [95.0, 65.4626, 53.1977, 78.2178, 87.169, 51.3453, 87.3695, 69.5525, 77.2074, 84.5391], 'loss': 0.8363311784744263}\n",
            "OCS >> Task 4: {'accuracy': 82.99, 'per_class_accuracy': [97.0408, 83.4361, 72.3837, 82.1782, 88.5947, 72.5336, 90.6054, 76.07, 81.0062, 86.0258], 'loss': 0.6424999090194702}\n",
            "OCS >> Task 5: {'accuracy': 86.63, 'per_class_accuracy': [97.1429, 95.7709, 77.8101, 82.6733, 89.002, 76.3453, 92.2756, 87.0623, 82.5462, 84.0436], 'loss': 0.5533527981758117}\n",
            "OCS >> (average accuracy): 72.916\n",
            "OCS >> (Forgetting): 0.23572500000000002\n",
            "Maximum per-task accuracies: [93.06]\n",
            "\n",
            "---- Task 6 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 35.61\n",
            "Epoch 0.2 >> (class accuracy): [48.3673, 47.1366, 25.0, 31.7822, 14.4603, 0.0, 72.4426, 9.6304, 96.5092, 9.7126]\n",
            "Epoch 0.4 >> (per-task accuracy): 62.44\n",
            "Epoch 0.4 >> (class accuracy): [97.8571, 96.3877, 60.562, 79.2079, 83.6049, 0.0, 54.6973, 60.1167, 82.4435, 0.0]\n",
            "Epoch 0.6 >> (per-task accuracy): 73.16\n",
            "Epoch 0.6 >> (class accuracy): [96.3265, 95.4185, 66.2791, 83.8614, 92.3625, 30.7175, 86.4301, 81.8093, 82.0329, 10.8028]\n",
            "Epoch 0.8 >> (per-task accuracy): 81.76\n",
            "Epoch 0.8 >> (class accuracy): [95.6122, 94.8899, 74.9031, 86.1386, 89.002, 61.3229, 88.2046, 84.8249, 83.7782, 55.996]\n",
            "Epoch 1.0 >> (per-task accuracy): 83.91\n",
            "Epoch 1.0 >> (class accuracy): [94.5918, 94.7137, 78.2946, 80.7921, 89.1039, 72.1973, 88.2046, 85.7977, 86.037, 67.4926]\n",
            "OCS >> Task 1: {'accuracy': 42.52, 'per_class_accuracy': [87.0408, 7.2247, 11.6279, 40.0, 69.5519, 18.3857, 64.405, 44.9416, 52.5667, 35.1833], 'loss': 1.7256665395736694}\n",
            "OCS >> Task 2: {'accuracy': 54.19, 'per_class_accuracy': [91.1224, 24.0529, 20.5426, 57.5248, 78.9206, 26.5695, 70.1461, 62.9377, 68.5832, 45.6888], 'loss': 1.4100665378570556}\n",
            "OCS >> Task 3: {'accuracy': 65.11, 'per_class_accuracy': [94.0816, 45.9912, 38.1783, 71.3861, 81.8737, 39.9103, 77.3486, 71.8872, 79.0554, 53.7166], 'loss': 1.1221001358032228}\n",
            "OCS >> Task 4: {'accuracy': 74.45, 'per_class_accuracy': [95.102, 64.6696, 58.3333, 80.0, 83.2994, 55.4933, 82.881, 79.3774, 84.4969, 61.7443], 'loss': 0.8952125219345093}\n",
            "OCS >> Task 5: {'accuracy': 81.99, 'per_class_accuracy': [94.0816, 85.1101, 72.2868, 84.1584, 87.78, 66.1435, 87.3695, 86.6732, 86.2423, 68.8801], 'loss': 0.7371735015869141}\n",
            "OCS >> Task 6: {'accuracy': 83.91, 'per_class_accuracy': [94.5918, 94.7137, 78.2946, 80.7921, 89.1039, 72.1973, 88.2046, 85.7977, 86.037, 67.4926], 'loss': 0.6900866925239563}\n",
            "OCS >> (average accuracy): 67.02833333333332\n",
            "OCS >> (Forgetting): 0.29408000000000006\n",
            "Maximum per-task accuracies: [93.06]\n",
            "\n",
            "---- Task 7 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 20.15\n",
            "Epoch 0.2 >> (class accuracy): [10.4082, 25.022, 2.2287, 91.7822, 26.3747, 0.0, 4.3841, 0.0, 38.809, 0.0]\n",
            "Epoch 0.4 >> (per-task accuracy): 51.83\n",
            "Epoch 0.4 >> (class accuracy): [96.8367, 65.7269, 45.5426, 95.2475, 62.6273, 0.0, 26.4092, 65.9533, 25.3593, 26.0654]\n",
            "Epoch 0.6 >> (per-task accuracy): 69.7\n",
            "Epoch 0.6 >> (class accuracy): [96.1224, 91.8062, 66.2791, 91.0891, 91.5479, 0.5605, 81.0021, 85.8949, 66.6324, 16.8484]\n",
            "Epoch 0.8 >> (per-task accuracy): 78.59\n",
            "Epoch 0.8 >> (class accuracy): [95.0, 93.4802, 72.3837, 87.9208, 90.0204, 38.3408, 88.5177, 85.214, 79.2608, 50.5451]\n",
            "Epoch 1.0 >> (per-task accuracy): 81.8\n",
            "Epoch 1.0 >> (class accuracy): [92.7551, 91.9824, 73.5465, 87.2277, 89.1039, 59.417, 87.8914, 86.7704, 82.4435, 63.9247]\n",
            "OCS >> Task 1: {'accuracy': 33.72, 'per_class_accuracy': [76.6327, 1.4978, 5.5233, 43.8614, 66.0896, 12.8924, 59.7077, 18.5798, 36.653, 21.8038], 'loss': 1.9334324676513672}\n",
            "OCS >> Task 2: {'accuracy': 42.21, 'per_class_accuracy': [79.3878, 7.0485, 9.4961, 62.5743, 73.9308, 17.4888, 65.762, 34.9222, 48.5626, 28.6422], 'loss': 1.687274232673645}\n",
            "OCS >> Task 3: {'accuracy': 52.79, 'per_class_accuracy': [85.7143, 29.163, 20.8333, 74.7525, 78.9206, 23.5426, 73.4864, 50.8755, 60.883, 33.003], 'loss': 1.4187992929458617}\n",
            "OCS >> Task 4: {'accuracy': 62.55, 'per_class_accuracy': [87.9592, 48.7225, 39.3411, 84.1584, 81.8737, 31.3901, 78.7056, 63.035, 69.0965, 42.1209], 'loss': 1.1846622634887696}\n",
            "OCS >> Task 5: {'accuracy': 73.78, 'per_class_accuracy': [90.4082, 69.8678, 59.6899, 89.3069, 84.1141, 43.4978, 83.8205, 79.8638, 79.7741, 56.0951], 'loss': 0.962774165725708}\n",
            "OCS >> Task 6: {'accuracy': 79.18, 'per_class_accuracy': [91.1224, 84.141, 70.4457, 89.3069, 86.7617, 55.8296, 86.952, 83.3658, 81.9302, 59.7621], 'loss': 0.852183678817749}\n",
            "OCS >> Task 7: {'accuracy': 81.8, 'per_class_accuracy': [92.7551, 91.9824, 73.5465, 87.2277, 89.1039, 59.417, 87.8914, 86.7704, 82.4435, 63.9247], 'loss': 0.8122630935668945}\n",
            "OCS >> (average accuracy): 60.86142857142857\n",
            "OCS >> (Forgetting): 0.3568833333333334\n",
            "Maximum per-task accuracies: [93.06]\n",
            "\n",
            "---- Task 8 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 10.57\n",
            "Epoch 0.2 >> (class accuracy): [0.0, 0.0, 0.0, 99.2079, 0.1018, 0.0, 0.7307, 0.0, 4.8255, 0.0]\n",
            "Epoch 0.4 >> (per-task accuracy): 29.89\n",
            "Epoch 0.4 >> (class accuracy): [85.9184, 5.3744, 23.5465, 96.8317, 36.2525, 0.0, 1.8789, 21.7899, 24.7433, 2.5768]\n",
            "Epoch 0.6 >> (per-task accuracy): 62.39\n",
            "Epoch 0.6 >> (class accuracy): [94.1837, 86.8722, 47.3837, 92.5743, 87.169, 0.7848, 66.1795, 76.5564, 60.3696, 3.3697]\n",
            "Epoch 0.8 >> (per-task accuracy): 72.35\n",
            "Epoch 0.8 >> (class accuracy): [94.6939, 93.1278, 65.407, 87.4257, 91.0387, 16.704, 85.9081, 80.7393, 72.3819, 28.8404]\n",
            "Epoch 1.0 >> (per-task accuracy): 77.0\n",
            "Epoch 1.0 >> (class accuracy): [93.1633, 93.5683, 70.4457, 82.7723, 90.4277, 51.9058, 89.7704, 85.5058, 77.6181, 31.3181]\n",
            "OCS >> Task 1: {'accuracy': 29.19, 'per_class_accuracy': [65.2041, 0.1762, 4.1667, 46.8317, 58.6558, 6.7265, 56.0543, 2.7237, 49.384, 7.9286], 'loss': 2.0486788452148437}\n",
            "OCS >> Task 2: {'accuracy': 34.1, 'per_class_accuracy': [71.0204, 1.4978, 5.814, 62.1782, 66.3951, 9.7534, 53.9666, 7.9767, 51.6427, 16.6501], 'loss': 1.910962426185608}\n",
            "OCS >> Task 3: {'accuracy': 41.87, 'per_class_accuracy': [76.3265, 10.9251, 12.3062, 73.0693, 73.1161, 13.2287, 71.7119, 16.1479, 56.7762, 20.6145], 'loss': 1.7130459854125977}\n",
            "OCS >> Task 4: {'accuracy': 51.44, 'per_class_accuracy': [83.8776, 32.6872, 26.5504, 82.6733, 79.2261, 19.3946, 76.4092, 29.3774, 59.3429, 27.6511], 'loss': 1.5172535354614258}\n",
            "OCS >> Task 5: {'accuracy': 64.08, 'per_class_accuracy': [88.7755, 54.4493, 46.3178, 87.4257, 83.9104, 32.5112, 81.3152, 57.4903, 70.3285, 38.6521], 'loss': 1.277708525657654}\n",
            "OCS >> Task 6: {'accuracy': 71.46, 'per_class_accuracy': [91.9388, 68.7225, 63.0814, 88.7129, 87.9837, 44.3946, 85.2818, 71.8872, 75.9754, 35.8771], 'loss': 1.1156532815933227}\n",
            "OCS >> Task 7: {'accuracy': 76.06, 'per_class_accuracy': [93.2653, 83.8767, 71.124, 87.6238, 88.4929, 47.3094, 87.8914, 81.323, 76.5914, 40.2379], 'loss': 1.0237038358688355}\n",
            "OCS >> Task 8: {'accuracy': 77.0, 'per_class_accuracy': [93.1633, 93.5683, 70.4457, 82.7723, 90.4277, 51.9058, 89.7704, 85.5058, 77.6181, 31.3181], 'loss': 0.9915980839729309}\n",
            "OCS >> (average accuracy): 55.650000000000006\n",
            "OCS >> (Forgetting): 0.4046\n",
            "Maximum per-task accuracies: [93.06]\n",
            "\n",
            "---- Task 9 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 10.09\n",
            "Epoch 0.2 >> (class accuracy): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 100.0]\n",
            "Epoch 0.4 >> (per-task accuracy): 46.88\n",
            "Epoch 0.4 >> (class accuracy): [92.2449, 78.1498, 12.5, 95.6436, 0.9165, 6.9507, 55.3236, 51.6537, 18.2752, 48.7611]\n",
            "Epoch 0.6 >> (per-task accuracy): 49.8\n",
            "Epoch 0.6 >> (class accuracy): [95.5102, 59.207, 12.7907, 94.8515, 68.2281, 0.2242, 49.1649, 71.8872, 26.1807, 14.3707]\n",
            "Epoch 0.8 >> (per-task accuracy): 68.43\n",
            "Epoch 0.8 >> (class accuracy): [97.6531, 98.1498, 53.7791, 82.0792, 89.9185, 6.8386, 77.1399, 73.3463, 69.5072, 27.1556]\n",
            "Epoch 1.0 >> (per-task accuracy): 75.57\n",
            "Epoch 1.0 >> (class accuracy): [93.6735, 96.3877, 67.1512, 83.8614, 83.9104, 22.87, 93.1106, 81.1284, 77.5154, 49.1576]\n",
            "OCS >> Task 1: {'accuracy': 25.42, 'per_class_accuracy': [49.1837, 0.9692, 1.4535, 9.3069, 57.7393, 0.4484, 42.7975, 1.9455, 61.7043, 33.4985], 'loss': 2.1351646701812745}\n",
            "OCS >> Task 2: {'accuracy': 29.01, 'per_class_accuracy': [53.8776, 3.8767, 1.2597, 12.3762, 56.5173, 1.2332, 49.2693, 5.0584, 71.9713, 39.6432], 'loss': 2.0241268146514892}\n",
            "OCS >> Task 3: {'accuracy': 37.44, 'per_class_accuracy': [65.8163, 25.2863, 1.7442, 25.7426, 58.0448, 2.5785, 65.8664, 11.4786, 76.7967, 44.004], 'loss': 1.843331653213501}\n",
            "OCS >> Task 4: {'accuracy': 45.42, 'per_class_accuracy': [76.0204, 40.9692, 5.5233, 42.9703, 61.2016, 4.4843, 74.2171, 21.8872, 76.386, 51.5362], 'loss': 1.6706720499038696}\n",
            "OCS >> Task 5: {'accuracy': 55.78, 'per_class_accuracy': [83.8776, 65.6388, 11.4341, 59.901, 63.9511, 8.9686, 79.3319, 48.1518, 78.9528, 55.1041], 'loss': 1.4652541316986083}\n",
            "OCS >> Task 6: {'accuracy': 64.49, 'per_class_accuracy': [87.551, 78.5903, 30.5233, 74.9505, 74.7454, 14.9103, 83.1942, 63.8132, 79.8768, 52.4281], 'loss': 1.2933717164993286}\n",
            "OCS >> Task 7: {'accuracy': 72.04, 'per_class_accuracy': [91.3265, 92.1586, 50.2907, 80.7921, 77.0876, 20.6278, 87.1608, 75.4864, 78.2341, 60.8523], 'loss': 1.1726541427612305}\n",
            "OCS >> Task 8: {'accuracy': 75.4, 'per_class_accuracy': [93.7755, 97.4449, 62.1124, 83.6634, 82.8921, 21.7489, 91.6493, 80.642, 79.7741, 53.221], 'loss': 1.1038589706420898}\n",
            "OCS >> Task 9: {'accuracy': 75.57, 'per_class_accuracy': [93.6735, 96.3877, 67.1512, 83.8614, 83.9104, 22.87, 93.1106, 81.1284, 77.5154, 49.1576], 'loss': 1.0874712701797484}\n",
            "OCS >> (average accuracy): 53.39666666666667\n",
            "OCS >> (Forgetting): 0.42435\n",
            "Maximum per-task accuracies: [93.06]\n",
            "\n",
            "---- Task 10 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 18.21\n",
            "Epoch 0.2 >> (class accuracy): [0.0, 13.9207, 0.0, 66.7327, 0.0, 0.0, 0.0, 0.0, 0.0, 98.0178]\n",
            "Epoch 0.4 >> (per-task accuracy): 44.16\n",
            "Epoch 0.4 >> (class accuracy): [47.7551, 85.3744, 59.7868, 53.5644, 46.7413, 0.0, 67.4322, 0.9728, 1.232, 68.781]\n",
            "Epoch 0.6 >> (per-task accuracy): 58.1\n",
            "Epoch 0.6 >> (class accuracy): [82.449, 93.304, 49.8062, 78.0198, 68.8391, 3.0269, 73.4864, 26.5564, 46.5092, 50.3469]\n",
            "Epoch 0.8 >> (per-task accuracy): 69.73\n",
            "Epoch 0.8 >> (class accuracy): [90.0, 93.0396, 63.3721, 71.5842, 69.8574, 16.704, 76.096, 64.9805, 72.0739, 71.7542]\n",
            "Epoch 1.0 >> (per-task accuracy): 74.21\n",
            "Epoch 1.0 >> (class accuracy): [86.1224, 95.7709, 71.7054, 78.2178, 79.8371, 28.0269, 82.881, 65.6615, 71.3552, 75.5203]\n",
            "OCS >> Task 1: {'accuracy': 29.67, 'per_class_accuracy': [38.7755, 0.2643, 3.0039, 57.9208, 23.9308, 0.1121, 71.9207, 0.3891, 67.8645, 37.4628], 'loss': 2.4564659381866454}\n",
            "OCS >> Task 2: {'accuracy': 29.01, 'per_class_accuracy': [35.2041, 2.9956, 1.7442, 54.5545, 27.1894, 0.0, 65.9708, 1.3619, 69.6099, 35.8771], 'loss': 2.406260417175293}\n",
            "OCS >> Task 3: {'accuracy': 30.36, 'per_class_accuracy': [42.551, 14.185, 1.5504, 53.6634, 33.9104, 0.2242, 66.3883, 1.2646, 64.6817, 28.3449], 'loss': 2.2894153038024903}\n",
            "OCS >> Task 4: {'accuracy': 33.23, 'per_class_accuracy': [49.3878, 36.652, 3.0039, 50.9901, 29.1242, 0.7848, 62.3173, 1.8482, 62.423, 35.6789], 'loss': 2.1121580570220946}\n",
            "OCS >> Task 5: {'accuracy': 38.28, 'per_class_accuracy': [50.8163, 62.7313, 7.7519, 54.1584, 29.2261, 1.7937, 63.4656, 5.1556, 59.5483, 44.3013], 'loss': 1.8330292119979859}\n",
            "OCS >> Task 6: {'accuracy': 44.05, 'per_class_accuracy': [61.1224, 74.8899, 19.6705, 55.5446, 33.6049, 6.9507, 60.4384, 6.0311, 64.4764, 52.6264], 'loss': 1.6223871912002563}\n",
            "OCS >> Task 7: {'accuracy': 53.68, 'per_class_accuracy': [74.3878, 90.6608, 38.9535, 61.0891, 35.6415, 9.9776, 69.4154, 15.1751, 67.8645, 66.4024], 'loss': 1.3657285669326782}\n",
            "OCS >> Task 8: {'accuracy': 63.69, 'per_class_accuracy': [79.7959, 97.8855, 56.9767, 68.6139, 59.9796, 12.5561, 74.9478, 31.9066, 76.694, 69.4747], 'loss': 1.1453039670944214}\n",
            "OCS >> Task 9: {'accuracy': 70.12, 'per_class_accuracy': [84.1837, 97.9736, 68.7984, 71.0891, 68.7373, 20.6278, 83.0898, 52.7237, 75.0513, 71.2587], 'loss': 1.0173247265815735}\n",
            "OCS >> Task 10: {'accuracy': 74.21, 'per_class_accuracy': [86.1224, 95.7709, 71.7054, 78.2178, 79.8371, 28.0269, 82.881, 65.6615, 71.3552, 75.5203], 'loss': 0.9598362526893616}\n",
            "OCS >> (average accuracy): 46.63\n",
            "OCS >> (Forgetting): 0.4949444444444444\n",
            "Maximum per-task accuracies: [93.06]\n",
            "\n",
            "---- Task 11 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 15.35\n",
            "Epoch 0.2 >> (class accuracy): [0.0, 0.0, 0.0, 5.6436, 11.609, 0.0, 89.7704, 4.7665, 46.7146, 0.0]\n",
            "Epoch 0.4 >> (per-task accuracy): 28.32\n",
            "Epoch 0.4 >> (class accuracy): [96.9388, 3.7004, 4.5543, 85.8416, 0.9165, 0.0, 15.3445, 34.9222, 42.1971, 0.0]\n",
            "Epoch 0.6 >> (per-task accuracy): 24.46\n",
            "Epoch 0.6 >> (class accuracy): [99.3878, 23.6123, 11.1434, 76.4356, 0.3055, 0.0, 0.5219, 14.2023, 16.7351, 0.0]\n",
            "Epoch 0.8 >> (per-task accuracy): 27.46\n",
            "Epoch 0.8 >> (class accuracy): [99.7959, 55.7709, 12.5, 56.2376, 1.833, 0.0, 0.4175, 26.1673, 15.0924, 0.0]\n",
            "Epoch 1.0 >> (per-task accuracy): 37.99\n",
            "Epoch 1.0 >> (class accuracy): [99.7959, 78.1498, 20.5426, 56.5347, 21.3849, 0.0, 9.3946, 55.2529, 28.9528, 0.0991]\n",
            "OCS >> Task 1: {'accuracy': 12.6, 'per_class_accuracy': [99.5918, 7.9295, 0.0969, 8.5149, 5.3971, 0.0, 2.1921, 0.7782, 2.5667, 0.0], 'loss': 2.249215414428711}\n",
            "OCS >> Task 2: {'accuracy': 12.29, 'per_class_accuracy': [99.5918, 4.8458, 0.0969, 8.5149, 7.5356, 0.0, 1.9833, 0.3891, 1.4374, 0.0], 'loss': 2.2516628074645997}\n",
            "OCS >> Task 3: {'accuracy': 12.64, 'per_class_accuracy': [99.7959, 12.4229, 0.0969, 6.2376, 6.4155, 0.0, 1.2526, 0.3891, 0.2053, 0.0], 'loss': 2.2474564598083497}\n",
            "OCS >> Task 4: {'accuracy': 12.61, 'per_class_accuracy': [99.898, 13.8326, 0.6783, 4.2574, 5.7026, 0.0, 1.1482, 0.3891, 0.4107, 0.0], 'loss': 2.24526209526062}\n",
            "OCS >> Task 5: {'accuracy': 13.35, 'per_class_accuracy': [99.898, 19.7357, 1.2597, 7.0297, 3.5642, 0.0, 0.6263, 0.3891, 0.308, 0.0], 'loss': 2.2349143241882325}\n",
            "OCS >> Task 6: {'accuracy': 14.94, 'per_class_accuracy': [100.0, 27.6652, 3.1008, 13.2673, 1.7312, 0.0, 0.4175, 0.6809, 0.616, 0.0], 'loss': 2.2198798011779783}\n",
            "OCS >> Task 7: {'accuracy': 17.35, 'per_class_accuracy': [100.0, 37.533, 6.1047, 19.3069, 1.0183, 0.0, 0.7307, 3.1128, 2.2587, 0.0], 'loss': 2.2022514465332033}\n",
            "OCS >> Task 8: {'accuracy': 22.63, 'per_class_accuracy': [99.898, 48.6344, 12.5, 31.9802, 4.6843, 0.0, 2.1921, 12.8405, 8.3162, 0.0], 'loss': 2.1751477306365965}\n",
            "OCS >> Task 9: {'accuracy': 28.92, 'per_class_accuracy': [99.898, 66.3436, 16.5698, 43.4653, 11.609, 0.0, 4.071, 23.249, 16.2218, 0.0], 'loss': 2.1503643642425536}\n",
            "OCS >> Task 10: {'accuracy': 34.13, 'per_class_accuracy': [99.898, 66.9604, 21.2209, 47.7228, 19.7556, 0.0, 8.2463, 42.4125, 27.1047, 0.0], 'loss': 2.126830078125}\n",
            "OCS >> Task 11: {'accuracy': 37.99, 'per_class_accuracy': [99.7959, 78.1498, 20.5426, 56.5347, 21.3849, 0.0, 9.3946, 55.2529, 28.9528, 0.0991], 'loss': 2.1164214447021483}\n",
            "OCS >> (average accuracy): 19.950000000000003\n",
            "OCS >> (Forgetting): 0.74914\n",
            "Maximum per-task accuracies: [93.06]\n",
            "\n",
            "---- Task 12 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 16.2\n",
            "Epoch 0.2 >> (class accuracy): [0.0, 0.0, 0.0, 1.3861, 18.1263, 0.2242, 70.0418, 8.1712, 68.8912, 0.0]\n",
            "Epoch 0.4 >> (per-task accuracy): 29.88\n",
            "Epoch 0.4 >> (class accuracy): [68.8776, 0.0881, 0.8721, 79.0099, 0.5092, 0.0, 9.9165, 67.2179, 73.306, 0.0]\n",
            "Epoch 0.6 >> (per-task accuracy): 34.97\n",
            "Epoch 0.6 >> (class accuracy): [93.6735, 23.6123, 7.0736, 92.1782, 0.0, 0.0, 3.3403, 70.3307, 56.6735, 0.0]\n",
            "Epoch 0.8 >> (per-task accuracy): 37.25\n",
            "Epoch 0.8 >> (class accuracy): [98.2653, 35.9471, 18.7984, 90.0, 0.0, 0.0, 3.9666, 66.9261, 53.9014, 0.0]\n",
            "Epoch 1.0 >> (per-task accuracy): 39.31\n",
            "Epoch 1.0 >> (class accuracy): [98.8776, 45.5507, 29.3605, 81.7822, 0.5092, 0.0, 7.7244, 66.9261, 56.3655, 0.0]\n",
            "OCS >> Task 1: {'accuracy': 14.05, 'per_class_accuracy': [95.3061, 2.9075, 0.0969, 35.3465, 0.0, 0.0, 2.0877, 2.9183, 3.0801, 0.0], 'loss': 2.273450357055664}\n",
            "OCS >> Task 2: {'accuracy': 13.28, 'per_class_accuracy': [94.6939, 1.3216, 0.7752, 31.1881, 0.0, 0.0, 3.0271, 2.3346, 0.924, 0.0], 'loss': 2.2753741363525393}\n",
            "OCS >> Task 3: {'accuracy': 13.48, 'per_class_accuracy': [98.4694, 7.0485, 1.1628, 26.2376, 0.0, 0.0, 1.4614, 0.9728, 0.2053, 0.0], 'loss': 2.2748601081848143}\n",
            "OCS >> Task 4: {'accuracy': 12.3, 'per_class_accuracy': [99.0816, 6.1674, 1.4535, 15.4455, 0.0, 0.0, 1.1482, 0.4864, 0.2053, 0.0], 'loss': 2.275594825363159}\n",
            "OCS >> Task 5: {'accuracy': 12.9, 'per_class_accuracy': [99.4898, 6.696, 1.938, 19.2079, 0.0, 0.0, 1.0438, 0.7782, 0.7187, 0.0], 'loss': 2.2727519298553465}\n",
            "OCS >> Task 6: {'accuracy': 14.17, 'per_class_accuracy': [99.898, 10.6608, 4.0698, 24.4554, 0.0, 0.0, 0.3132, 0.6809, 1.848, 0.0], 'loss': 2.2667674728393554}\n",
            "OCS >> Task 7: {'accuracy': 16.02, 'per_class_accuracy': [99.7959, 7.4009, 5.814, 41.1881, 0.0, 0.0, 0.3132, 1.4591, 4.7228, 0.0], 'loss': 2.2597322944641114}\n",
            "OCS >> Task 8: {'accuracy': 19.37, 'per_class_accuracy': [99.7959, 7.0485, 15.0194, 49.4059, 0.0, 0.0, 0.8351, 6.9066, 14.9897, 0.0], 'loss': 2.245720858001709}\n",
            "OCS >> Task 9: {'accuracy': 24.39, 'per_class_accuracy': [100.0, 8.9868, 20.8333, 64.5545, 0.2037, 0.0, 1.9833, 17.5097, 29.6715, 0.0], 'loss': 2.2299138240814207}\n",
            "OCS >> Task 10: {'accuracy': 30.79, 'per_class_accuracy': [99.4898, 19.1189, 30.4264, 66.0396, 0.0, 0.0, 5.6367, 39.9805, 45.2772, 0.0], 'loss': 2.2120449394226074}\n",
            "OCS >> Task 11: {'accuracy': 37.34, 'per_class_accuracy': [99.1837, 34.5374, 33.0426, 78.3168, 0.7128, 0.0, 8.142, 58.9494, 56.1602, 0.0], 'loss': 2.197387995147705}\n",
            "OCS >> Task 12: {'accuracy': 39.31, 'per_class_accuracy': [98.8776, 45.5507, 29.3605, 81.7822, 0.5092, 0.0, 7.7244, 66.9261, 56.3655, 0.0], 'loss': 2.1913483589172364}\n",
            "OCS >> (average accuracy): 20.616666666666664\n",
            "OCS >> (Forgetting): 0.7414272727272727\n",
            "Maximum per-task accuracies: [93.06]\n",
            "\n",
            "---- Task 13 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 14.81\n",
            "Epoch 0.2 >> (class accuracy): [0.0, 0.0, 0.0, 0.297, 12.6273, 0.0, 56.0543, 0.7782, 83.0595, 0.0]\n",
            "Epoch 0.4 >> (per-task accuracy): 20.76\n",
            "Epoch 0.4 >> (class accuracy): [50.6122, 0.0, 0.0, 36.3366, 3.666, 0.0, 11.5866, 15.0778, 93.5318, 0.0]\n",
            "Epoch 0.6 >> (per-task accuracy): 28.57\n",
            "Epoch 0.6 >> (class accuracy): [88.1633, 0.1762, 4.6512, 77.0297, 0.9165, 0.0, 2.8184, 33.3658, 80.6982, 0.0]\n",
            "Epoch 0.8 >> (per-task accuracy): 30.86\n",
            "Epoch 0.8 >> (class accuracy): [95.7143, 6.9604, 11.3372, 85.3465, 0.5092, 0.0, 1.357, 41.1479, 66.6324, 0.0]\n",
            "Epoch 1.0 >> (per-task accuracy): 34.2\n",
            "Epoch 1.0 >> (class accuracy): [98.0612, 28.0176, 19.2829, 83.8614, 0.9165, 0.0, 1.357, 46.6926, 60.883, 0.0]\n",
            "OCS >> Task 1: {'accuracy': 14.35, 'per_class_accuracy': [80.0, 0.9692, 1.1628, 52.5743, 0.0, 0.0, 0.5219, 0.9728, 8.4189, 0.0], 'loss': 2.2818422721862794}\n",
            "OCS >> Task 2: {'accuracy': 13.39, 'per_class_accuracy': [80.8163, 0.0, 1.938, 48.5149, 0.0, 0.0, 0.5219, 0.6809, 2.5667, 0.0], 'loss': 2.2842007358551024}\n",
            "OCS >> Task 3: {'accuracy': 13.81, 'per_class_accuracy': [92.551, 0.0881, 3.2946, 40.5941, 0.5092, 0.0, 0.7307, 0.2918, 1.4374, 0.0], 'loss': 2.284095333480835}\n",
            "OCS >> Task 4: {'accuracy': 12.92, 'per_class_accuracy': [95.6122, 0.0, 4.2636, 29.1089, 0.1018, 0.0, 0.6263, 0.0973, 0.924, 0.0], 'loss': 2.2852422786712645}\n",
            "OCS >> Task 5: {'accuracy': 12.76, 'per_class_accuracy': [96.6327, 0.1762, 2.3256, 26.0396, 0.0, 0.0, 0.5219, 0.3891, 3.1828, 0.0], 'loss': 2.2847594291687012}\n",
            "OCS >> Task 6: {'accuracy': 13.56, 'per_class_accuracy': [98.7755, 0.1762, 2.5194, 30.7921, 0.1018, 0.0, 0.3132, 0.1946, 4.4148, 0.0], 'loss': 2.2819638134002687}\n",
            "OCS >> Task 7: {'accuracy': 15.29, 'per_class_accuracy': [98.9796, 0.1762, 3.5853, 42.3762, 0.0, 0.0, 0.0, 0.9728, 8.4189, 0.0], 'loss': 2.2790427703857423}\n",
            "OCS >> Task 8: {'accuracy': 17.51, 'per_class_accuracy': [99.1837, 0.0, 8.3333, 47.2277, 0.2037, 0.0, 0.1044, 1.9455, 19.8152, 0.0], 'loss': 2.271386543273926}\n",
            "OCS >> Task 9: {'accuracy': 20.61, 'per_class_accuracy': [99.3878, 0.0, 10.7558, 58.5149, 0.2037, 0.0, 0.1044, 5.7393, 33.1622, 0.0], 'loss': 2.261681074523926}\n",
            "OCS >> Task 10: {'accuracy': 25.05, 'per_class_accuracy': [98.5714, 2.0264, 17.4419, 63.7624, 0.5092, 0.0, 1.1482, 21.2062, 47.0226, 0.0], 'loss': 2.248978328704834}\n",
            "OCS >> Task 11: {'accuracy': 30.93, 'per_class_accuracy': [98.8776, 10.837, 21.6085, 78.5149, 1.4257, 0.0, 1.5658, 37.3541, 58.7269, 0.0], 'loss': 2.2375599773406982}\n",
            "OCS >> Task 12: {'accuracy': 33.88, 'per_class_accuracy': [98.5714, 22.2907, 25.7752, 83.0693, 0.7128, 0.0, 1.0438, 43.0934, 62.0123, 0.0], 'loss': 2.231404112625122}\n",
            "OCS >> Task 13: {'accuracy': 34.2, 'per_class_accuracy': [98.0612, 28.0176, 19.2829, 83.8614, 0.9165, 0.0, 1.357, 46.6926, 60.883, 0.0], 'loss': 2.2276227199554444}\n",
            "OCS >> (average accuracy): 19.866153846153846\n",
            "OCS >> (Forgetting): 0.7438833333333333\n",
            "Maximum per-task accuracies: [93.06]\n",
            "\n",
            "---- Task 14 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 14.52\n",
            "Epoch 0.2 >> (class accuracy): [0.0, 0.0, 0.0, 0.0, 13.9511, 0.0, 59.7077, 1.2646, 74.9487, 0.0]\n",
            "Epoch 0.4 >> (per-task accuracy): 19.62\n",
            "Epoch 0.4 >> (class accuracy): [48.5714, 0.0, 0.0, 17.3267, 11.8126, 0.0, 20.6681, 12.1595, 89.5277, 0.0]\n",
            "Epoch 0.6 >> (per-task accuracy): 24.44\n",
            "Epoch 0.6 >> (class accuracy): [92.8571, 0.0, 0.0, 51.1881, 2.6477, 0.0, 3.5491, 17.607, 79.6715, 0.0]\n",
            "Epoch 0.8 >> (per-task accuracy): 24.05\n",
            "Epoch 0.8 >> (class accuracy): [98.5714, 0.0, 0.0969, 64.3564, 0.611, 0.0, 0.8351, 15.4669, 63.1417, 0.0]\n",
            "Epoch 1.0 >> (per-task accuracy): 22.73\n",
            "Epoch 1.0 >> (class accuracy): [99.5918, 1.8502, 0.1938, 61.0891, 0.1018, 0.0, 0.2088, 14.6887, 51.6427, 0.0]\n",
            "OCS >> Task 1: {'accuracy': 14.22, 'per_class_accuracy': [90.2041, 0.7048, 0.0969, 46.6337, 0.3055, 0.0, 0.0, 0.0973, 5.5441, 0.0], 'loss': 2.285783200836182}\n",
            "OCS >> Task 2: {'accuracy': 12.82, 'per_class_accuracy': [87.8571, 0.0, 0.0, 39.703, 0.5092, 0.0, 0.0, 0.0, 1.54, 0.0], 'loss': 2.2885311935424806}\n",
            "OCS >> Task 3: {'accuracy': 12.55, 'per_class_accuracy': [95.4082, 0.0, 0.0, 30.495, 0.5092, 0.0, 0.0, 0.0, 0.7187, 0.0], 'loss': 2.289075760650635}\n",
            "OCS >> Task 4: {'accuracy': 11.03, 'per_class_accuracy': [97.3469, 0.0, 0.0, 13.8614, 0.5092, 0.0, 0.2088, 0.0, 0.2053, 0.0], 'loss': 2.2901497478485107}\n",
            "OCS >> Task 5: {'accuracy': 10.82, 'per_class_accuracy': [97.449, 0.0, 0.0, 11.5842, 0.0, 0.0, 0.2088, 0.0, 0.8214, 0.0], 'loss': 2.290599298477173}\n",
            "OCS >> Task 6: {'accuracy': 11.1, 'per_class_accuracy': [99.1837, 0.0, 0.0, 12.4752, 0.1018, 0.0, 0.1044, 0.0973, 0.924, 0.0], 'loss': 2.289296035003662}\n",
            "OCS >> Task 7: {'accuracy': 11.68, 'per_class_accuracy': [99.5918, 0.0, 0.0, 15.6436, 0.2037, 0.0, 0.0, 0.2918, 2.9774, 0.0], 'loss': 2.28832234916687}\n",
            "OCS >> Task 8: {'accuracy': 12.5, 'per_class_accuracy': [99.4898, 0.0, 0.0, 19.604, 0.4073, 0.0, 0.0, 0.0973, 7.3922, 0.0], 'loss': 2.2842389961242677}\n",
            "OCS >> Task 9: {'accuracy': 14.01, 'per_class_accuracy': [99.6939, 0.0, 0.0, 25.7426, 0.611, 0.0, 0.0, 0.1946, 16.0164, 0.0], 'loss': 2.2783533180236817}\n",
            "OCS >> Task 10: {'accuracy': 14.92, 'per_class_accuracy': [99.5918, 0.0881, 0.0, 23.4653, 1.3238, 0.0, 0.0, 1.8482, 25.2567, 0.0], 'loss': 2.2696760860443117}\n",
            "OCS >> Task 11: {'accuracy': 17.57, 'per_class_accuracy': [99.5918, 0.0881, 0.0, 35.3465, 1.9348, 0.0, 0.0, 4.3774, 36.8583, 0.0], 'loss': 2.2615542137145996}\n",
            "OCS >> Task 12: {'accuracy': 21.4, 'per_class_accuracy': [99.6939, 1.3216, 0.1938, 50.9901, 1.833, 0.0, 0.0, 10.7004, 51.6427, 0.0], 'loss': 2.2551260250091554}\n",
            "OCS >> Task 13: {'accuracy': 21.09, 'per_class_accuracy': [99.6939, 0.1762, 0.0, 50.0, 1.4257, 0.0, 0.0, 11.8677, 50.2053, 0.0], 'loss': 2.2515022369384767}\n",
            "OCS >> Task 14: {'accuracy': 22.73, 'per_class_accuracy': [99.5918, 1.8502, 0.1938, 61.0891, 0.1018, 0.0, 0.2088, 14.6887, 51.6427, 0.0], 'loss': 2.2494396255493165}\n",
            "OCS >> (average accuracy): 14.888571428571428\n",
            "OCS >> (Forgetting): 0.7877461538461538\n",
            "Maximum per-task accuracies: [93.06]\n",
            "\n",
            "---- Task 15 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 13.32\n",
            "Epoch 0.2 >> (class accuracy): [0.0, 0.0, 0.0, 0.0, 3.666, 0.0, 57.8288, 0.6809, 75.462, 0.0]\n",
            "Epoch 0.4 >> (per-task accuracy): 12.94\n",
            "Epoch 0.4 >> (class accuracy): [4.7959, 0.0, 0.0, 3.9604, 3.8697, 0.0, 28.9144, 4.6693, 86.653, 0.0]\n",
            "Epoch 0.6 >> (per-task accuracy): 18.17\n",
            "Epoch 0.6 >> (class accuracy): [57.0408, 0.0, 0.0, 32.2772, 1.4257, 0.0, 7.9332, 6.5175, 79.5688, 0.0]\n",
            "Epoch 0.8 >> (per-task accuracy): 22.44\n",
            "Epoch 0.8 >> (class accuracy): [86.4286, 0.1762, 0.0, 65.0495, 0.5092, 0.0, 1.4614, 5.5447, 67.9671, 0.0]\n",
            "Epoch 1.0 >> (per-task accuracy): 23.68\n",
            "Epoch 1.0 >> (class accuracy): [95.3061, 0.793, 0.0, 82.0792, 0.0, 0.0, 0.6263, 3.9883, 56.3655, 0.0]\n",
            "OCS >> Task 1: {'accuracy': 14.29, 'per_class_accuracy': [40.7143, 2.2907, 0.0, 87.2277, 0.2037, 0.0, 0.4175, 0.0, 12.0123, 0.0], 'loss': 2.290227568435669}\n",
            "OCS >> Task 2: {'accuracy': 12.22, 'per_class_accuracy': [36.2245, 0.3524, 0.0, 82.4752, 0.5092, 0.0, 0.2088, 0.0, 2.3614, 0.0], 'loss': 2.293089298248291}\n",
            "OCS >> Task 3: {'accuracy': 13.17, 'per_class_accuracy': [51.5306, 0.0, 0.0, 78.1188, 0.7128, 0.0, 0.5219, 0.0, 1.1294, 0.0], 'loss': 2.2940421432495115}\n",
            "OCS >> Task 4: {'accuracy': 13.96, 'per_class_accuracy': [69.0816, 0.0, 0.0, 69.1089, 0.3055, 0.0, 0.6263, 0.0, 1.232, 0.0], 'loss': 2.2952508033752443}\n",
            "OCS >> Task 5: {'accuracy': 13.59, 'per_class_accuracy': [72.0408, 0.0881, 0.0969, 60.495, 0.2037, 0.0, 0.2088, 0.0, 3.6961, 0.0], 'loss': 2.2960031944274903}\n",
            "OCS >> Task 6: {'accuracy': 13.39, 'per_class_accuracy': [73.1633, 0.0, 0.0, 57.3267, 0.1018, 0.0, 0.2088, 0.0, 4.1068, 0.0], 'loss': 2.2952016590118407}\n",
            "OCS >> Task 7: {'accuracy': 14.91, 'per_class_accuracy': [83.4694, 0.0, 0.0, 58.9109, 0.1018, 0.0, 0.0, 0.1946, 7.7002, 0.0], 'loss': 2.295253273773193}\n",
            "OCS >> Task 8: {'accuracy': 16.01, 'per_class_accuracy': [84.7959, 0.0, 0.0, 60.7921, 1.222, 0.0, 0.0, 0.0, 14.7844, 0.0], 'loss': 2.292648188018799}\n",
            "OCS >> Task 9: {'accuracy': 17.97, 'per_class_accuracy': [93.8776, 0.0, 0.0, 59.802, 1.0183, 0.0, 0.1044, 0.0, 26.8994, 0.0], 'loss': 2.2888580810546877}\n",
            "OCS >> Task 10: {'accuracy': 18.92, 'per_class_accuracy': [89.6939, 0.0, 0.0, 57.3267, 2.2403, 0.0, 0.4175, 0.0, 41.8891, 0.0], 'loss': 2.283148910140991}\n",
            "OCS >> Task 11: {'accuracy': 21.55, 'per_class_accuracy': [94.7959, 0.0, 0.0, 70.198, 2.5458, 0.0, 0.4175, 0.5837, 49.4867, 0.0], 'loss': 2.278059214401245}\n",
            "OCS >> Task 12: {'accuracy': 23.2, 'per_class_accuracy': [95.6122, 0.4405, 0.0, 78.0198, 2.1385, 0.0, 0.2088, 1.1673, 56.9815, 0.0], 'loss': 2.2730497737884523}\n",
            "OCS >> Task 13: {'accuracy': 22.75, 'per_class_accuracy': [94.2857, 0.3524, 0.0, 78.6139, 1.4257, 0.0, 0.2088, 0.4864, 54.6201, 0.0], 'loss': 2.269198041152954}\n",
            "OCS >> Task 14: {'accuracy': 24.03, 'per_class_accuracy': [96.7347, 1.5859, 0.0, 82.2772, 0.2037, 0.0, 1.6701, 3.4047, 56.7762, 0.0], 'loss': 2.2664538875579834}\n",
            "OCS >> Task 15: {'accuracy': 23.68, 'per_class_accuracy': [95.3061, 0.793, 0.0, 82.0792, 0.0, 0.0, 0.6263, 3.9883, 56.3655, 0.0], 'loss': 2.2669040550231934}\n",
            "OCS >> (average accuracy): 17.576\n",
            "OCS >> (Forgetting): 0.7592\n",
            "Maximum per-task accuracies: [93.06]\n",
            "\n",
            "---- Task 16 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 12.88\n",
            "Epoch 0.2 >> (class accuracy): [0.0, 0.0, 0.0, 0.0, 4.6843, 0.0, 65.4489, 1.1673, 61.9097, 0.0]\n",
            "Epoch 0.4 >> (per-task accuracy): 13.03\n",
            "Epoch 0.4 >> (class accuracy): [1.0204, 0.0, 0.0, 1.1881, 5.499, 0.0, 44.2589, 4.3774, 77.8234, 0.0]\n",
            "Epoch 0.6 >> (per-task accuracy): 15.6\n",
            "Epoch 0.6 >> (class accuracy): [38.3673, 0.0, 0.0, 9.3069, 5.9063, 0.0, 17.2234, 7.9767, 80.5955, 0.0]\n",
            "Epoch 0.8 >> (per-task accuracy): 19.11\n",
            "Epoch 0.8 >> (class accuracy): [75.102, 0.0881, 0.0, 23.9604, 3.9715, 0.0, 5.7411, 8.9494, 76.5914, 0.0]\n",
            "Epoch 1.0 >> (per-task accuracy): 21.66\n",
            "Epoch 1.0 >> (class accuracy): [91.1224, 1.4978, 0.0, 44.8515, 2.6477, 0.0, 1.4614, 8.2685, 69.6099, 0.0]\n",
            "OCS >> Task 1: {'accuracy': 14.68, 'per_class_accuracy': [41.6327, 1.1454, 0.0, 63.0693, 4.0733, 0.0, 3.7578, 0.3891, 33.8809, 0.0], 'loss': 2.293031171798706}\n",
            "OCS >> Task 2: {'accuracy': 12.09, 'per_class_accuracy': [34.7959, 0.2643, 0.0, 60.396, 6.721, 0.0, 3.2359, 0.2918, 15.9138, 0.0], 'loss': 2.29574969329834}\n",
            "OCS >> Task 3: {'accuracy': 11.73, 'per_class_accuracy': [50.102, 0.0, 0.0, 51.7822, 6.11, 0.0, 2.9228, 0.2918, 6.9815, 0.0], 'loss': 2.2967040126800535}\n",
            "OCS >> Task 4: {'accuracy': 12.2, 'per_class_accuracy': [68.9796, 0.0, 0.0, 39.0099, 4.7862, 0.0, 2.9228, 0.0, 7.7002, 0.0], 'loss': 2.2977659488677977}\n",
            "OCS >> Task 5: {'accuracy': 11.69, 'per_class_accuracy': [70.102, 0.0, 0.0, 29.901, 4.1752, 0.0, 1.7745, 0.0, 12.5257, 0.0], 'loss': 2.2986653144836424}\n",
            "OCS >> Task 6: {'accuracy': 11.67, 'per_class_accuracy': [69.0816, 0.0, 0.0, 30.297, 3.5642, 0.0, 1.0438, 0.0, 14.271, 0.0], 'loss': 2.297766203689575}\n",
            "OCS >> Task 7: {'accuracy': 13.84, 'per_class_accuracy': [81.2245, 0.0, 0.0, 31.2871, 1.222, 0.0, 0.1044, 0.0973, 26.4887, 0.0], 'loss': 2.2982042461395262}\n",
            "OCS >> Task 8: {'accuracy': 15.76, 'per_class_accuracy': [81.6327, 0.0, 0.0, 29.901, 6.6191, 0.0, 1.4614, 0.0, 40.5544, 0.0], 'loss': 2.296525575256348}\n",
            "OCS >> Task 9: {'accuracy': 17.57, 'per_class_accuracy': [92.0408, 0.0, 0.0969, 27.3267, 7.7393, 0.0, 2.2965, 0.0, 49.2813, 0.0], 'loss': 2.293757522583008}\n",
            "OCS >> Task 10: {'accuracy': 18.58, 'per_class_accuracy': [86.1224, 0.0, 0.0, 27.9208, 8.1466, 0.0, 4.5929, 0.0973, 62.3203, 0.0], 'loss': 2.289511206436157}\n",
            "OCS >> Task 11: {'accuracy': 20.72, 'per_class_accuracy': [92.449, 0.0, 0.0, 37.3267, 10.6925, 0.0, 2.714, 0.4864, 67.0431, 0.0], 'loss': 2.2863647563934326}\n",
            "OCS >> Task 12: {'accuracy': 21.88, 'per_class_accuracy': [93.4694, 0.0, 0.0, 43.6634, 8.2485, 0.0, 1.357, 0.5837, 75.0513, 0.0], 'loss': 2.283142964172363}\n",
            "OCS >> Task 13: {'accuracy': 21.34, 'per_class_accuracy': [91.8367, 0.0, 0.0, 41.9802, 7.332, 0.0, 1.0438, 0.3891, 74.3326, 0.0], 'loss': 2.280201738739014}\n",
            "OCS >> Task 14: {'accuracy': 22.1, 'per_class_accuracy': [95.102, 0.0881, 0.0, 44.2574, 3.8697, 0.0, 2.6096, 1.4591, 77.2074, 0.0], 'loss': 2.277311894226074}\n",
            "OCS >> Task 15: {'accuracy': 21.49, 'per_class_accuracy': [91.9388, 0.0881, 0.0, 38.9109, 2.8513, 0.0, 1.4614, 4.0856, 79.0554, 0.0], 'loss': 2.277393412017822}\n",
            "OCS >> Task 16: {'accuracy': 21.66, 'per_class_accuracy': [91.1224, 1.4978, 0.0, 44.8515, 2.6477, 0.0, 1.4614, 8.2685, 69.6099, 0.0], 'loss': 2.2767590507507323}\n",
            "OCS >> (average accuracy): 16.8125\n",
            "OCS >> (Forgetting): 0.7657066666666666\n",
            "Maximum per-task accuracies: [93.06]\n",
            "\n",
            "---- Task 17 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 12.69\n",
            "Epoch 0.2 >> (class accuracy): [0.0, 0.0, 0.0, 0.0, 2.2403, 0.0, 74.9478, 3.0156, 51.1294, 0.0]\n",
            "Epoch 0.4 >> (per-task accuracy): 13.29\n",
            "Epoch 0.4 >> (class accuracy): [0.102, 0.0, 0.0, 0.8911, 2.0367, 0.0, 52.714, 8.2685, 72.7926, 0.0]\n",
            "Epoch 0.6 >> (per-task accuracy): 12.84\n",
            "Epoch 0.6 >> (class accuracy): [12.9592, 0.0, 0.0, 3.7624, 1.9348, 0.0, 22.1294, 11.3813, 79.1581, 0.0]\n",
            "Epoch 0.8 >> (per-task accuracy): 15.51\n",
            "Epoch 0.8 >> (class accuracy): [52.2449, 0.0, 0.0, 8.0198, 1.222, 0.0, 6.1587, 13.035, 77.3101, 0.0]\n",
            "Epoch 1.0 >> (per-task accuracy): 17.7\n",
            "Epoch 1.0 >> (class accuracy): [75.2041, 0.793, 0.0, 16.3366, 0.8147, 0.0, 1.2526, 12.8405, 72.5873, 0.0]\n",
            "OCS >> Task 1: {'accuracy': 14.82, 'per_class_accuracy': [19.2857, 0.5286, 0.0, 63.2673, 4.9898, 0.0, 2.714, 0.7782, 58.0082, 0.0], 'loss': 2.294387199020386}\n",
            "OCS >> Task 2: {'accuracy': 12.54, 'per_class_accuracy': [15.5102, 0.1762, 0.0, 67.3267, 5.7026, 0.0, 1.357, 0.7782, 35.2156, 0.0], 'loss': 2.2970818618774413}\n",
            "OCS >> Task 3: {'accuracy': 10.57, 'per_class_accuracy': [24.0816, 0.0, 0.0, 52.2772, 7.8411, 0.0, 1.7745, 0.5837, 19.8152, 0.0], 'loss': 2.298094871520996}\n",
            "OCS >> Task 4: {'accuracy': 11.76, 'per_class_accuracy': [42.1429, 0.0, 0.0, 48.3168, 6.0081, 0.0, 1.4614, 0.0, 20.7392, 0.0], 'loss': 2.29924553565979}\n",
            "OCS >> Task 5: {'accuracy': 11.11, 'per_class_accuracy': [41.9388, 0.0, 0.0, 34.7525, 3.4623, 0.0, 0.6263, 0.0, 31.7248, 0.0], 'loss': 2.300351135635376}\n",
            "OCS >> Task 6: {'accuracy': 10.89, 'per_class_accuracy': [39.4898, 0.0, 0.0, 35.6436, 2.9532, 0.0, 0.6263, 0.0, 31.5195, 0.0], 'loss': 2.299477098083496}\n",
            "OCS >> Task 7: {'accuracy': 13.1, 'per_class_accuracy': [50.3061, 0.0, 0.0, 34.8515, 1.3238, 0.0, 0.3132, 0.1946, 45.8932, 0.0], 'loss': 2.2998846519470213}\n",
            "OCS >> Task 8: {'accuracy': 15.13, 'per_class_accuracy': [48.9796, 0.0, 0.0, 32.1782, 5.8045, 0.0, 0.9395, 0.0, 65.9138, 0.0], 'loss': 2.2987404933929443}\n",
            "OCS >> Task 9: {'accuracy': 17.15, 'per_class_accuracy': [67.1429, 0.0, 0.0969, 21.8812, 7.8411, 0.0, 1.357, 0.0, 76.4887, 0.0], 'loss': 2.296449824142456}\n",
            "OCS >> Task 10: {'accuracy': 17.05, 'per_class_accuracy': [61.7347, 0.0, 0.0, 18.6139, 8.6558, 0.0, 1.5658, 0.0, 83.3676, 0.0], 'loss': 2.293051957321167}\n",
            "OCS >> Task 11: {'accuracy': 19.46, 'per_class_accuracy': [72.6531, 0.0, 0.0, 23.5644, 10.6925, 0.0, 1.6701, 0.2918, 89.5277, 0.0], 'loss': 2.2910119663238526}\n",
            "OCS >> Task 12: {'accuracy': 19.4, 'per_class_accuracy': [74.3878, 0.0, 0.0, 19.901, 9.5723, 0.0, 0.7307, 0.3891, 92.9158, 0.0], 'loss': 2.2889358306884766}\n",
            "OCS >> Task 13: {'accuracy': 19.38, 'per_class_accuracy': [79.6939, 0.0, 0.0, 16.9307, 8.0448, 0.0, 0.4175, 0.0973, 92.6078, 0.0], 'loss': 2.2871567890167235}\n",
            "OCS >> Task 14: {'accuracy': 19.03, 'per_class_accuracy': [82.1429, 0.0, 0.0, 12.3762, 4.6843, 0.0, 1.9833, 0.6809, 92.5051, 0.0], 'loss': 2.284754762649536}\n",
            "OCS >> Task 15: {'accuracy': 16.87, 'per_class_accuracy': [69.2857, 0.0, 0.0, 6.2376, 2.8513, 0.0, 1.2526, 2.7237, 90.0411, 0.0], 'loss': 2.2847497528076173}\n",
            "OCS >> Task 16: {'accuracy': 16.78, 'per_class_accuracy': [69.3878, 0.1762, 0.0, 8.7129, 2.1385, 0.0, 0.6263, 5.642, 84.4969, 0.0], 'loss': 2.2838305698394774}\n",
            "OCS >> Task 17: {'accuracy': 17.7, 'per_class_accuracy': [75.2041, 0.793, 0.0, 16.3366, 0.8147, 0.0, 1.2526, 12.8405, 72.5873, 0.0], 'loss': 2.282112044143677}\n",
            "OCS >> (average accuracy): 15.455294117647059\n",
            "OCS >> (Forgetting): 0.7774500000000001\n",
            "Maximum per-task accuracies: [93.06]\n",
            "\n",
            "---- Task 18 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 10.79\n",
            "Epoch 0.2 >> (class accuracy): [0.0, 0.0, 0.0, 0.0, 2.3422, 0.0, 78.2881, 0.7782, 30.5955, 0.0]\n",
            "Epoch 0.4 >> (per-task accuracy): 11.01\n",
            "Epoch 0.4 >> (class accuracy): [0.0, 0.0, 0.0, 0.396, 1.7312, 0.0, 61.8998, 1.4591, 48.46, 0.0]\n",
            "Epoch 0.6 >> (per-task accuracy): 10.81\n",
            "Epoch 0.6 >> (class accuracy): [0.4082, 0.0, 0.0, 1.1881, 1.0183, 0.0, 45.0939, 2.821, 60.9856, 0.0]\n",
            "Epoch 0.8 >> (per-task accuracy): 11.41\n",
            "Epoch 0.8 >> (class accuracy): [14.7959, 0.0, 0.0, 2.6733, 0.4073, 0.0, 27.9749, 3.6965, 67.6591, 0.0]\n",
            "Epoch 1.0 >> (per-task accuracy): 13.74\n",
            "Epoch 1.0 >> (class accuracy): [45.0, 0.8811, 0.0, 5.9406, 0.2037, 0.0, 14.7182, 4.572, 69.0965, 0.0]\n",
            "OCS >> Task 1: {'accuracy': 13.07, 'per_class_accuracy': [15.5102, 0.6167, 0.0, 30.7921, 0.8147, 0.0, 15.4489, 1.07, 68.7885, 0.0], 'loss': 2.2958788856506347}\n",
            "OCS >> Task 2: {'accuracy': 10.85, 'per_class_accuracy': [10.9184, 0.793, 0.0, 34.6535, 0.9165, 0.0, 9.8121, 1.1673, 51.7454, 0.0], 'loss': 2.298536580657959}\n",
            "OCS >> Task 3: {'accuracy': 8.63, 'per_class_accuracy': [17.7551, 0.0, 0.0, 25.4455, 1.5275, 0.0, 11.4823, 0.8755, 30.5955, 0.0], 'loss': 2.299565718841553}\n",
            "OCS >> Task 4: {'accuracy': 9.75, 'per_class_accuracy': [36.5306, 0.0, 0.0, 23.6634, 0.9165, 0.0, 8.977, 0.0, 29.0554, 0.0], 'loss': 2.3006816482543946}\n",
            "OCS >> Task 5: {'accuracy': 8.89, 'per_class_accuracy': [33.4694, 0.0, 0.0, 14.2574, 0.5092, 0.0, 6.6806, 0.0973, 35.6263, 0.0], 'loss': 2.301934159088135}\n",
            "OCS >> Task 6: {'accuracy': 9.04, 'per_class_accuracy': [29.7959, 0.0, 0.0, 21.4851, 0.2037, 0.0, 5.2192, 0.0, 35.2156, 0.0], 'loss': 2.3010049781799315}\n",
            "OCS >> Task 7: {'accuracy': 11.71, 'per_class_accuracy': [40.4082, 0.0, 0.0, 21.3861, 0.2037, 0.0, 6.6806, 0.2918, 50.308, 0.0], 'loss': 2.3015155834198}\n",
            "OCS >> Task 8: {'accuracy': 13.16, 'per_class_accuracy': [35.6122, 0.0, 0.0, 19.4059, 1.5275, 0.0, 8.8727, 0.0973, 68.7885, 0.0], 'loss': 2.300543069458008}\n",
            "OCS >> Task 9: {'accuracy': 14.38, 'per_class_accuracy': [47.9592, 0.0, 0.0, 12.1782, 1.6293, 0.0, 10.2296, 0.0, 75.0513, 0.0], 'loss': 2.2984814888000487}\n",
            "OCS >> Task 10: {'accuracy': 15.26, 'per_class_accuracy': [41.3265, 0.0, 0.0, 10.297, 2.9532, 0.0, 14.5094, 0.0973, 87.0637, 0.0], 'loss': 2.2957225357055666}\n",
            "OCS >> Task 11: {'accuracy': 16.37, 'per_class_accuracy': [51.1224, 0.0, 0.0, 12.7723, 2.2403, 0.0, 9.1858, 0.2918, 91.7864, 0.0], 'loss': 2.2941594009399413}\n",
            "OCS >> Task 12: {'accuracy': 15.21, 'per_class_accuracy': [43.9796, 0.0, 0.0, 9.1089, 1.9348, 0.0, 6.6806, 0.2918, 93.6345, 0.0], 'loss': 2.2927136501312257}\n",
            "OCS >> Task 13: {'accuracy': 16.6, 'per_class_accuracy': [60.4082, 0.0, 0.0, 7.0297, 1.4257, 0.0, 5.7411, 0.0, 95.2772, 0.0], 'loss': 2.2917848571777344}\n",
            "OCS >> Task 14: {'accuracy': 16.71, 'per_class_accuracy': [62.6531, 0.0881, 0.0, 3.9604, 1.0183, 0.0, 8.3507, 0.1946, 94.8665, 0.0], 'loss': 2.290161433792114}\n",
            "OCS >> Task 15: {'accuracy': 14.79, 'per_class_accuracy': [47.0408, 0.0, 0.0, 1.7822, 0.3055, 0.0, 8.6639, 1.4591, 92.2998, 0.0], 'loss': 2.290489799118042}\n",
            "OCS >> Task 16: {'accuracy': 14.81, 'per_class_accuracy': [50.5102, 0.3524, 0.0, 1.7822, 0.1018, 0.0, 7.2025, 3.7938, 87.7823, 0.0], 'loss': 2.2895786952972412}\n",
            "OCS >> Task 17: {'accuracy': 15.93, 'per_class_accuracy': [55.3061, 0.8811, 0.0, 5.1485, 0.1018, 0.0, 10.8559, 10.0195, 80.1848, 0.0], 'loss': 2.287637045669556}\n",
            "OCS >> Task 18: {'accuracy': 13.74, 'per_class_accuracy': [45.0, 0.8811, 0.0, 5.9406, 0.2037, 0.0, 14.7182, 4.572, 69.0965, 0.0], 'loss': 2.288738817214966}\n",
            "OCS >> (average accuracy): 13.272222222222224\n",
            "OCS >> (Forgetting): 0.7981529411764707\n",
            "Maximum per-task accuracies: [93.06]\n",
            "\n",
            "---- Task 19 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 11.27\n",
            "Epoch 0.2 >> (class accuracy): [0.0, 0.0, 0.0, 0.099, 2.6477, 0.0, 91.4405, 0.1946, 22.7926, 0.0]\n",
            "Epoch 0.4 >> (per-task accuracy): 11.86\n",
            "Epoch 0.4 >> (class accuracy): [0.0, 0.0, 0.0, 0.297, 2.3422, 0.0, 84.9687, 0.6809, 34.8049, 0.0]\n",
            "Epoch 0.6 >> (per-task accuracy): 12.19\n",
            "Epoch 0.6 >> (class accuracy): [0.4082, 0.0, 0.0, 0.5941, 1.6293, 0.0, 76.7223, 1.07, 45.8932, 0.0]\n",
            "Epoch 0.8 >> (per-task accuracy): 12.39\n",
            "Epoch 0.8 >> (class accuracy): [2.551, 0.0, 0.0, 0.8911, 0.9165, 0.0, 65.2401, 1.5564, 56.9815, 0.0]\n",
            "Epoch 1.0 >> (per-task accuracy): 13.54\n",
            "Epoch 1.0 >> (class accuracy): [20.102, 0.0, 0.0969, 1.9802, 0.611, 0.0, 53.9666, 2.0428, 60.7803, 0.0]\n",
            "OCS >> Task 1: {'accuracy': 12.11, 'per_class_accuracy': [5.102, 0.0, 0.0, 16.9307, 3.2587, 0.0, 31.7328, 2.6265, 64.3737, 0.0], 'loss': 2.296908724594116}\n",
            "OCS >> Task 2: {'accuracy': 10.26, 'per_class_accuracy': [2.6531, 0.0881, 0.0, 18.3168, 3.055, 0.0, 26.3048, 2.6265, 51.848, 0.0], 'loss': 2.2996174320220946}\n",
            "OCS >> Task 3: {'accuracy': 8.12, 'per_class_accuracy': [6.3265, 0.0, 0.0, 14.5545, 3.7678, 0.0, 27.3486, 2.1401, 28.9528, 0.0], 'loss': 2.3008054599761962}\n",
            "OCS >> Task 4: {'accuracy': 8.3, 'per_class_accuracy': [18.6735, 0.0, 0.0, 16.2376, 3.4623, 0.0, 19.7286, 0.2918, 26.386, 0.0], 'loss': 2.3018847518920897}\n",
            "OCS >> Task 5: {'accuracy': 7.12, 'per_class_accuracy': [15.7143, 0.0, 0.0, 8.3168, 1.4257, 0.0, 19.7286, 0.4864, 27.3101, 0.0], 'loss': 2.303287773513794}\n",
            "OCS >> Task 6: {'accuracy': 6.96, 'per_class_accuracy': [11.9388, 0.0, 0.0, 16.8317, 0.9165, 0.0, 13.8831, 0.0, 27.4127, 0.0], 'loss': 2.3023429779052735}\n",
            "OCS >> Task 7: {'accuracy': 8.39, 'per_class_accuracy': [16.7347, 0.0, 0.0, 16.2376, 0.2037, 0.0, 14.9269, 0.2918, 37.269, 0.0], 'loss': 2.302867391204834}\n",
            "OCS >> Task 8: {'accuracy': 10.36, 'per_class_accuracy': [11.6327, 0.0, 0.0, 16.1386, 2.3422, 0.0, 18.1628, 0.0973, 57.5975, 0.0], 'loss': 2.302050115585327}\n",
            "OCS >> Task 9: {'accuracy': 11.28, 'per_class_accuracy': [18.2653, 0.0, 0.0, 9.3069, 3.8697, 0.0, 21.5031, 0.0, 62.731, 0.0], 'loss': 2.300146363449097}\n",
            "OCS >> Task 10: {'accuracy': 13.12, 'per_class_accuracy': [15.102, 0.0, 0.0, 9.1089, 4.888, 0.0, 26.9311, 0.1946, 78.4394, 0.0], 'loss': 2.297608854675293}\n",
            "OCS >> Task 11: {'accuracy': 13.13, 'per_class_accuracy': [14.2857, 0.0, 0.0, 9.3069, 4.277, 0.0, 20.1461, 0.3891, 86.2423, 0.0], 'loss': 2.296417280578613}\n",
            "OCS >> Task 12: {'accuracy': 12.28, 'per_class_accuracy': [10.9184, 0.0, 0.0, 5.4455, 4.277, 0.0, 14.8225, 0.5837, 89.9384, 0.0], 'loss': 2.2954167518615725}\n",
            "OCS >> Task 13: {'accuracy': 13.44, 'per_class_accuracy': [24.1837, 0.0, 0.0, 4.1584, 3.1568, 0.0, 12.6305, 0.1946, 93.5318, 0.0], 'loss': 2.294944245147705}\n",
            "OCS >> Task 14: {'accuracy': 13.94, 'per_class_accuracy': [26.5306, 0.0, 0.0, 1.6832, 1.833, 0.0, 19.3111, 1.1673, 92.6078, 0.0], 'loss': 2.293726728439331}\n",
            "OCS >> Task 15: {'accuracy': 12.81, 'per_class_accuracy': [18.1633, 0.0, 0.0, 0.5941, 0.611, 0.0, 19.9374, 2.5292, 89.7331, 0.0], 'loss': 2.29456918258667}\n",
            "OCS >> Task 16: {'accuracy': 13.01, 'per_class_accuracy': [22.449, 0.0, 0.0, 0.9901, 0.3055, 0.0, 19.4154, 4.4747, 85.8316, 0.0], 'loss': 2.293813759613037}\n",
            "OCS >> Task 17: {'accuracy': 14.6, 'per_class_accuracy': [26.9388, 0.0, 0.0, 2.1782, 0.3055, 0.0, 27.8706, 12.2568, 79.8768, 0.0], 'loss': 2.2918321140289306}\n",
            "OCS >> Task 18: {'accuracy': 12.09, 'per_class_accuracy': [16.7347, 0.0, 0.0, 2.4752, 0.7128, 0.0, 32.881, 5.5447, 65.8111, 0.0], 'loss': 2.292768858718872}\n",
            "OCS >> Task 19: {'accuracy': 13.54, 'per_class_accuracy': [20.102, 0.0, 0.0969, 1.9802, 0.611, 0.0, 53.9666, 2.0428, 60.7803, 0.0], 'loss': 2.293045781326294}\n",
            "OCS >> (average accuracy): 11.308421052631578\n",
            "OCS >> (Forgetting): 0.8187555555555557\n",
            "Maximum per-task accuracies: [93.06]\n",
            "\n",
            "---- Task 20 (OCS) ----\n",
            "Epoch 0.2 >> (per-task accuracy): 10.65\n",
            "Epoch 0.2 >> (class accuracy): [0.0, 0.0, 0.0, 0.099, 5.2953, 0.0, 94.572, 0.0, 10.883, 0.0]\n",
            "Epoch 0.4 >> (per-task accuracy): 10.91\n",
            "Epoch 0.4 >> (class accuracy): [0.0, 0.0, 0.0, 0.297, 5.1935, 0.0, 92.5887, 0.0973, 15.2977, 0.0]\n",
            "Epoch 0.6 >> (per-task accuracy): 11.39\n",
            "Epoch 0.6 >> (class accuracy): [0.0, 0.0, 0.0, 0.8911, 4.5825, 0.0, 90.501, 0.0973, 22.2793, 0.0]\n",
            "Epoch 0.8 >> (per-task accuracy): 11.58\n",
            "Epoch 0.8 >> (class accuracy): [0.0, 0.0, 0.0, 2.6733, 3.9715, 0.0, 86.1169, 0.0973, 27.3101, 0.0]\n",
            "Epoch 1.0 >> (per-task accuracy): 11.66\n",
            "Epoch 1.0 >> (class accuracy): [0.7143, 0.0, 0.0969, 5.1485, 3.5642, 0.0, 80.8977, 0.4864, 29.8768, 0.0]\n",
            "OCS >> Task 1: {'accuracy': 11.94, 'per_class_accuracy': [1.2245, 0.1762, 0.0, 8.9109, 7.943, 0.0, 69.6242, 1.8482, 33.4702, 0.0], 'loss': 2.2977724777221677}\n",
            "OCS >> Task 2: {'accuracy': 11.41, 'per_class_accuracy': [0.6122, 0.3524, 0.0, 11.5842, 9.4705, 0.0, 66.0752, 2.5292, 26.8994, 0.0], 'loss': 2.300433187866211}\n",
            "OCS >> Task 3: {'accuracy': 10.01, 'per_class_accuracy': [1.3265, 0.0, 0.0, 7.4257, 11.609, 0.0, 70.6681, 1.3619, 11.0883, 0.0], 'loss': 2.301649885559082}\n",
            "OCS >> Task 4: {'accuracy': 8.6, 'per_class_accuracy': [6.2245, 0.0, 0.0, 9.505, 9.8778, 0.0, 53.8622, 0.2918, 8.9322, 0.0], 'loss': 2.302630912780762}\n",
            "OCS >> Task 5: {'accuracy': 7.46, 'per_class_accuracy': [4.7959, 0.0, 0.0, 5.9406, 4.7862, 0.0, 52.5052, 0.3891, 8.7269, 0.0], 'loss': 2.3040396759033204}\n",
            "OCS >> Task 6: {'accuracy': 6.57, 'per_class_accuracy': [3.9796, 0.0, 0.0, 13.2673, 3.666, 0.0, 38.5177, 0.1946, 7.9055, 0.0], 'loss': 2.303044397735596}\n",
            "OCS >> Task 7: {'accuracy': 7.31, 'per_class_accuracy': [5.8163, 0.0, 0.0, 13.4653, 1.5275, 0.0, 41.9624, 0.1946, 12.2177, 0.0], 'loss': 2.303649545669556}\n",
            "OCS >> Task 8: {'accuracy': 8.83, 'per_class_accuracy': [4.1837, 0.0, 0.0, 13.1683, 4.277, 0.0, 44.3633, 0.0, 24.846, 0.0], 'loss': 2.302905810546875}\n",
            "OCS >> Task 9: {'accuracy': 9.57, 'per_class_accuracy': [3.9796, 0.0, 0.0, 8.1188, 6.11, 0.0, 52.4008, 0.0, 28.1314, 0.0], 'loss': 2.3011584213256837}\n",
            "OCS >> Task 10: {'accuracy': 12.17, 'per_class_accuracy': [2.8571, 0.0, 0.0, 9.802, 8.554, 0.0, 53.2359, 0.0, 50.924, 0.0], 'loss': 2.298735781478882}\n",
            "OCS >> Task 11: {'accuracy': 13.03, 'per_class_accuracy': [3.1633, 0.0, 0.0, 8.9109, 10.5906, 0.0, 48.7474, 0.8755, 61.807, 0.0], 'loss': 2.2976855239868166}\n",
            "OCS >> Task 12: {'accuracy': 12.73, 'per_class_accuracy': [2.2449, 0.0, 0.0, 5.0495, 10.6925, 0.0, 39.3528, 0.8755, 72.7926, 0.0], 'loss': 2.29686891784668}\n",
            "OCS >> Task 13: {'accuracy': 12.55, 'per_class_accuracy': [5.0, 0.0, 0.0, 3.9604, 7.332, 0.0, 38.9353, 0.6809, 73.306, 0.0], 'loss': 2.29663508644104}\n",
            "OCS >> Task 14: {'accuracy': 13.24, 'per_class_accuracy': [7.2449, 0.0, 0.0, 1.5842, 4.4807, 0.0, 44.8852, 1.751, 76.4887, 0.0], 'loss': 2.295702773284912}\n",
            "OCS >> Task 15: {'accuracy': 12.45, 'per_class_accuracy': [4.4898, 0.0, 0.0, 0.5941, 1.3238, 0.0, 42.7975, 3.0156, 76.078, 0.0], 'loss': 2.2969883712768553}\n",
            "OCS >> Task 16: {'accuracy': 12.34, 'per_class_accuracy': [7.3469, 0.0, 0.0, 1.0891, 0.7128, 0.0, 45.3027, 5.1556, 67.4538, 0.0], 'loss': 2.29660301322937}\n",
            "OCS >> Task 17: {'accuracy': 14.1, 'per_class_accuracy': [10.7143, 0.0, 0.0, 2.5743, 0.5092, 0.0, 61.1691, 11.6732, 58.3162, 0.0], 'loss': 2.295041796875}\n",
            "OCS >> Task 18: {'accuracy': 11.49, 'per_class_accuracy': [5.9184, 0.0, 0.0, 2.3762, 1.6293, 0.0, 65.5532, 3.7938, 39.4251, 0.0], 'loss': 2.2959618690490724}\n",
            "OCS >> Task 19: {'accuracy': 11.93, 'per_class_accuracy': [5.5102, 0.0, 0.0969, 2.5743, 1.833, 0.0, 80.4802, 1.6537, 31.4168, 0.0], 'loss': 2.2962780811309815}\n",
            "OCS >> Task 20: {'accuracy': 11.66, 'per_class_accuracy': [0.7143, 0.0, 0.0969, 5.1485, 3.5642, 0.0, 80.8977, 0.4864, 29.8768, 0.0], 'loss': 2.2953557704925536}\n",
            "OCS >> (average accuracy): 10.9695\n",
            "OCS >> (Forgetting): 0.8212684210526315\n",
            "Maximum per-task accuracies: [93.06]\n",
            "\n",
            "{'num_tasks': 20, 'per_task_rotation': 9, 'memory_size': 200, 'dataset': 'noisy-rot-mnist', 'device': 'cuda', 'momentum': 0.8, 'mlp_hiddens': 256, 'dropout': 0.2, 'lr_decay': 0.75, 'n_classes': 10, 'seq_lr': 0.005, 'stream_size': 100, 'ocspick': True, 'batch_size': 20, 'ref_hyp': 10.0, 'n_substeps': 5}\n"
          ]
        }
      ],
      "source": [
        "DATASET = 'noisy-rot-mnist'\n",
        "HIDDENS = 256\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "config = {\n",
        "    'num_tasks': 20,\n",
        "    'per_task_rotation': 9,\n",
        "    'memory_size': 200,\n",
        "    'dataset': DATASET,\n",
        "    'device': DEVICE,\n",
        "    'momentum': 0.8,\n",
        "    'mlp_hiddens': HIDDENS,\n",
        "    'dropout': 0.2,\n",
        "    'lr_decay': 0.75 if 'rot-mnist' in DATASET else 0.8,\n",
        "    'n_classes': 10,\n",
        "    'seq_lr': 0.005,\n",
        "    'stream_size': 100,\n",
        "    'ocspick': True,\n",
        "    'batch_size': 20,\n",
        "    # 'tau': 1000.0,\n",
        "    'ref_hyp': 10. if 'rot-mnist' in DATASET else 50\n",
        "}\n",
        "\n",
        "log_dir =  f\"./summery/{config['dataset']}\"\n",
        "summary = SummaryWriter(log_dir)\n",
        "\n",
        "experiment = Experiment(api_key=\"hidden_key\", project_name=\"mnist\", disabled=True)\n",
        "\n",
        "loaders = get_all_loaders(config)\n",
        "\n",
        "\n",
        "def evaluate_model(model, task, loaders, config):\n",
        "    accuracies, losses = [], []\n",
        "    for t in range(1, task + 1):\n",
        "        metrics = eval_single_epoch(model, loaders['sequential'][t]['val'], config)\n",
        "        accuracies.append(metrics['accuracy'])\n",
        "        losses.append(metrics['loss'])\n",
        "        print(f'OCS >> Task {t}: {metrics}')\n",
        "    return accuracies, losses\n",
        "\n",
        "def main():\n",
        "    setup_experiment(experiment, config)\n",
        "\n",
        "    max_accuracies = [0.0] * config['num_tasks']\n",
        "    for task in range(1, config['num_tasks'] + 1):\n",
        "        print(f'---- Task {task} (OCS) ----')\n",
        "        model = train_task_sequentially(task, loaders, config, summary)\n",
        "\n",
        "        accuracies, _ = evaluate_model(model, task, loaders, config)\n",
        "        max_accuracies = [max(acc, max_acc) for acc, max_acc in zip(accuracies, max_accuracies)]\n",
        "\n",
        "        avg_accuracy = np.mean(accuracies)\n",
        "        if task > 1:\n",
        "            forgetting = np.mean(np.array(max_accuracies[:task - 1]) - np.array(accuracies[:task - 1]))/ 100\n",
        "        else:\n",
        "            forgetting = 0.0\n",
        "\n",
        "        print(f\"OCS >> (average accuracy): {avg_accuracy}\")\n",
        "        print(f\"OCS >> (Forgetting): {forgetting}\")\n",
        "        summary.add_scalar('cl_average_accuracy', avg_accuracy, task - 1)\n",
        "        print(f'Maximum per-task accuracies: {max_accuracies}\\n')\n",
        "\n",
        "    print(config)\n",
        "    experiment.end()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajkgIh4AJdqa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3736ac5-9206-4d47-c473-2c734e64f0f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:02<00:00, 80650888.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data/\n",
            "Files already downloaded and verified\n",
            "loading coreset placeholder noisy-rot-mnist\n",
            "loading noisy-rot-mnist for task 1\n",
            "Noisy Rotated MNIST\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 46908716.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 1859563.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 13933076.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 5392167.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading noisy-rot-mnist for task 2\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 3\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 4\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 5\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 6\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 7\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 8\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 9\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 10\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 11\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 12\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 13\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 14\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 15\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 16\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 17\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 18\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 19\n",
            "Noisy Rotated MNIST\n",
            "loading noisy-rot-mnist for task 20\n",
            "Noisy Rotated MNIST\n",
            "---- Task 1 (OCS) ----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0.05 >> (per-task accuracy): 49.45\n",
            "Epoch 0.05 >> (class accuracy): [96.0204, 89.6916, 91.0853, 8.1188, 91.0387, 0.0, 13.9875, 91.0506, 0.0, 0.0]\n",
            "Epoch 0.1 >> (per-task accuracy): 86.18\n",
            "Epoch 0.1 >> (class accuracy): [97.2449, 98.326, 74.2248, 90.495, 90.4277, 69.5067, 91.023, 83.5603, 79.4661, 84.8365]\n",
            "Epoch 0.15 >> (per-task accuracy): 88.18\n",
            "Epoch 0.15 >> (class accuracy): [95.7143, 98.6784, 75.0969, 84.6535, 96.5377, 75.3363, 94.0501, 86.7704, 88.6037, 84.6383]\n",
            "Epoch 0.2 >> (per-task accuracy): 91.03\n",
            "Epoch 0.2 >> (class accuracy): [97.2449, 98.7665, 81.2016, 89.703, 95.6212, 81.6143, 92.5887, 86.965, 93.2238, 92.0714]\n",
            "Epoch 0.25 >> (per-task accuracy): 93.14\n",
            "Epoch 0.25 >> (class accuracy): [97.8571, 98.6784, 88.8566, 91.1881, 97.3523, 92.0404, 94.0501, 91.2451, 89.9384, 89.6928]\n",
            "Epoch 0.3 >> (per-task accuracy): 93.7\n",
            "Epoch 0.3 >> (class accuracy): [98.8776, 98.7665, 91.376, 89.4059, 97.4542, 89.2377, 94.3633, 91.9261, 95.8932, 88.999]\n",
            "Epoch 0.35 >> (per-task accuracy): 94.17\n",
            "Epoch 0.35 >> (class accuracy): [98.2653, 98.6784, 89.6318, 90.297, 94.3992, 94.7309, 94.9896, 91.1479, 95.0719, 94.3508]\n",
            "Epoch 0.4 >> (per-task accuracy): 95.16\n",
            "Epoch 0.4 >> (class accuracy): [98.7755, 98.9427, 93.5078, 95.2475, 97.1487, 91.2556, 94.3633, 93.7743, 96.7146, 91.1794]\n",
            "Epoch 0.45 >> (per-task accuracy): 95.41\n",
            "Epoch 0.45 >> (class accuracy): [98.0612, 99.0308, 94.4767, 94.0594, 96.8432, 93.6099, 96.9729, 92.9961, 95.6879, 91.9722]\n",
            "Epoch 0.5 >> (per-task accuracy): 95.48\n",
            "Epoch 0.5 >> (class accuracy): [98.8776, 99.0308, 93.5078, 94.3564, 96.8432, 93.722, 95.7203, 94.4553, 93.5318, 94.2517]\n",
            "Epoch 0.55 >> (per-task accuracy): 96.08\n",
            "Epoch 0.55 >> (class accuracy): [98.4694, 98.9427, 95.5426, 94.6535, 97.1487, 96.5247, 96.3466, 95.7198, 94.7639, 92.4678]\n",
            "Epoch 0.6 >> (per-task accuracy): 96.11\n",
            "Epoch 0.6 >> (class accuracy): [98.2653, 99.1189, 94.8643, 94.9505, 96.8432, 97.5336, 96.1378, 93.9689, 95.3799, 93.9544]\n",
            "Epoch 0.65 >> (per-task accuracy): 96.11\n",
            "Epoch 0.65 >> (class accuracy): [98.6735, 99.0308, 95.2519, 96.9307, 96.7413, 95.9641, 96.7641, 92.2179, 93.4292, 95.8375]\n",
            "Epoch 0.7 >> (per-task accuracy): 96.36\n",
            "Epoch 0.7 >> (class accuracy): [98.7755, 99.0308, 96.9961, 94.0594, 96.4358, 96.5247, 97.4948, 95.0389, 95.1745, 93.8553]\n",
            "Epoch 0.75 >> (per-task accuracy): 96.58\n",
            "Epoch 0.75 >> (class accuracy): [97.9592, 99.2952, 96.8023, 96.6337, 97.4542, 97.1973, 95.5115, 95.1362, 95.3799, 94.1526]\n",
            "Epoch 0.8 >> (per-task accuracy): 96.53\n",
            "Epoch 0.8 >> (class accuracy): [97.8571, 98.5903, 97.1899, 95.6436, 96.8432, 95.2915, 94.9896, 95.7198, 97.2279, 95.5401]\n",
            "Epoch 0.85 >> (per-task accuracy): 96.67\n",
            "Epoch 0.85 >> (class accuracy): [99.1837, 99.0308, 96.4147, 95.0495, 97.1487, 97.9821, 96.1378, 95.8171, 95.7906, 94.0535]\n",
            "Epoch 0.9 >> (per-task accuracy): 96.39\n",
            "Epoch 0.9 >> (class accuracy): [99.1837, 98.9427, 96.7054, 94.1584, 95.2138, 97.5336, 96.9729, 95.5253, 95.3799, 94.1526]\n",
            "Epoch 0.95 >> (per-task accuracy): 96.88\n",
            "Epoch 0.95 >> (class accuracy): [99.2857, 99.0308, 96.3178, 95.5446, 96.8432, 96.0762, 96.9729, 96.5953, 96.6119, 95.2428]\n",
            "Epoch 1.0 >> (per-task accuracy): 96.34\n",
            "Epoch 1.0 >> (class accuracy): [99.4898, 99.0308, 96.2209, 93.8614, 94.2974, 97.0852, 96.4509, 97.0817, 94.3532, 95.2428]\n",
            "OCS >> Task 1: {'accuracy': 96.34, 'per_class_accuracy': [99.4898, 99.0308, 96.2209, 93.8614, 94.2974, 97.0852, 96.4509, 97.0817, 94.3532, 95.2428], 'loss': 0.1393142095386982}\n",
            "OCS >> (average accuracy): 96.34\n",
            "OCS >> (Forgetting): 0.0\n",
            "Maximum per-task accuracies: [96.34]\n",
            "\n",
            "---- Task 2 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 69.42\n",
            "Epoch 0.05 >> (class accuracy): [90.0, 95.7709, 63.0814, 67.6238, 66.7006, 42.3767, 60.334, 82.1012, 73.2033, 46.6799]\n",
            "Epoch 0.1 >> (per-task accuracy): 88.21\n",
            "Epoch 0.1 >> (class accuracy): [93.4694, 97.7093, 84.4961, 92.8713, 92.9735, 74.7758, 93.5282, 91.9261, 76.2834, 81.4668]\n",
            "Epoch 0.15 >> (per-task accuracy): 91.49\n",
            "Epoch 0.15 >> (class accuracy): [97.8571, 98.326, 86.7248, 90.297, 96.1303, 84.7534, 95.5115, 93.677, 84.7023, 85.6293]\n",
            "Epoch 0.2 >> (per-task accuracy): 93.43\n",
            "Epoch 0.2 >> (class accuracy): [98.0612, 97.8855, 89.0504, 96.0396, 95.6212, 89.9103, 93.5282, 92.7043, 89.4251, 91.2785]\n",
            "Epoch 0.25 >> (per-task accuracy): 94.66\n",
            "Epoch 0.25 >> (class accuracy): [98.3673, 97.7093, 93.9922, 96.1386, 97.2505, 91.3677, 96.0334, 93.1907, 90.4517, 91.4767]\n",
            "Epoch 0.3 >> (per-task accuracy): 94.79\n",
            "Epoch 0.3 >> (class accuracy): [98.1633, 98.326, 93.4109, 88.7129, 97.0468, 96.7489, 96.4509, 95.8171, 92.1971, 90.9812]\n",
            "Epoch 0.35 >> (per-task accuracy): 95.78\n",
            "Epoch 0.35 >> (class accuracy): [98.7755, 98.4141, 93.6047, 94.2574, 96.5377, 96.9731, 96.1378, 94.9416, 94.2505, 93.8553]\n",
            "Epoch 0.4 >> (per-task accuracy): 95.71\n",
            "Epoch 0.4 >> (class accuracy): [98.8776, 98.7665, 94.9612, 95.1485, 97.7597, 95.9641, 96.3466, 95.428, 92.7105, 90.8821]\n",
            "Epoch 0.45 >> (per-task accuracy): 95.88\n",
            "Epoch 0.45 >> (class accuracy): [99.0816, 98.6784, 94.5736, 96.0396, 95.6212, 93.4978, 95.5115, 95.0389, 95.3799, 94.8464]\n",
            "Epoch 0.5 >> (per-task accuracy): 96.28\n",
            "Epoch 0.5 >> (class accuracy): [99.0816, 98.8546, 95.5426, 94.7525, 97.2505, 94.1704, 97.286, 95.9144, 95.9959, 93.558]\n",
            "Epoch 0.55 >> (per-task accuracy): 95.77\n",
            "Epoch 0.55 >> (class accuracy): [99.5918, 98.9427, 94.4767, 96.8317, 96.334, 93.1614, 97.5992, 93.7743, 91.9918, 94.45]\n",
            "Epoch 0.6 >> (per-task accuracy): 96.15\n",
            "Epoch 0.6 >> (class accuracy): [98.8776, 98.7665, 93.7984, 97.1287, 95.0102, 95.1794, 96.9729, 95.1362, 94.2505, 96.0357]\n",
            "Epoch 0.65 >> (per-task accuracy): 96.0\n",
            "Epoch 0.65 >> (class accuracy): [99.1837, 99.0308, 96.124, 95.4455, 95.6212, 95.5157, 98.1211, 94.8444, 89.6304, 96.0357]\n",
            "Epoch 0.7 >> (per-task accuracy): 96.06\n",
            "Epoch 0.7 >> (class accuracy): [99.1837, 98.5903, 95.2519, 94.0594, 95.8248, 96.3004, 97.7035, 96.2062, 93.6345, 93.6571]\n",
            "Epoch 0.75 >> (per-task accuracy): 96.3\n",
            "Epoch 0.75 >> (class accuracy): [97.9592, 98.8546, 97.093, 94.0594, 93.279, 96.9731, 98.3299, 95.7198, 93.4292, 97.0268]\n",
            "Epoch 0.8 >> (per-task accuracy): 96.56\n",
            "Epoch 0.8 >> (class accuracy): [99.0816, 99.0308, 94.0891, 93.6634, 98.0652, 96.861, 97.4948, 97.5681, 95.1745, 94.45]\n",
            "Epoch 0.85 >> (per-task accuracy): 96.27\n",
            "Epoch 0.85 >> (class accuracy): [99.0816, 99.1189, 94.5736, 93.0693, 96.6395, 96.5247, 98.2255, 96.1089, 95.1745, 94.0535]\n",
            "Epoch 0.9 >> (per-task accuracy): 96.45\n",
            "Epoch 0.9 >> (class accuracy): [99.4898, 98.6784, 95.9302, 97.0297, 95.723, 95.4036, 96.3466, 97.0817, 92.8131, 95.5401]\n",
            "Epoch 0.95 >> (per-task accuracy): 96.51\n",
            "Epoch 0.95 >> (class accuracy): [98.5714, 98.4141, 97.1899, 97.3267, 95.6212, 93.722, 98.1211, 96.2062, 93.3265, 96.0357]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-b440c4a92a02>:12: RuntimeWarning: divide by zero encountered in scalar floor_divide\n",
            "  if (num_residuals // num_class) > 0:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1.0 >> (per-task accuracy): 96.07\n",
            "Epoch 1.0 >> (class accuracy): [98.6735, 98.7665, 96.8992, 96.7327, 90.9369, 94.6188, 97.3904, 98.3463, 92.4025, 95.2428]\n",
            "OCS >> Task 1: {'accuracy': 94.07, 'per_class_accuracy': [97.6531, 96.2115, 94.0891, 95.9406, 91.6497, 94.1704, 94.6764, 96.4008, 88.6037, 90.8821], 'loss': 0.19551556173861026}\n",
            "OCS >> Task 2: {'accuracy': 96.07, 'per_class_accuracy': [98.6735, 98.7665, 96.8992, 96.7327, 90.9369, 94.6188, 97.3904, 98.3463, 92.4025, 95.2428], 'loss': 0.13875203743129969}\n",
            "OCS >> (average accuracy): 95.07\n",
            "OCS >> (Forgetting): 0.022700000000000102\n",
            "Maximum per-task accuracies: [96.34]\n",
            "\n",
            "---- Task 3 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 59.06\n",
            "Epoch 0.05 >> (class accuracy): [75.3061, 97.8855, 37.4031, 14.0594, 55.1935, 73.991, 49.2693, 69.0661, 40.5544, 74.331]\n",
            "Epoch 0.1 >> (per-task accuracy): 82.5\n",
            "Epoch 0.1 >> (class accuracy): [94.5918, 98.326, 84.7868, 60.6931, 93.3809, 82.7354, 89.8747, 82.7821, 65.6057, 70.664]\n",
            "Epoch 0.15 >> (per-task accuracy): 88.63\n",
            "Epoch 0.15 >> (class accuracy): [95.3061, 97.7093, 88.8566, 78.3168, 90.6314, 89.574, 93.1106, 89.4942, 76.2834, 86.0258]\n",
            "Epoch 0.2 >> (per-task accuracy): 90.85\n",
            "Epoch 0.2 >> (class accuracy): [97.1429, 98.326, 86.1434, 84.0594, 90.224, 92.4888, 94.2589, 88.8132, 85.4209, 91.1794]\n",
            "Epoch 0.25 >> (per-task accuracy): 92.33\n",
            "Epoch 0.25 >> (class accuracy): [96.6327, 98.8546, 90.9884, 86.5347, 92.3625, 94.2825, 94.8852, 89.8833, 87.577, 90.8821]\n",
            "Epoch 0.3 >> (per-task accuracy): 94.14\n",
            "Epoch 0.3 >> (class accuracy): [97.9592, 98.9427, 93.2171, 90.8911, 94.9084, 92.713, 96.3466, 91.7315, 93.0185, 91.1794]\n",
            "Epoch 0.35 >> (per-task accuracy): 94.48\n",
            "Epoch 0.35 >> (class accuracy): [98.4694, 99.1189, 92.8295, 94.6535, 95.4175, 91.5919, 95.3027, 92.3152, 91.0678, 93.2607]\n",
            "Epoch 0.4 >> (per-task accuracy): 94.42\n",
            "Epoch 0.4 >> (class accuracy): [97.8571, 98.6784, 92.7326, 94.4554, 91.9552, 91.704, 95.6159, 92.2179, 92.6078, 95.6392]\n",
            "Epoch 0.45 >> (per-task accuracy): 95.87\n",
            "Epoch 0.45 >> (class accuracy): [98.1633, 98.7665, 95.7364, 95.8416, 96.6395, 93.722, 96.1378, 94.4553, 94.4559, 94.2517]\n",
            "Epoch 0.5 >> (per-task accuracy): 95.36\n",
            "Epoch 0.5 >> (class accuracy): [97.7551, 98.9427, 94.0891, 94.9505, 96.1303, 94.5067, 95.8246, 92.9961, 92.8131, 95.1437]\n",
            "Epoch 0.55 >> (per-task accuracy): 95.78\n",
            "Epoch 0.55 >> (class accuracy): [98.3673, 98.7665, 95.2519, 97.1287, 96.8432, 91.704, 95.3027, 94.6498, 94.5585, 94.45]\n",
            "Epoch 0.6 >> (per-task accuracy): 95.89\n",
            "Epoch 0.6 >> (class accuracy): [98.3673, 99.207, 95.2519, 96.7327, 96.8432, 90.9193, 96.6597, 94.4553, 93.9425, 95.6392]\n",
            "Epoch 0.65 >> (per-task accuracy): 95.8\n",
            "Epoch 0.65 >> (class accuracy): [98.3673, 98.8546, 95.7364, 93.9604, 97.0468, 97.87, 94.572, 96.1089, 92.0945, 93.1615]\n",
            "Epoch 0.7 >> (per-task accuracy): 95.5\n",
            "Epoch 0.7 >> (class accuracy): [97.8571, 99.207, 93.8953, 97.7228, 97.556, 95.6278, 96.0334, 93.5798, 88.809, 94.2517]\n",
            "Epoch 0.75 >> (per-task accuracy): 95.72\n",
            "Epoch 0.75 >> (class accuracy): [99.0816, 98.9427, 93.314, 98.1188, 96.4358, 92.3767, 94.9896, 95.2335, 93.0185, 94.9455]\n",
            "Epoch 0.8 >> (per-task accuracy): 96.0\n",
            "Epoch 0.8 >> (class accuracy): [97.7551, 98.7665, 93.5078, 97.9208, 97.3523, 95.2915, 96.1378, 94.9416, 92.8131, 95.1437]\n",
            "Epoch 0.85 >> (per-task accuracy): 96.08\n",
            "Epoch 0.85 >> (class accuracy): [98.2653, 98.7665, 95.6395, 96.8317, 95.6212, 95.9641, 94.8852, 94.7471, 93.7372, 95.9366]\n",
            "Epoch 0.9 >> (per-task accuracy): 96.05\n",
            "Epoch 0.9 >> (class accuracy): [98.0612, 99.0308, 96.2209, 95.7426, 94.6029, 96.6368, 96.6597, 95.5253, 91.2731, 96.333]\n",
            "Epoch 0.95 >> (per-task accuracy): 95.8\n",
            "Epoch 0.95 >> (class accuracy): [98.0612, 98.5903, 93.314, 97.3267, 95.112, 96.7489, 95.3027, 94.9416, 92.4025, 95.9366]\n",
            "Epoch 1.0 >> (per-task accuracy): 95.85\n",
            "Epoch 1.0 >> (class accuracy): [97.7551, 98.5022, 93.1202, 97.8218, 96.7413, 95.4036, 96.8685, 94.4553, 92.1971, 95.3419]\n",
            "OCS >> Task 1: {'accuracy': 87.01, 'per_class_accuracy': [96.5306, 90.9251, 78.7791, 90.099, 87.0672, 92.2646, 87.8914, 76.6537, 83.6756, 86.9177], 'loss': 0.4060077402710915}\n",
            "OCS >> Task 2: {'accuracy': 92.18, 'per_class_accuracy': [96.2245, 96.8282, 88.0814, 92.8713, 92.8717, 96.3004, 93.215, 86.6732, 89.2197, 89.6928], 'loss': 0.25967933770418167}\n",
            "OCS >> Task 3: {'accuracy': 95.85, 'per_class_accuracy': [97.7551, 98.5022, 93.1202, 97.8218, 96.7413, 95.4036, 96.8685, 94.4553, 92.1971, 95.3419], 'loss': 0.16783739119768143}\n",
            "OCS >> (average accuracy): 91.67999999999999\n",
            "OCS >> (Forgetting): 0.06744999999999997\n",
            "Maximum per-task accuracies: [96.34]\n",
            "\n",
            "---- Task 4 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 49.38\n",
            "Epoch 0.05 >> (class accuracy): [73.1633, 60.1762, 59.7868, 21.4851, 56.9246, 47.6457, 7.7244, 80.642, 27.0021, 54.9058]\n",
            "Epoch 0.1 >> (per-task accuracy): 82.49\n",
            "Epoch 0.1 >> (class accuracy): [96.0204, 96.5639, 76.2597, 88.9109, 68.6354, 60.7623, 87.1608, 84.8249, 73.614, 87.9088]\n",
            "Epoch 0.15 >> (per-task accuracy): 87.06\n",
            "Epoch 0.15 >> (class accuracy): [96.2245, 96.9163, 74.1279, 90.0, 88.6965, 78.0269, 86.7432, 86.4786, 83.6756, 87.9088]\n",
            "Epoch 0.2 >> (per-task accuracy): 89.2\n",
            "Epoch 0.2 >> (class accuracy): [94.4898, 97.533, 75.3876, 92.6733, 92.9735, 80.3812, 92.6931, 89.4942, 87.9877, 87.0168]\n",
            "Epoch 0.25 >> (per-task accuracy): 90.62\n",
            "Epoch 0.25 >> (class accuracy): [95.8163, 97.7974, 76.4535, 89.703, 94.8065, 88.9013, 94.1545, 90.2724, 90.8624, 87.116]\n",
            "Epoch 0.3 >> (per-task accuracy): 92.33\n",
            "Epoch 0.3 >> (class accuracy): [97.0408, 97.7974, 82.8488, 92.5743, 95.6212, 91.0314, 94.3633, 91.537, 92.1971, 88.0079]\n",
            "Epoch 0.35 >> (per-task accuracy): 92.63\n",
            "Epoch 0.35 >> (class accuracy): [97.551, 98.6784, 85.8527, 86.7327, 93.1772, 95.0673, 94.4676, 91.6342, 90.3491, 92.666]\n",
            "Epoch 0.4 >> (per-task accuracy): 93.86\n",
            "Epoch 0.4 >> (class accuracy): [98.1633, 98.1498, 87.3062, 92.3762, 93.9919, 92.2646, 95.5115, 92.4125, 95.2772, 92.8642]\n",
            "Epoch 0.45 >> (per-task accuracy): 94.32\n",
            "Epoch 0.45 >> (class accuracy): [97.8571, 98.8546, 92.0543, 90.396, 95.8248, 95.6278, 94.7808, 91.1479, 93.6345, 92.8642]\n",
            "Epoch 0.5 >> (per-task accuracy): 94.18\n",
            "Epoch 0.5 >> (class accuracy): [96.7347, 98.8546, 89.7287, 90.9901, 95.723, 95.852, 95.6159, 92.3152, 92.8131, 93.0624]\n",
            "Epoch 0.55 >> (per-task accuracy): 94.96\n",
            "Epoch 0.55 >> (class accuracy): [98.5714, 98.8546, 94.0891, 92.1782, 95.9267, 94.6188, 95.1983, 92.2179, 92.9158, 94.6482]\n",
            "Epoch 0.6 >> (per-task accuracy): 94.99\n",
            "Epoch 0.6 >> (class accuracy): [97.9592, 98.6784, 94.5736, 92.4752, 96.334, 96.4126, 95.7203, 93.0934, 90.6571, 93.7562]\n",
            "Epoch 0.65 >> (per-task accuracy): 95.24\n",
            "Epoch 0.65 >> (class accuracy): [98.3673, 99.0308, 93.314, 92.8713, 95.8248, 95.0673, 96.8685, 94.358, 91.8891, 94.45]\n",
            "Epoch 0.7 >> (per-task accuracy): 95.29\n",
            "Epoch 0.7 >> (class accuracy): [97.8571, 98.7665, 93.9922, 93.4653, 95.8248, 96.5247, 96.4509, 93.0934, 91.9918, 94.7473]\n",
            "Epoch 0.75 >> (per-task accuracy): 95.57\n",
            "Epoch 0.75 >> (class accuracy): [98.4694, 99.0308, 94.5736, 93.0693, 96.334, 96.9731, 95.5115, 93.7743, 92.6078, 95.1437]\n",
            "Epoch 0.8 >> (per-task accuracy): 95.36\n",
            "Epoch 0.8 >> (class accuracy): [98.5714, 99.1189, 94.186, 91.7822, 96.334, 96.5247, 96.4509, 93.0934, 92.0945, 95.2428]\n",
            "Epoch 0.85 >> (per-task accuracy): 95.23\n",
            "Epoch 0.85 >> (class accuracy): [97.7551, 99.1189, 94.2829, 94.0594, 94.7047, 96.861, 96.3466, 94.5525, 88.193, 96.0357]\n",
            "Epoch 0.9 >> (per-task accuracy): 95.64\n",
            "Epoch 0.9 >> (class accuracy): [98.2653, 99.207, 94.4767, 93.1683, 94.6029, 97.0852, 95.5115, 94.8444, 94.2505, 94.7473]\n",
            "Epoch 0.95 >> (per-task accuracy): 95.61\n",
            "Epoch 0.95 >> (class accuracy): [98.3673, 98.8546, 94.9612, 94.7525, 92.3625, 96.861, 96.8685, 94.8444, 91.8891, 96.0357]\n",
            "Epoch 1.0 >> (per-task accuracy): 95.78\n",
            "Epoch 1.0 >> (class accuracy): [98.5714, 99.207, 94.9612, 94.4554, 95.5193, 96.5247, 96.6597, 92.8016, 94.2505, 94.6482]\n",
            "OCS >> Task 1: {'accuracy': 72.66, 'per_class_accuracy': [93.0612, 90.3965, 59.9806, 67.7228, 77.6986, 82.0628, 67.0146, 68.0934, 69.0965, 51.0406], 'loss': 0.8058726300239563}\n",
            "OCS >> Task 2: {'accuracy': 84.7, 'per_class_accuracy': [96.1224, 97.2687, 80.1357, 78.0198, 90.1222, 91.4798, 80.5846, 83.2685, 86.7556, 62.9336], 'loss': 0.46897120823860167}\n",
            "OCS >> Task 3: {'accuracy': 92.7, 'per_class_accuracy': [96.8367, 99.4714, 90.2132, 93.2673, 95.0102, 93.1614, 92.4843, 91.3424, 89.2197, 85.332], 'loss': 0.269432285618782}\n",
            "OCS >> Task 4: {'accuracy': 95.78, 'per_class_accuracy': [98.5714, 99.207, 94.9612, 94.4554, 95.5193, 96.5247, 96.6597, 92.8016, 94.2505, 94.6482], 'loss': 0.1929510549545288}\n",
            "OCS >> (average accuracy): 86.46000000000001\n",
            "OCS >> (Forgetting): 0.12986666666666669\n",
            "Maximum per-task accuracies: [96.34]\n",
            "\n",
            "---- Task 5 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 43.68\n",
            "Epoch 0.05 >> (class accuracy): [71.9388, 43.7885, 86.6279, 64.2574, 87.5764, 7.6233, 10.2296, 53.4047, 0.308, 4.4599]\n",
            "Epoch 0.1 >> (per-task accuracy): 79.87\n",
            "Epoch 0.1 >> (class accuracy): [94.3878, 92.7753, 83.4302, 86.8317, 56.11, 63.9013, 89.1441, 75.8755, 66.4271, 86.1249]\n",
            "Epoch 0.15 >> (per-task accuracy): 84.32\n",
            "Epoch 0.15 >> (class accuracy): [96.3265, 94.0969, 87.7907, 75.7426, 84.0122, 82.8475, 89.8747, 86.7704, 63.7577, 80.3766]\n",
            "Epoch 0.2 >> (per-task accuracy): 87.53\n",
            "Epoch 0.2 >> (class accuracy): [95.8163, 95.7709, 84.593, 80.6931, 88.7984, 83.4081, 91.858, 87.7432, 82.1355, 83.3499]\n",
            "Epoch 0.25 >> (per-task accuracy): 89.37\n",
            "Epoch 0.25 >> (class accuracy): [96.3265, 97.7093, 88.1783, 80.8911, 90.6314, 88.565, 91.5449, 88.1323, 83.5729, 87.2151]\n",
            "Epoch 0.3 >> (per-task accuracy): 90.34\n",
            "Epoch 0.3 >> (class accuracy): [96.7347, 97.2687, 84.593, 82.7723, 90.835, 89.0135, 92.9019, 90.1751, 89.5277, 88.999]\n",
            "Epoch 0.35 >> (per-task accuracy): 91.08\n",
            "Epoch 0.35 >> (class accuracy): [95.3061, 97.7974, 87.7907, 82.6733, 91.0387, 89.7982, 95.4071, 87.5486, 91.0678, 91.8731]\n",
            "Epoch 0.4 >> (per-task accuracy): 91.69\n",
            "Epoch 0.4 >> (class accuracy): [95.9184, 97.7974, 88.1783, 87.0297, 92.7699, 91.2556, 93.9457, 86.6732, 90.5544, 92.3687]\n",
            "Epoch 0.45 >> (per-task accuracy): 92.77\n",
            "Epoch 0.45 >> (class accuracy): [96.7347, 97.8855, 89.9225, 89.802, 94.7047, 90.4709, 95.1983, 89.2023, 91.0678, 92.1705]\n",
            "Epoch 0.5 >> (per-task accuracy): 93.28\n",
            "Epoch 0.5 >> (class accuracy): [97.0408, 97.7974, 90.2132, 89.0099, 95.2138, 93.4978, 94.9896, 92.3152, 90.8624, 91.5758]\n",
            "Epoch 0.55 >> (per-task accuracy): 93.83\n",
            "Epoch 0.55 >> (class accuracy): [97.8571, 98.326, 90.1163, 90.9901, 95.4175, 92.3767, 94.572, 92.607, 92.7105, 92.8642]\n",
            "Epoch 0.6 >> (per-task accuracy): 94.28\n",
            "Epoch 0.6 >> (class accuracy): [97.1429, 98.8546, 93.9922, 92.9703, 95.6212, 93.722, 95.0939, 89.9805, 91.7864, 93.1615]\n",
            "Epoch 0.65 >> (per-task accuracy): 94.15\n",
            "Epoch 0.65 >> (class accuracy): [97.6531, 98.7665, 93.314, 92.3762, 96.2322, 95.0673, 94.6764, 89.786, 90.0411, 93.2607]\n",
            "Epoch 0.7 >> (per-task accuracy): 94.83\n",
            "Epoch 0.7 >> (class accuracy): [97.8571, 98.5022, 93.2171, 92.7723, 95.2138, 95.2915, 94.8852, 92.9961, 93.8398, 93.4589]\n",
            "Epoch 0.75 >> (per-task accuracy): 94.98\n",
            "Epoch 0.75 >> (class accuracy): [97.7551, 98.6784, 95.0581, 94.0594, 95.2138, 94.9552, 96.0334, 90.3696, 92.9158, 94.45]\n",
            "Epoch 0.8 >> (per-task accuracy): 94.51\n",
            "Epoch 0.8 >> (class accuracy): [97.9592, 98.5022, 93.9922, 91.2871, 96.0285, 96.6368, 94.7808, 90.2724, 91.4784, 94.0535]\n",
            "Epoch 0.85 >> (per-task accuracy): 95.16\n",
            "Epoch 0.85 >> (class accuracy): [97.6531, 98.5022, 95.5426, 93.0693, 96.0285, 96.3004, 96.4509, 92.8016, 91.1704, 93.8553]\n",
            "Epoch 0.9 >> (per-task accuracy): 94.96\n",
            "Epoch 0.9 >> (class accuracy): [97.7551, 98.6784, 94.2829, 93.8614, 94.7047, 96.7489, 95.3027, 92.8988, 90.2464, 94.8464]\n",
            "Epoch 0.95 >> (per-task accuracy): 95.0\n",
            "Epoch 0.95 >> (class accuracy): [97.2449, 98.7665, 95.2519, 95.5446, 97.0468, 94.6188, 95.6159, 91.3424, 89.7331, 94.3508]\n",
            "Epoch 1.0 >> (per-task accuracy): 95.04\n",
            "Epoch 1.0 >> (class accuracy): [97.1429, 98.5903, 93.4109, 94.7525, 96.2322, 95.6278, 96.1378, 92.9961, 91.9918, 93.2607]\n",
            "OCS >> Task 1: {'accuracy': 58.91, 'per_class_accuracy': [89.898, 56.8282, 40.2132, 67.5248, 76.6802, 70.0673, 38.9353, 60.9922, 51.6427, 38.3548], 'loss': 1.3367604941368103}\n",
            "OCS >> Task 2: {'accuracy': 72.61, 'per_class_accuracy': [92.1429, 76.652, 65.407, 76.9307, 84.5214, 83.6323, 58.977, 73.249, 67.4538, 48.0674], 'loss': 0.8374990253448487}\n",
            "OCS >> Task 3: {'accuracy': 84.45, 'per_class_accuracy': [95.5102, 85.6388, 82.1705, 91.5842, 90.7332, 89.0135, 77.9749, 85.7977, 80.8008, 65.7086], 'loss': 0.5000010316371918}\n",
            "OCS >> Task 4: {'accuracy': 91.79, 'per_class_accuracy': [96.6327, 96.8282, 90.8915, 92.3762, 94.6029, 94.5067, 93.0063, 92.2179, 85.0103, 81.5659], 'loss': 0.30951089155673983}\n",
            "OCS >> Task 5: {'accuracy': 95.04, 'per_class_accuracy': [97.1429, 98.5903, 93.4109, 94.7525, 96.2322, 95.6278, 96.1378, 92.9961, 91.9918, 93.2607], 'loss': 0.2268822045624256}\n",
            "OCS >> (average accuracy): 80.56\n",
            "OCS >> (Forgetting): 0.19400000000000003\n",
            "Maximum per-task accuracies: [96.34]\n",
            "\n",
            "---- Task 6 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 34.11\n",
            "Epoch 0.05 >> (class accuracy): [45.9184, 32.2467, 48.6434, 63.3663, 96.5377, 0.6726, 0.2088, 46.5953, 0.8214, 0.9911]\n",
            "Epoch 0.1 >> (per-task accuracy): 71.17\n",
            "Epoch 0.1 >> (class accuracy): [92.6531, 94.978, 70.8333, 77.5248, 76.5784, 20.1794, 86.2213, 92.1206, 55.8522, 36.4718]\n",
            "Epoch 0.15 >> (per-task accuracy): 82.02\n",
            "Epoch 0.15 >> (class accuracy): [96.7347, 95.6828, 79.2636, 79.703, 82.4847, 58.9686, 89.2484, 87.0623, 79.1581, 68.1863]\n",
            "Epoch 0.2 >> (per-task accuracy): 84.43\n",
            "Epoch 0.2 >> (class accuracy): [97.449, 96.5639, 80.3295, 85.1485, 84.4196, 61.2108, 88.4134, 84.8249, 87.4743, 75.0248]\n",
            "Epoch 0.25 >> (per-task accuracy): 86.61\n",
            "Epoch 0.25 >> (class accuracy): [97.449, 96.9163, 82.0736, 83.1683, 85.4379, 76.4574, 91.6493, 86.8677, 85.9343, 78.2953]\n",
            "Epoch 0.3 >> (per-task accuracy): 88.07\n",
            "Epoch 0.3 >> (class accuracy): [97.0408, 97.0925, 82.3643, 85.1485, 90.1222, 81.3901, 91.6493, 86.284, 89.7331, 78.6918]\n",
            "Epoch 0.35 >> (per-task accuracy): 89.51\n",
            "Epoch 0.35 >> (class accuracy): [96.8367, 97.1806, 84.8837, 86.2376, 91.1405, 85.2018, 91.9624, 88.1323, 90.2464, 82.3588]\n",
            "Epoch 0.4 >> (per-task accuracy): 90.34\n",
            "Epoch 0.4 >> (class accuracy): [97.2449, 97.533, 84.1085, 88.3168, 92.5662, 86.3229, 92.6931, 88.3268, 92.6078, 82.9534]\n",
            "Epoch 0.45 >> (per-task accuracy): 91.43\n",
            "Epoch 0.45 >> (class accuracy): [97.449, 97.8855, 85.1744, 90.198, 92.668, 86.435, 92.9019, 90.0778, 93.9425, 86.7195]\n",
            "Epoch 0.5 >> (per-task accuracy): 91.76\n",
            "Epoch 0.5 >> (class accuracy): [97.8571, 97.9736, 87.0155, 91.2871, 94.9084, 89.4619, 93.215, 90.856, 91.8891, 82.557]\n",
            "Epoch 0.55 >> (per-task accuracy): 92.52\n",
            "Epoch 0.55 >> (class accuracy): [97.551, 98.1498, 88.5659, 88.6139, 93.7882, 92.6009, 94.4676, 91.9261, 92.9158, 86.3231]\n",
            "Epoch 0.6 >> (per-task accuracy): 93.08\n",
            "Epoch 0.6 >> (class accuracy): [97.2449, 98.6784, 89.9225, 92.3762, 95.6212, 91.704, 95.4071, 92.7043, 90.8624, 85.7284]\n",
            "Epoch 0.65 >> (per-task accuracy): 93.57\n",
            "Epoch 0.65 >> (class accuracy): [96.9388, 98.5022, 90.2132, 93.6634, 93.7882, 92.0404, 95.6159, 91.3424, 92.4025, 90.6838]\n",
            "Epoch 0.7 >> (per-task accuracy): 93.79\n",
            "Epoch 0.7 >> (class accuracy): [97.8571, 98.9427, 90.6977, 93.2673, 95.723, 94.3946, 94.7808, 91.3424, 89.7331, 90.783]\n",
            "Epoch 0.75 >> (per-task accuracy): 94.09\n",
            "Epoch 0.75 >> (class accuracy): [97.7551, 98.7665, 91.0853, 94.6535, 95.8248, 93.6099, 96.2422, 91.6342, 89.5277, 91.3776]\n",
            "Epoch 0.8 >> (per-task accuracy): 94.52\n",
            "Epoch 0.8 >> (class accuracy): [97.6531, 98.4141, 91.0853, 94.0594, 95.3157, 92.6009, 96.4509, 93.1907, 94.5585, 91.4767]\n",
            "Epoch 0.85 >> (per-task accuracy): 94.52\n",
            "Epoch 0.85 >> (class accuracy): [97.8571, 98.9427, 90.9884, 93.7624, 96.8432, 93.722, 95.7203, 93.3852, 93.8398, 89.7919]\n",
            "Epoch 0.9 >> (per-task accuracy): 94.81\n",
            "Epoch 0.9 >> (class accuracy): [97.1429, 98.7665, 93.314, 93.9604, 96.5377, 94.9552, 95.8246, 93.4825, 92.7105, 91.0803]\n",
            "Epoch 0.95 >> (per-task accuracy): 94.83\n",
            "Epoch 0.95 >> (class accuracy): [98.0612, 98.5022, 91.2791, 96.2376, 95.112, 91.5919, 95.7203, 95.8171, 93.2238, 92.0714]\n",
            "Epoch 1.0 >> (per-task accuracy): 94.88\n",
            "Epoch 1.0 >> (class accuracy): [97.2449, 98.1498, 92.345, 94.0594, 96.0285, 92.8251, 96.6597, 95.2335, 95.2772, 90.5847]\n",
            "OCS >> Task 1: {'accuracy': 40.18, 'per_class_accuracy': [75.102, 18.6784, 25.0969, 36.6337, 59.8778, 35.6502, 34.9687, 59.7276, 47.7413, 11.9921], 'loss': 1.886393455886841}\n",
            "OCS >> Task 2: {'accuracy': 53.56, 'per_class_accuracy': [82.1429, 38.7665, 40.3101, 53.3663, 66.4969, 46.5247, 47.0772, 66.4397, 68.6858, 28.2458], 'loss': 1.4085991786956786}\n",
            "OCS >> Task 3: {'accuracy': 70.04, 'per_class_accuracy': [85.6122, 61.4097, 66.0853, 80.495, 80.5499, 64.2377, 63.8831, 82.2957, 79.0554, 37.7602], 'loss': 0.9462710338592529}\n",
            "OCS >> Task 4: {'accuracy': 82.93, 'per_class_accuracy': [92.0408, 79.4714, 81.3953, 89.1089, 85.5397, 76.2332, 85.6994, 88.716, 84.8049, 66.4024], 'loss': 0.5901189355373383}\n",
            "OCS >> Task 5: {'accuracy': 89.96, 'per_class_accuracy': [94.1837, 91.1013, 88.9535, 92.4752, 92.2607, 83.6323, 93.8413, 93.7743, 88.809, 79.9802], 'loss': 0.3991996466875076}\n",
            "OCS >> Task 6: {'accuracy': 94.88, 'per_class_accuracy': [97.2449, 98.1498, 92.345, 94.0594, 96.0285, 92.8251, 96.6597, 95.2335, 95.2772, 90.5847], 'loss': 0.2767017736196518}\n",
            "OCS >> (average accuracy): 71.925\n",
            "OCS >> (Forgetting): 0.29005999999999993\n",
            "Maximum per-task accuracies: [96.34]\n",
            "\n",
            "---- Task 7 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 22.85\n",
            "Epoch 0.05 >> (class accuracy): [88.2653, 32.9515, 0.0, 11.7822, 94.3992, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.1 >> (per-task accuracy): 57.16\n",
            "Epoch 0.1 >> (class accuracy): [96.9388, 97.0044, 68.0233, 71.6832, 97.2505, 2.5785, 62.6305, 34.9222, 24.9487, 5.8474]\n",
            "Epoch 0.15 >> (per-task accuracy): 78.56\n",
            "Epoch 0.15 >> (class accuracy): [97.0408, 95.5947, 72.6744, 81.2871, 74.7454, 35.5381, 90.6054, 76.9455, 78.5421, 76.7096]\n",
            "Epoch 0.2 >> (per-task accuracy): 82.04\n",
            "Epoch 0.2 >> (class accuracy): [94.4898, 92.2467, 81.1047, 76.6337, 86.558, 65.3587, 92.5887, 85.0195, 78.0287, 66.0059]\n",
            "Epoch 0.25 >> (per-task accuracy): 83.34\n",
            "Epoch 0.25 >> (class accuracy): [94.3878, 91.7181, 79.4574, 73.3663, 86.3544, 79.3722, 93.6326, 84.8249, 75.9754, 73.5382]\n",
            "Epoch 0.3 >> (per-task accuracy): 85.02\n",
            "Epoch 0.3 >> (class accuracy): [95.9184, 93.2159, 81.1047, 75.6436, 88.2892, 82.5112, 92.9019, 84.8249, 76.8994, 78.1962]\n",
            "Epoch 0.35 >> (per-task accuracy): 86.14\n",
            "Epoch 0.35 >> (class accuracy): [95.4082, 94.0969, 81.8798, 78.4158, 89.9185, 82.3991, 94.6764, 87.2568, 79.8768, 76.7096]\n",
            "Epoch 0.4 >> (per-task accuracy): 87.75\n",
            "Epoch 0.4 >> (class accuracy): [96.3265, 95.7709, 83.0426, 80.396, 90.1222, 84.5291, 94.0501, 88.035, 83.3676, 81.0704]\n",
            "Epoch 0.45 >> (per-task accuracy): 88.57\n",
            "Epoch 0.45 >> (class accuracy): [96.5306, 95.4185, 84.2054, 79.1089, 89.613, 87.5561, 92.5887, 89.9805, 83.5729, 86.5213]\n",
            "Epoch 0.5 >> (per-task accuracy): 89.67\n",
            "Epoch 0.5 >> (class accuracy): [96.6327, 97.1806, 84.7868, 82.3762, 90.1222, 87.3318, 93.0063, 90.2724, 84.5996, 89.4945]\n",
            "Epoch 0.55 >> (per-task accuracy): 90.53\n",
            "Epoch 0.55 >> (class accuracy): [97.2449, 97.7093, 86.9186, 83.1683, 89.7149, 89.2377, 93.1106, 89.5914, 87.7823, 90.0892]\n",
            "Epoch 0.6 >> (per-task accuracy): 91.03\n",
            "Epoch 0.6 >> (class accuracy): [97.3469, 98.1498, 87.1124, 85.6436, 89.2057, 87.5561, 94.9896, 88.716, 89.5277, 91.1794]\n",
            "Epoch 0.65 >> (per-task accuracy): 91.55\n",
            "Epoch 0.65 >> (class accuracy): [96.7347, 97.9736, 88.1783, 85.4455, 89.9185, 90.583, 94.1545, 89.786, 90.5544, 91.5758]\n",
            "Epoch 0.7 >> (per-task accuracy): 91.84\n",
            "Epoch 0.7 >> (class accuracy): [97.2449, 98.5022, 88.3721, 85.4455, 92.1589, 91.2556, 94.7808, 90.1751, 88.2957, 91.5758]\n",
            "Epoch 0.75 >> (per-task accuracy): 92.39\n",
            "Epoch 0.75 >> (class accuracy): [97.551, 98.7665, 89.5349, 88.0198, 91.7515, 92.8251, 93.6326, 90.5642, 88.501, 92.1705]\n",
            "Epoch 0.8 >> (per-task accuracy): 92.9\n",
            "Epoch 0.8 >> (class accuracy): [97.1429, 98.7665, 88.8566, 91.1881, 90.7332, 91.704, 94.9896, 90.6615, 91.0678, 93.2607]\n",
            "Epoch 0.85 >> (per-task accuracy): 93.17\n",
            "Epoch 0.85 >> (class accuracy): [97.3469, 98.9427, 91.7636, 89.901, 92.1589, 92.3767, 95.5115, 89.8833, 89.7331, 93.4589]\n",
            "Epoch 0.9 >> (per-task accuracy): 93.75\n",
            "Epoch 0.9 >> (class accuracy): [97.551, 98.8546, 91.376, 92.2772, 92.9735, 92.2646, 95.5115, 91.1479, 91.3758, 93.558]\n",
            "Epoch 0.95 >> (per-task accuracy): 93.57\n",
            "Epoch 0.95 >> (class accuracy): [96.4286, 98.8546, 91.9574, 91.5842, 89.4094, 94.6188, 94.7808, 90.6615, 91.7864, 95.1437]\n",
            "Epoch 1.0 >> (per-task accuracy): 94.04\n",
            "Epoch 1.0 >> (class accuracy): [96.8367, 98.8546, 92.4419, 92.5743, 91.1405, 94.2825, 95.8246, 91.2451, 92.2998, 94.45]\n",
            "OCS >> Task 1: {'accuracy': 36.97, 'per_class_accuracy': [75.6122, 10.3084, 25.8721, 40.8911, 54.1752, 38.3408, 51.7745, 23.0545, 29.0554, 26.6601], 'loss': 2.128165111541748}\n",
            "OCS >> Task 2: {'accuracy': 50.89, 'per_class_accuracy': [81.4286, 32.4229, 40.8915, 56.3366, 60.387, 50.8969, 61.3779, 40.7588, 47.6386, 41.0307], 'loss': 1.593083743286133}\n",
            "OCS >> Task 3: {'accuracy': 64.22, 'per_class_accuracy': [81.7347, 48.0176, 61.1434, 72.4752, 70.2648, 63.3408, 70.2505, 66.0506, 59.7536, 51.9326], 'loss': 1.134932585144043}\n",
            "OCS >> Task 4: {'accuracy': 76.65, 'per_class_accuracy': [92.3469, 72.3348, 74.6124, 80.6931, 75.5601, 70.2915, 83.7161, 78.0156, 70.9446, 68.3845], 'loss': 0.7701474807739258}\n",
            "OCS >> Task 5: {'accuracy': 86.45, 'per_class_accuracy': [94.0816, 88.6344, 84.2054, 87.2277, 84.1141, 83.7444, 90.1879, 86.4786, 81.1088, 84.3409], 'loss': 0.5034170578956604}\n",
            "OCS >> Task 6: {'accuracy': 90.87, 'per_class_accuracy': [96.7347, 97.8855, 90.6008, 88.0198, 89.2057, 90.9193, 93.737, 86.7704, 85.1129, 88.999], 'loss': 0.3856115000963211}\n",
            "OCS >> Task 7: {'accuracy': 94.04, 'per_class_accuracy': [96.8367, 98.8546, 92.4419, 92.5743, 91.1405, 94.2825, 95.8246, 91.2451, 92.2998, 94.45], 'loss': 0.3231031184434891}\n",
            "OCS >> (average accuracy): 71.44142857142857\n",
            "OCS >> (Forgetting): 0.28664999999999996\n",
            "Maximum per-task accuracies: [96.34]\n",
            "\n",
            "---- Task 8 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 16.3\n",
            "Epoch 0.05 >> (class accuracy): [8.6735, 0.0, 0.0, 95.1485, 0.0, 0.0, 0.0, 56.8093, 0.0, 0.0]\n",
            "Epoch 0.1 >> (per-task accuracy): 49.61\n",
            "Epoch 0.1 >> (class accuracy): [97.7551, 46.2555, 71.8023, 42.9703, 94.2974, 0.1121, 41.5449, 61.6732, 30.1848, 4.9554]\n",
            "Epoch 0.15 >> (per-task accuracy): 74.38\n",
            "Epoch 0.15 >> (class accuracy): [94.898, 94.0969, 69.9612, 59.505, 74.2363, 47.1973, 80.7933, 81.9066, 73.8193, 62.6363]\n",
            "Epoch 0.2 >> (per-task accuracy): 78.59\n",
            "Epoch 0.2 >> (class accuracy): [94.7959, 92.4229, 73.062, 73.6634, 60.6925, 61.2108, 88.9353, 77.5292, 80.6982, 79.8811]\n",
            "Epoch 0.25 >> (per-task accuracy): 79.81\n",
            "Epoch 0.25 >> (class accuracy): [91.9388, 93.1278, 75.0, 76.3366, 64.053, 61.5471, 91.1273, 78.3074, 83.6756, 79.9802]\n",
            "Epoch 0.3 >> (per-task accuracy): 81.51\n",
            "Epoch 0.3 >> (class accuracy): [94.2857, 94.6256, 75.3876, 78.0198, 68.8391, 64.574, 91.7537, 77.1401, 84.1889, 83.5481]\n",
            "Epoch 0.35 >> (per-task accuracy): 83.28\n",
            "Epoch 0.35 >> (class accuracy): [94.0816, 94.8018, 81.7829, 78.4158, 77.2912, 67.1525, 90.6054, 78.5992, 85.6263, 81.8632]\n",
            "Epoch 0.4 >> (per-task accuracy): 83.93\n",
            "Epoch 0.4 >> (class accuracy): [92.8571, 95.5947, 78.5853, 76.7327, 73.8289, 76.1211, 90.8142, 80.5447, 85.0103, 87.4133]\n",
            "Epoch 0.45 >> (per-task accuracy): 85.2\n",
            "Epoch 0.45 >> (class accuracy): [94.5918, 96.2115, 79.3605, 82.8713, 79.1242, 69.9552, 91.7537, 83.6576, 85.4209, 86.5213]\n",
            "Epoch 0.5 >> (per-task accuracy): 86.52\n",
            "Epoch 0.5 >> (class accuracy): [94.7959, 95.7709, 81.3953, 83.8614, 81.8737, 75.7848, 90.8142, 86.8677, 85.1129, 86.9177]\n",
            "Epoch 0.55 >> (per-task accuracy): 87.22\n",
            "Epoch 0.55 >> (class accuracy): [96.1224, 96.4758, 82.2674, 82.2772, 83.5031, 76.7937, 91.4405, 86.7704, 86.653, 88.0079]\n",
            "Epoch 0.6 >> (per-task accuracy): 88.18\n",
            "Epoch 0.6 >> (class accuracy): [96.2245, 96.8282, 84.1085, 80.6931, 88.7984, 81.9507, 91.858, 87.2568, 85.729, 87.0168]\n",
            "Epoch 0.65 >> (per-task accuracy): 88.81\n",
            "Epoch 0.65 >> (class accuracy): [97.1429, 96.8282, 84.7868, 82.1782, 88.391, 82.9596, 91.3361, 88.716, 86.4476, 88.0079]\n",
            "Epoch 0.7 >> (per-task accuracy): 89.6\n",
            "Epoch 0.7 >> (class accuracy): [97.2449, 97.2687, 85.1744, 84.2574, 89.3075, 83.6323, 91.3361, 89.3969, 88.0903, 88.999]\n",
            "Epoch 0.75 >> (per-task accuracy): 90.35\n",
            "Epoch 0.75 >> (class accuracy): [97.8571, 97.3568, 87.2093, 88.0198, 88.5947, 81.5022, 91.9624, 89.786, 89.6304, 89.9901]\n",
            "Epoch 0.8 >> (per-task accuracy): 90.57\n",
            "Epoch 0.8 >> (class accuracy): [96.5306, 97.4449, 86.4341, 84.3564, 90.1222, 89.2377, 92.2756, 90.0778, 89.0144, 89.4945]\n",
            "Epoch 0.85 >> (per-task accuracy): 91.48\n",
            "Epoch 0.85 >> (class accuracy): [97.6531, 98.0617, 87.9845, 87.2277, 91.2424, 90.6951, 92.7975, 91.3424, 88.193, 88.8999]\n",
            "Epoch 0.9 >> (per-task accuracy): 91.99\n",
            "Epoch 0.9 >> (class accuracy): [97.551, 97.8855, 89.7287, 88.4158, 91.1405, 90.4709, 92.4843, 92.3152, 89.2197, 89.891]\n",
            "Epoch 0.95 >> (per-task accuracy): 92.32\n",
            "Epoch 0.95 >> (class accuracy): [97.449, 98.2379, 88.9535, 90.297, 91.446, 90.3587, 93.8413, 93.0934, 89.2197, 89.4945]\n",
            "Epoch 1.0 >> (per-task accuracy): 92.59\n",
            "Epoch 1.0 >> (class accuracy): [97.551, 98.1498, 89.7287, 90.8911, 91.6497, 90.3587, 92.6931, 93.2879, 90.6571, 90.0892]\n",
            "OCS >> Task 1: {'accuracy': 30.62, 'per_class_accuracy': [65.7143, 6.2555, 8.2364, 50.5941, 59.9796, 13.2287, 39.3528, 36.5759, 16.8378, 12.5867], 'loss': 2.2611433227539064}\n",
            "OCS >> Task 2: {'accuracy': 38.56, 'per_class_accuracy': [72.7551, 20.2643, 18.0233, 64.0594, 60.2851, 16.0314, 40.501, 46.4981, 28.1314, 20.3171], 'loss': 1.8896683650970458}\n",
            "OCS >> Task 3: {'accuracy': 49.91, 'per_class_accuracy': [79.7959, 40.8811, 32.8488, 73.9604, 65.6823, 25.8969, 49.7912, 72.2763, 29.6715, 27.1556], 'loss': 1.5174193534851075}\n",
            "OCS >> Task 4: {'accuracy': 60.79, 'per_class_accuracy': [87.1429, 63.2599, 44.186, 74.0594, 66.3951, 33.6323, 68.0585, 78.6965, 44.3532, 45.3915], 'loss': 1.1848939754486083}\n",
            "OCS >> Task 5: {'accuracy': 72.16, 'per_class_accuracy': [93.9796, 73.0396, 58.7209, 85.9406, 73.5234, 54.0359, 73.7996, 90.5642, 65.5031, 50.7433], 'loss': 0.855684947013855}\n",
            "OCS >> Task 6: {'accuracy': 81.81, 'per_class_accuracy': [94.6939, 92.0705, 73.6434, 87.4257, 80.2444, 72.0852, 81.9415, 88.6187, 75.2567, 69.8712], 'loss': 0.6158830439567566}\n",
            "OCS >> Task 7: {'accuracy': 88.46, 'per_class_accuracy': [95.5102, 98.2379, 83.4302, 90.9901, 83.0957, 82.0628, 88.2046, 93.9689, 83.9836, 83.0525], 'loss': 0.4740876160144806}\n",
            "OCS >> Task 8: {'accuracy': 92.59, 'per_class_accuracy': [97.551, 98.1498, 89.7287, 90.8911, 91.6497, 90.3587, 92.6931, 93.2879, 90.6571, 90.0892], 'loss': 0.3926763230085373}\n",
            "OCS >> (average accuracy): 64.3625\n",
            "OCS >> (Forgetting): 0.36010000000000003\n",
            "Maximum per-task accuracies: [96.34]\n",
            "\n",
            "---- Task 9 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 17.3\n",
            "Epoch 0.05 >> (class accuracy): [70.9184, 0.0, 0.0, 72.8713, 22.6069, 0.0, 0.0, 7.393, 0.0, 0.0991]\n",
            "Epoch 0.1 >> (per-task accuracy): 38.42\n",
            "Epoch 0.1 >> (class accuracy): [99.0816, 50.9251, 70.155, 12.6733, 28.4114, 0.0, 5.0104, 71.4008, 0.0, 37.6611]\n",
            "Epoch 0.15 >> (per-task accuracy): 59.16\n",
            "Epoch 0.15 >> (class accuracy): [97.3469, 98.1498, 77.0349, 36.3366, 83.4012, 5.4933, 55.5324, 81.2257, 11.807, 33.3003]\n",
            "Epoch 0.2 >> (per-task accuracy): 72.9\n",
            "Epoch 0.2 >> (class accuracy): [96.6327, 98.4141, 74.4186, 60.297, 82.4847, 51.4574, 83.0898, 73.7354, 45.9959, 57.2844]\n",
            "Epoch 0.25 >> (per-task accuracy): 77.68\n",
            "Epoch 0.25 >> (class accuracy): [95.102, 96.8282, 72.6744, 73.9604, 70.2648, 63.7892, 87.7871, 77.5292, 65.8111, 69.5738]\n",
            "Epoch 0.3 >> (per-task accuracy): 79.61\n",
            "Epoch 0.3 >> (class accuracy): [91.8367, 97.2687, 76.8411, 77.5248, 66.5988, 67.9372, 87.9958, 78.5019, 74.9487, 73.439]\n",
            "Epoch 0.35 >> (per-task accuracy): 81.02\n",
            "Epoch 0.35 >> (class accuracy): [91.6327, 97.0925, 77.7132, 82.1782, 75.7637, 69.1704, 86.6388, 81.2257, 74.23, 71.4569]\n",
            "Epoch 0.4 >> (per-task accuracy): 82.26\n",
            "Epoch 0.4 >> (class accuracy): [92.551, 97.1806, 80.4264, 81.1881, 79.1242, 73.3184, 84.3424, 79.6693, 77.3101, 74.8266]\n",
            "Epoch 0.45 >> (per-task accuracy): 83.29\n",
            "Epoch 0.45 >> (class accuracy): [94.0816, 97.0044, 78.876, 82.3762, 77.9022, 75.0, 85.2818, 82.9767, 79.9795, 77.0069]\n",
            "Epoch 0.5 >> (per-task accuracy): 84.02\n",
            "Epoch 0.5 >> (class accuracy): [93.7755, 97.2687, 81.2016, 81.1881, 79.0224, 77.2422, 86.2213, 83.8521, 81.4168, 76.8087]\n",
            "Epoch 0.55 >> (per-task accuracy): 84.6\n",
            "Epoch 0.55 >> (class accuracy): [93.3673, 97.533, 81.9767, 80.8911, 79.5316, 81.6143, 84.8643, 84.9222, 80.3901, 78.9891]\n",
            "Epoch 0.6 >> (per-task accuracy): 85.2\n",
            "Epoch 0.6 >> (class accuracy): [95.0, 97.9736, 83.5271, 82.2772, 81.4664, 81.3901, 84.4468, 85.0195, 80.4928, 78.3944]\n",
            "Epoch 0.65 >> (per-task accuracy): 85.97\n",
            "Epoch 0.65 >> (class accuracy): [94.7959, 97.6211, 84.593, 81.2871, 84.7251, 85.2018, 86.1169, 85.8949, 81.1088, 76.9078]\n",
            "Epoch 0.7 >> (per-task accuracy): 85.96\n",
            "Epoch 0.7 >> (class accuracy): [95.0, 98.0617, 84.0116, 78.9109, 83.8086, 87.4439, 85.595, 82.1012, 80.9035, 82.557]\n",
            "Epoch 0.75 >> (per-task accuracy): 86.92\n",
            "Epoch 0.75 >> (class accuracy): [94.6939, 97.0044, 85.1744, 80.6931, 86.6599, 89.4619, 86.1169, 85.0195, 82.4435, 81.0704]\n",
            "Epoch 0.8 >> (per-task accuracy): 87.5\n",
            "Epoch 0.8 >> (class accuracy): [95.3061, 97.1806, 85.2713, 81.8812, 87.8819, 88.9013, 85.4906, 86.3813, 83.7782, 81.9623]\n",
            "Epoch 0.85 >> (per-task accuracy): 88.17\n",
            "Epoch 0.85 >> (class accuracy): [95.4082, 97.8855, 86.531, 82.4752, 87.9837, 90.2466, 86.3257, 87.1595, 83.8809, 82.8543]\n",
            "Epoch 0.9 >> (per-task accuracy): 88.78\n",
            "Epoch 0.9 >> (class accuracy): [96.8367, 98.5022, 85.4651, 85.3465, 88.5947, 87.7803, 87.7871, 88.4241, 83.1622, 84.6383]\n",
            "Epoch 0.95 >> (per-task accuracy): 89.13\n",
            "Epoch 0.95 >> (class accuracy): [95.8163, 98.2379, 87.4031, 84.4554, 90.1222, 89.4619, 89.0397, 88.1323, 83.5729, 84.0436]\n",
            "Epoch 1.0 >> (per-task accuracy): 89.24\n",
            "Epoch 1.0 >> (class accuracy): [95.8163, 98.2379, 87.1124, 82.8713, 87.9837, 90.8072, 90.9186, 87.3541, 83.1622, 87.3142]\n",
            "OCS >> Task 1: {'accuracy': 27.73, 'per_class_accuracy': [60.8163, 5.022, 2.8101, 47.1287, 54.7862, 8.9686, 34.0292, 28.1128, 19.6099, 18.9296], 'loss': 2.253143320465088}\n",
            "OCS >> Task 2: {'accuracy': 33.05, 'per_class_accuracy': [66.7347, 9.6916, 9.4961, 55.5446, 53.666, 13.2287, 31.7328, 40.5642, 19.7125, 32.111], 'loss': 2.007936817550659}\n",
            "OCS >> Task 3: {'accuracy': 43.5, 'per_class_accuracy': [72.1429, 32.9515, 18.7016, 73.6634, 52.7495, 20.4036, 49.0605, 60.7004, 13.5524, 40.2379], 'loss': 1.6854087501525878}\n",
            "OCS >> Task 4: {'accuracy': 51.33, 'per_class_accuracy': [82.2449, 51.9824, 29.0698, 69.505, 50.8147, 26.4574, 58.7683, 70.2335, 15.4004, 55.996], 'loss': 1.4433466777801514}\n",
            "OCS >> Task 5: {'accuracy': 60.71, 'per_class_accuracy': [90.2041, 62.467, 40.0194, 79.1089, 56.4155, 42.9372, 64.5094, 77.2374, 31.1088, 60.8523], 'loss': 1.1576400358200074}\n",
            "OCS >> Task 6: {'accuracy': 69.44, 'per_class_accuracy': [93.8776, 77.533, 58.0426, 80.297, 49.4908, 60.6502, 74.3215, 77.1401, 41.8891, 78.6918], 'loss': 0.9175898056030274}\n",
            "OCS >> Task 7: {'accuracy': 77.98, 'per_class_accuracy': [93.8776, 88.8106, 72.3837, 82.2772, 61.7108, 73.5426, 82.2547, 83.5603, 59.0349, 80.0793], 'loss': 0.7046627513408661}\n",
            "OCS >> Task 8: {'accuracy': 84.78, 'per_class_accuracy': [95.5102, 96.8282, 85.4651, 80.495, 80.2444, 85.426, 87.9958, 79.9611, 72.2793, 82.1606], 'loss': 0.5584366662025452}\n",
            "OCS >> Task 9: {'accuracy': 89.24, 'per_class_accuracy': [95.8163, 98.2379, 87.1124, 82.8713, 87.9837, 90.8072, 90.9186, 87.3541, 83.1622, 87.3142], 'loss': 0.478615314412117}\n",
            "OCS >> (average accuracy): 59.75111111111111\n",
            "OCS >> (Forgetting): 0.40275000000000005\n",
            "Maximum per-task accuracies: [96.34]\n",
            "\n",
            "---- Task 10 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 9.76\n",
            "Epoch 0.05 >> (class accuracy): [0.0, 0.0, 0.0, 0.0, 0.2037, 0.0, 0.0, 0.0, 100.0, 0.0]\n",
            "Epoch 0.1 >> (per-task accuracy): 15.89\n",
            "Epoch 0.1 >> (class accuracy): [100.0, 4.4053, 10.7558, 0.5941, 43.4827, 0.0, 0.0, 1.07, 0.308, 0.0991]\n",
            "Epoch 0.15 >> (per-task accuracy): 35.69\n",
            "Epoch 0.15 >> (class accuracy): [96.3265, 33.7445, 14.9225, 76.5347, 90.1222, 0.0, 7.3069, 7.2957, 28.1314, 1.0902]\n",
            "Epoch 0.2 >> (per-task accuracy): 61.38\n",
            "Epoch 0.2 >> (class accuracy): [96.0204, 81.8502, 69.0891, 71.6832, 92.5662, 24.5516, 72.7557, 14.5914, 57.7002, 29.1378]\n",
            "Epoch 0.25 >> (per-task accuracy): 69.89\n",
            "Epoch 0.25 >> (class accuracy): [92.8571, 91.4537, 69.3798, 63.8614, 82.6884, 44.3946, 82.4635, 41.6342, 78.7474, 48.2656]\n",
            "Epoch 0.3 >> (per-task accuracy): 73.08\n",
            "Epoch 0.3 >> (class accuracy): [89.4898, 91.9824, 68.8953, 71.7822, 68.2281, 54.9327, 81.9415, 52.9183, 79.7741, 67.889]\n",
            "Epoch 0.35 >> (per-task accuracy): 76.19\n",
            "Epoch 0.35 >> (class accuracy): [91.6327, 90.9251, 70.3488, 71.5842, 74.5418, 68.6099, 82.6722, 67.0233, 82.9569, 60.1586]\n",
            "Epoch 0.4 >> (per-task accuracy): 78.52\n",
            "Epoch 0.4 >> (class accuracy): [92.449, 91.8943, 72.2868, 74.1584, 74.1344, 70.2915, 85.6994, 74.2218, 82.8542, 65.6095]\n",
            "Epoch 0.45 >> (per-task accuracy): 79.03\n",
            "Epoch 0.45 >> (class accuracy): [88.4694, 92.5991, 70.8333, 75.9406, 73.7271, 75.1121, 86.6388, 71.1089, 82.5462, 72.1506]\n",
            "Epoch 0.5 >> (per-task accuracy): 80.63\n",
            "Epoch 0.5 >> (class accuracy): [92.551, 93.0396, 72.7713, 80.198, 79.2261, 71.7489, 87.3695, 77.6265, 82.8542, 67.2944]\n",
            "Epoch 0.55 >> (per-task accuracy): 81.9\n",
            "Epoch 0.55 >> (class accuracy): [93.0612, 93.3921, 73.7403, 79.901, 77.0876, 76.2332, 90.6054, 78.2101, 79.9795, 75.5203]\n",
            "Epoch 0.6 >> (per-task accuracy): 82.84\n",
            "Epoch 0.6 >> (class accuracy): [93.5714, 93.1278, 75.969, 80.8911, 79.7352, 78.3632, 90.1879, 82.1012, 81.0062, 72.3489]\n",
            "Epoch 0.65 >> (per-task accuracy): 83.81\n",
            "Epoch 0.65 >> (class accuracy): [93.7755, 93.7445, 76.938, 81.8812, 80.9572, 79.3722, 90.3967, 81.2257, 81.9302, 76.8087]\n",
            "Epoch 0.7 >> (per-task accuracy): 83.67\n",
            "Epoch 0.7 >> (class accuracy): [94.2857, 94.8018, 73.9341, 80.6931, 82.5866, 79.7085, 90.6054, 83.3658, 81.8275, 73.8355]\n",
            "Epoch 0.75 >> (per-task accuracy): 84.93\n",
            "Epoch 0.75 >> (class accuracy): [94.898, 95.5066, 75.6783, 82.3762, 84.4196, 81.9507, 89.7704, 84.3385, 82.3409, 77.0069]\n",
            "Epoch 0.8 >> (per-task accuracy): 85.25\n",
            "Epoch 0.8 >> (class accuracy): [94.4898, 95.6828, 77.0349, 81.9802, 87.0672, 82.7354, 90.1879, 84.5331, 82.0329, 75.8176]\n",
            "Epoch 0.85 >> (per-task accuracy): 85.53\n",
            "Epoch 0.85 >> (class accuracy): [95.102, 95.6828, 78.0039, 82.1782, 85.0305, 82.9596, 89.2484, 84.9222, 83.4702, 77.7007]\n",
            "Epoch 0.9 >> (per-task accuracy): 86.48\n",
            "Epoch 0.9 >> (class accuracy): [95.4082, 95.5066, 79.6512, 84.3564, 88.0855, 83.1839, 89.9791, 85.4086, 83.9836, 78.2953]\n",
            "Epoch 0.95 >> (per-task accuracy): 86.88\n",
            "Epoch 0.95 >> (class accuracy): [96.4286, 96.0352, 79.0698, 83.6634, 88.6965, 83.0717, 89.666, 84.8249, 84.4969, 81.8632]\n",
            "Epoch 1.0 >> (per-task accuracy): 87.36\n",
            "Epoch 1.0 >> (class accuracy): [96.2245, 96.652, 79.845, 85.1485, 87.78, 83.9686, 90.0835, 85.214, 84.3943, 83.2507]\n",
            "OCS >> Task 1: {'accuracy': 27.98, 'per_class_accuracy': [52.6531, 8.7225, 0.7752, 41.7822, 44.7047, 6.6143, 52.2965, 26.07, 17.1458, 31.6155], 'loss': 2.284513003540039}\n",
            "OCS >> Task 2: {'accuracy': 35.4, 'per_class_accuracy': [59.6939, 28.0176, 3.5853, 51.0891, 49.5927, 10.0897, 52.9228, 41.1479, 16.1191, 41.6254], 'loss': 2.0449471626281737}\n",
            "OCS >> Task 3: {'accuracy': 43.4, 'per_class_accuracy': [71.4286, 50.5727, 8.8178, 63.9604, 49.2872, 11.3229, 61.1691, 58.1712, 10.883, 44.995], 'loss': 1.778639226913452}\n",
            "OCS >> Task 4: {'accuracy': 48.58, 'per_class_accuracy': [79.7959, 69.6035, 13.7597, 61.7822, 43.5845, 11.435, 70.1461, 63.9105, 14.4764, 51.5362], 'loss': 1.6329215297698974}\n",
            "OCS >> Task 5: {'accuracy': 53.42, 'per_class_accuracy': [89.0816, 83.7004, 13.8566, 66.5347, 42.2607, 18.9462, 69.3111, 69.8444, 19.9179, 53.9148], 'loss': 1.4276623491287233}\n",
            "OCS >> Task 6: {'accuracy': 59.1, 'per_class_accuracy': [92.2449, 95.3304, 23.7403, 72.0792, 38.0855, 32.1749, 76.618, 62.1595, 27.9261, 63.9247], 'loss': 1.2525344224929809}\n",
            "OCS >> Task 7: {'accuracy': 67.39, 'per_class_accuracy': [95.5102, 97.3568, 34.593, 78.8119, 47.2505, 48.4305, 78.0793, 81.5175, 46.4066, 60.555], 'loss': 1.0027109292984009}\n",
            "OCS >> Task 8: {'accuracy': 77.75, 'per_class_accuracy': [96.1224, 98.9427, 57.3643, 82.7723, 68.8391, 66.4798, 87.7871, 75.0, 72.4846, 68.8801], 'loss': 0.7414719017028809}\n",
            "OCS >> Task 9: {'accuracy': 83.48, 'per_class_accuracy': [95.5102, 98.4141, 70.9302, 82.4752, 77.3931, 74.1031, 92.7975, 82.6848, 80.2875, 78.0971], 'loss': 0.6182083787441254}\n",
            "OCS >> Task 10: {'accuracy': 87.36, 'per_class_accuracy': [96.2245, 96.652, 79.845, 85.1485, 87.78, 83.9686, 90.0835, 85.214, 84.3943, 83.2507], 'loss': 0.5487322494506836}\n",
            "OCS >> (average accuracy): 58.386\n",
            "OCS >> (Forgetting): 0.4117333333333334\n",
            "Maximum per-task accuracies: [96.34]\n",
            "\n",
            "---- Task 11 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 17.59\n",
            "Epoch 0.05 >> (class accuracy): [79.3878, 0.0, 0.0969, 6.0396, 0.9165, 0.0, 0.0, 67.4125, 22.2793, 0.0]\n",
            "Epoch 0.1 >> (per-task accuracy): 16.37\n",
            "Epoch 0.1 >> (class accuracy): [100.0, 0.0, 9.1085, 18.8119, 1.0183, 0.0, 0.0, 3.4047, 0.1027, 32.4083]\n",
            "Epoch 0.15 >> (per-task accuracy): 23.66\n",
            "Epoch 0.15 >> (class accuracy): [99.898, 36.2115, 15.407, 9.901, 0.1018, 0.0, 0.0, 3.0156, 0.1027, 67.7899]\n",
            "Epoch 0.2 >> (per-task accuracy): 41.07\n",
            "Epoch 0.2 >> (class accuracy): [99.1837, 82.5551, 60.9496, 38.1188, 3.7678, 0.0, 0.8351, 3.6965, 19.0965, 90.6838]\n",
            "Epoch 0.25 >> (per-task accuracy): 50.77\n",
            "Epoch 0.25 >> (class accuracy): [98.0612, 93.6564, 74.9031, 66.2376, 10.387, 1.1211, 18.3716, 3.7938, 35.2156, 93.2607]\n",
            "Epoch 0.3 >> (per-task accuracy): 60.22\n",
            "Epoch 0.3 >> (class accuracy): [97.3469, 96.2996, 75.0969, 78.3168, 31.2627, 28.0269, 45.8246, 8.463, 43.5318, 89.3954]\n",
            "Epoch 0.35 >> (per-task accuracy): 68.92\n",
            "Epoch 0.35 >> (class accuracy): [96.0204, 97.7974, 74.2248, 82.3762, 49.389, 47.7578, 68.7891, 24.9027, 57.0842, 85.332]\n",
            "Epoch 0.4 >> (per-task accuracy): 74.89\n",
            "Epoch 0.4 >> (class accuracy): [94.6939, 97.6211, 75.4845, 82.8713, 72.3014, 61.7713, 76.3048, 38.716, 68.8912, 76.9078]\n",
            "Epoch 0.45 >> (per-task accuracy): 77.47\n",
            "Epoch 0.45 >> (class accuracy): [94.1837, 97.6211, 74.9031, 80.198, 70.3666, 64.6861, 83.1942, 61.0895, 73.7166, 71.556]\n",
            "Epoch 0.5 >> (per-task accuracy): 78.74\n",
            "Epoch 0.5 >> (class accuracy): [94.2857, 96.4758, 74.3217, 77.0297, 77.9022, 65.3587, 84.6555, 64.2023, 81.9302, 68.6819]\n",
            "Epoch 0.55 >> (per-task accuracy): 79.85\n",
            "Epoch 0.55 >> (class accuracy): [92.449, 96.2115, 73.8372, 80.099, 74.5418, 66.2556, 87.9958, 74.7082, 79.6715, 69.9703]\n",
            "Epoch 0.6 >> (per-task accuracy): 79.92\n",
            "Epoch 0.6 >> (class accuracy): [93.8776, 96.1233, 73.3527, 75.6436, 73.3198, 69.7309, 88.2046, 68.3852, 80.3901, 77.998]\n",
            "Epoch 0.65 >> (per-task accuracy): 81.1\n",
            "Epoch 0.65 >> (class accuracy): [94.3878, 96.652, 74.6124, 75.8416, 79.3279, 69.7309, 87.6827, 74.7082, 82.8542, 72.9435]\n",
            "Epoch 0.7 >> (per-task accuracy): 81.93\n",
            "Epoch 0.7 >> (class accuracy): [94.0816, 97.1806, 77.4225, 77.4257, 78.8187, 70.4036, 88.8309, 73.8327, 82.4435, 76.5114]\n",
            "Epoch 0.75 >> (per-task accuracy): 82.02\n",
            "Epoch 0.75 >> (class accuracy): [93.1633, 97.533, 75.0969, 77.1287, 78.2077, 71.4126, 89.666, 76.3619, 82.4435, 76.9078]\n",
            "Epoch 0.8 >> (per-task accuracy): 82.77\n",
            "Epoch 0.8 >> (class accuracy): [94.2857, 97.0925, 76.0659, 76.8317, 82.0774, 73.0942, 89.9791, 79.6693, 82.8542, 73.7364]\n",
            "Epoch 0.85 >> (per-task accuracy): 83.33\n",
            "Epoch 0.85 >> (class accuracy): [95.0, 97.2687, 77.1318, 77.6238, 80.7536, 73.6547, 89.8747, 79.3774, 83.0595, 77.5025]\n",
            "Epoch 0.9 >> (per-task accuracy): 83.64\n",
            "Epoch 0.9 >> (class accuracy): [94.2857, 97.2687, 78.1008, 76.3366, 81.6701, 74.6637, 90.0835, 82.5875, 84.7023, 74.7275]\n",
            "Epoch 0.95 >> (per-task accuracy): 84.27\n",
            "Epoch 0.95 >> (class accuracy): [94.3878, 97.1806, 79.6512, 77.8218, 83.2994, 75.3363, 89.3528, 84.0467, 84.1889, 75.4212]\n",
            "Epoch 1.0 >> (per-task accuracy): 84.4\n",
            "Epoch 1.0 >> (class accuracy): [94.7959, 97.2687, 78.7791, 77.3267, 83.1976, 76.6816, 90.0835, 83.2685, 82.7515, 77.998]\n",
            "OCS >> Task 1: {'accuracy': 26.46, 'per_class_accuracy': [42.551, 8.4581, 0.4845, 29.2079, 41.1405, 6.6143, 65.9708, 21.8872, 18.3778, 33.1021], 'loss': 2.3368029582977297}\n",
            "OCS >> Task 2: {'accuracy': 31.58, 'per_class_accuracy': [50.8163, 23.8767, 1.1628, 32.8713, 48.778, 6.9507, 64.7182, 29.4747, 20.7392, 37.5619], 'loss': 2.1640730850219727}\n",
            "OCS >> Task 3: {'accuracy': 37.4, 'per_class_accuracy': [55.7143, 48.0176, 1.5504, 43.4653, 49.5927, 10.8744, 66.8058, 48.5409, 9.6509, 37.3637], 'loss': 1.9404215673446654}\n",
            "OCS >> Task 4: {'accuracy': 41.64, 'per_class_accuracy': [66.7347, 67.6652, 2.3256, 38.5149, 44.0937, 9.3049, 71.5031, 51.751, 12.9363, 46.5808], 'loss': 1.8665521314620972}\n",
            "OCS >> Task 5: {'accuracy': 45.79, 'per_class_accuracy': [82.449, 87.2247, 4.3605, 38.8119, 42.1589, 15.9193, 68.7891, 53.8911, 11.6016, 45.7879], 'loss': 1.7295826400756835}\n",
            "OCS >> Task 6: {'accuracy': 49.07, 'per_class_accuracy': [85.4082, 95.4185, 8.3333, 39.505, 34.9287, 27.3543, 75.3653, 46.4981, 9.2402, 61.9425], 'loss': 1.6190260181427}\n",
            "OCS >> Task 7: {'accuracy': 55.49, 'per_class_accuracy': [90.8163, 97.8855, 14.438, 53.9604, 39.4094, 36.435, 77.453, 70.0389, 14.5791, 53.3201], 'loss': 1.395607975769043}\n",
            "OCS >> Task 8: {'accuracy': 65.49, 'per_class_accuracy': [93.4694, 97.8855, 29.9419, 59.802, 58.1466, 56.9507, 84.7599, 71.0117, 43.9425, 55.5005], 'loss': 1.0849561717033387}\n",
            "OCS >> Task 9: {'accuracy': 75.12, 'per_class_accuracy': [93.7755, 97.9736, 53.6822, 63.2673, 70.8758, 67.6009, 88.309, 79.7665, 66.4271, 66.997], 'loss': 0.8446303350448608}\n",
            "OCS >> Task 10: {'accuracy': 81.68, 'per_class_accuracy': [93.5714, 96.9163, 72.6744, 72.7723, 78.9206, 74.4395, 87.9958, 83.5603, 80.5955, 73.3399], 'loss': 0.6991773409843445}\n",
            "OCS >> Task 11: {'accuracy': 84.4, 'per_class_accuracy': [94.7959, 97.2687, 78.7791, 77.3267, 83.1976, 76.6816, 90.0835, 83.2685, 82.7515, 77.998], 'loss': 0.6398421366691589}\n",
            "OCS >> (average accuracy): 54.01090909090909\n",
            "OCS >> (Forgetting): 0.4536800000000001\n",
            "Maximum per-task accuracies: [96.34]\n",
            "\n",
            "---- Task 12 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 12.09\n",
            "Epoch 0.05 >> (class accuracy): [5.4082, 0.0, 0.0, 89.703, 0.0, 0.0, 0.0, 24.2218, 0.1027, 0.0]\n",
            "Epoch 0.1 >> (per-task accuracy): 17.48\n",
            "Epoch 0.1 >> (class accuracy): [75.3061, 0.0, 0.6783, 97.6238, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6848]\n",
            "Epoch 0.15 >> (per-task accuracy): 16.01\n",
            "Epoch 0.15 >> (class accuracy): [99.898, 0.0, 1.5504, 35.6436, 0.0, 0.0, 0.1044, 16.7315, 0.0, 7.2349]\n",
            "Epoch 0.2 >> (per-task accuracy): 27.35\n",
            "Epoch 0.2 >> (class accuracy): [99.898, 25.1982, 10.562, 25.5446, 2.7495, 0.0, 0.2088, 33.0739, 0.308, 72.448]\n",
            "Epoch 0.25 >> (per-task accuracy): 39.65\n",
            "Epoch 0.25 >> (class accuracy): [98.7755, 25.1101, 31.7829, 63.5644, 4.277, 0.0, 7.9332, 47.9572, 26.7967, 86.224]\n",
            "Epoch 0.3 >> (per-task accuracy): 50.67\n",
            "Epoch 0.3 >> (class accuracy): [97.7551, 77.7093, 67.345, 69.505, 6.9246, 1.7937, 38.2046, 5.9339, 37.885, 94.1526]\n",
            "Epoch 0.35 >> (per-task accuracy): 65.0\n",
            "Epoch 0.35 >> (class accuracy): [96.2245, 91.4537, 74.8062, 73.3663, 38.9002, 25.2242, 60.1253, 31.9066, 60.883, 89.3954]\n",
            "Epoch 0.4 >> (per-task accuracy): 71.66\n",
            "Epoch 0.4 >> (class accuracy): [95.2041, 95.2423, 73.4496, 72.6733, 49.6945, 45.6278, 75.1566, 50.3891, 69.4045, 84.3409]\n",
            "Epoch 0.45 >> (per-task accuracy): 74.68\n",
            "Epoch 0.45 >> (class accuracy): [95.4082, 96.2115, 71.7054, 73.7624, 73.2179, 55.4933, 75.261, 56.7121, 71.5606, 73.439]\n",
            "Epoch 0.5 >> (per-task accuracy): 75.2\n",
            "Epoch 0.5 >> (class accuracy): [94.5918, 95.859, 74.1279, 72.3762, 70.8758, 59.3049, 79.1232, 51.5564, 75.6674, 75.223]\n",
            "Epoch 0.55 >> (per-task accuracy): 76.52\n",
            "Epoch 0.55 >> (class accuracy): [94.1837, 96.0352, 76.5504, 71.9802, 65.0713, 62.6682, 83.1942, 60.3113, 75.6674, 76.3132]\n",
            "Epoch 0.6 >> (per-task accuracy): 77.28\n",
            "Epoch 0.6 >> (class accuracy): [93.5714, 95.6828, 74.8062, 75.7426, 72.4033, 61.5471, 86.2213, 59.9222, 77.6181, 72.3489]\n",
            "Epoch 0.65 >> (per-task accuracy): 77.33\n",
            "Epoch 0.65 >> (class accuracy): [92.2449, 94.8899, 74.5155, 74.1584, 66.1914, 63.0045, 87.6827, 60.6031, 80.9035, 76.4123]\n",
            "Epoch 0.7 >> (per-task accuracy): 78.07\n",
            "Epoch 0.7 >> (class accuracy): [92.8571, 94.3612, 76.0659, 72.8713, 73.7271, 66.0314, 85.8038, 65.4669, 80.6982, 70.4658]\n",
            "Epoch 0.75 >> (per-task accuracy): 78.9\n",
            "Epoch 0.75 >> (class accuracy): [91.1224, 93.9207, 74.8062, 74.8515, 73.4216, 68.3857, 89.7704, 63.716, 81.0062, 76.115]\n",
            "Epoch 0.8 >> (per-task accuracy): 79.6\n",
            "Epoch 0.8 >> (class accuracy): [91.1224, 94.0088, 75.5814, 77.2277, 71.7923, 67.713, 88.8309, 68.3852, 81.1088, 77.998]\n",
            "Epoch 0.85 >> (per-task accuracy): 79.85\n",
            "Epoch 0.85 >> (class accuracy): [92.0408, 93.6564, 75.4845, 74.2574, 72.6069, 69.2825, 88.4134, 69.8444, 82.3409, 78.5927]\n",
            "Epoch 0.9 >> (per-task accuracy): 80.55\n",
            "Epoch 0.9 >> (class accuracy): [91.3265, 93.7445, 75.1938, 77.4257, 76.8839, 70.6278, 87.8914, 72.179, 80.5955, 77.7007]\n",
            "Epoch 0.95 >> (per-task accuracy): 81.08\n",
            "Epoch 0.95 >> (class accuracy): [90.6122, 94.0969, 77.0349, 74.7525, 76.3747, 71.9731, 89.0397, 74.4163, 81.8275, 78.7909]\n",
            "Epoch 1.0 >> (per-task accuracy): 82.0\n",
            "Epoch 1.0 >> (class accuracy): [92.9592, 93.9207, 79.845, 75.9406, 76.4766, 72.7578, 89.4572, 76.1673, 82.0329, 78.5927]\n",
            "OCS >> Task 1: {'accuracy': 27.67, 'per_class_accuracy': [35.5102, 6.3436, 1.7442, 37.3267, 30.3462, 6.1659, 71.7119, 34.4358, 24.9487, 31.219], 'loss': 2.2442150802612306}\n",
            "OCS >> Task 2: {'accuracy': 31.66, 'per_class_accuracy': [44.0816, 17.7974, 3.3915, 39.802, 37.4745, 6.5022, 70.2505, 33.9494, 27.8234, 37.2646], 'loss': 2.1033600902557374}\n",
            "OCS >> Task 3: {'accuracy': 37.85, 'per_class_accuracy': [52.1429, 45.1101, 5.6202, 51.2871, 40.7332, 7.8475, 65.3445, 47.9572, 19.0965, 40.7334], 'loss': 1.8762513103485108}\n",
            "OCS >> Task 4: {'accuracy': 40.72, 'per_class_accuracy': [63.2653, 61.8502, 4.0698, 46.3366, 36.8635, 6.7265, 68.476, 44.6498, 20.7392, 49.6531], 'loss': 1.8514663984298707}\n",
            "OCS >> Task 5: {'accuracy': 43.47, 'per_class_accuracy': [77.7551, 80.793, 3.876, 46.8317, 36.0489, 10.0897, 62.2129, 46.3035, 12.8337, 50.9415], 'loss': 1.7756736696243287}\n",
            "OCS >> Task 6: {'accuracy': 44.79, 'per_class_accuracy': [82.9592, 93.304, 5.6202, 47.6238, 24.9491, 15.0224, 64.6138, 37.1595, 7.1869, 61.2488], 'loss': 1.7565281702041626}\n",
            "OCS >> Task 7: {'accuracy': 49.77, 'per_class_accuracy': [88.3673, 96.5639, 9.0116, 59.0099, 22.9124, 21.5247, 61.691, 59.8249, 8.2136, 61.7443], 'loss': 1.61376957321167}\n",
            "OCS >> Task 8: {'accuracy': 55.5, 'per_class_accuracy': [91.9388, 97.0925, 15.407, 61.6832, 30.3462, 39.3498, 74.2171, 59.0467, 24.0246, 55.8969], 'loss': 1.4095889282226564}\n",
            "OCS >> Task 9: {'accuracy': 62.99, 'per_class_accuracy': [90.5102, 97.1806, 30.4264, 64.1584, 36.7617, 52.5785, 77.6618, 67.3152, 41.0678, 67.4926], 'loss': 1.163516002178192}\n",
            "OCS >> Task 10: {'accuracy': 70.45, 'per_class_accuracy': [92.3469, 94.185, 53.6822, 68.9109, 46.4358, 59.5291, 77.2443, 72.9572, 60.3696, 74.9257], 'loss': 0.980725795841217}\n",
            "OCS >> Task 11: {'accuracy': 78.3, 'per_class_accuracy': [93.3673, 95.4185, 71.2209, 74.7525, 65.7841, 65.4709, 85.4906, 73.8327, 74.4353, 80.1784], 'loss': 0.7908779601097107}\n",
            "OCS >> Task 12: {'accuracy': 82.0, 'per_class_accuracy': [92.9592, 93.9207, 79.845, 75.9406, 76.4766, 72.7578, 89.4572, 76.1673, 82.0329, 78.5927], 'loss': 0.7058613376617432}\n",
            "OCS >> (average accuracy): 52.0975\n",
            "OCS >> (Forgetting): 0.469609090909091\n",
            "Maximum per-task accuracies: [96.34]\n",
            "\n",
            "---- Task 13 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 10.11\n",
            "Epoch 0.05 >> (class accuracy): [0.0, 0.0, 0.6783, 0.0, 100.0, 2.4664, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.1 >> (per-task accuracy): 9.88\n",
            "Epoch 0.1 >> (class accuracy): [0.6122, 0.0, 0.0, 0.0, 100.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.15 >> (per-task accuracy): 15.18\n",
            "Epoch 0.15 >> (class accuracy): [52.3469, 0.0, 0.0, 0.0, 11.4053, 0.0, 0.0, 56.323, 0.0, 31.1199]\n",
            "Epoch 0.2 >> (per-task accuracy): 20.22\n",
            "Epoch 0.2 >> (class accuracy): [74.5918, 0.0, 0.0, 7.7228, 36.0489, 0.0, 29.0188, 45.6226, 0.0, 11.1001]\n",
            "Epoch 0.25 >> (per-task accuracy): 41.28\n",
            "Epoch 0.25 >> (class accuracy): [89.4898, 16.4758, 0.0969, 66.6337, 89.613, 33.4081, 38.2046, 57.1984, 24.7433, 1.6848]\n",
            "Epoch 0.3 >> (per-task accuracy): 59.65\n",
            "Epoch 0.3 >> (class accuracy): [93.3673, 90.3965, 39.438, 62.1782, 69.9593, 17.2646, 81.1065, 72.0817, 44.5585, 19.4252]\n",
            "Epoch 0.35 >> (per-task accuracy): 63.45\n",
            "Epoch 0.35 >> (class accuracy): [93.2653, 89.6035, 52.2287, 68.3168, 60.5906, 51.1211, 82.6722, 59.144, 39.9384, 34.1923]\n",
            "Epoch 0.4 >> (per-task accuracy): 67.41\n",
            "Epoch 0.4 >> (class accuracy): [91.9388, 91.63, 56.2016, 68.7129, 68.6354, 41.4798, 84.7599, 68.0934, 55.0308, 43.0129]\n",
            "Epoch 0.45 >> (per-task accuracy): 72.1\n",
            "Epoch 0.45 >> (class accuracy): [93.1633, 92.1586, 63.9535, 69.901, 68.6354, 43.4978, 82.5678, 74.5136, 64.8871, 62.8345]\n",
            "Epoch 0.5 >> (per-task accuracy): 71.67\n",
            "Epoch 0.5 >> (class accuracy): [91.2245, 93.1278, 64.2442, 68.8119, 58.554, 40.2466, 81.3152, 74.5136, 70.0205, 69.0783]\n",
            "Epoch 0.55 >> (per-task accuracy): 73.62\n",
            "Epoch 0.55 >> (class accuracy): [90.6122, 94.978, 69.186, 71.3861, 69.0428, 41.5919, 77.8706, 73.5409, 73.306, 68.9792]\n",
            "Epoch 0.6 >> (per-task accuracy): 75.32\n",
            "Epoch 0.6 >> (class accuracy): [90.6122, 94.7137, 67.5388, 74.4554, 65.7841, 48.5426, 82.2547, 73.249, 77.2074, 74.1328]\n",
            "Epoch 0.65 >> (per-task accuracy): 76.77\n",
            "Epoch 0.65 >> (class accuracy): [90.9184, 95.5947, 72.8682, 75.6436, 67.2098, 56.278, 83.6117, 74.1245, 73.614, 73.6373]\n",
            "Epoch 0.7 >> (per-task accuracy): 77.68\n",
            "Epoch 0.7 >> (class accuracy): [90.4082, 95.2423, 72.4806, 78.7129, 71.4868, 53.8117, 85.1775, 73.93, 76.386, 74.9257]\n",
            "Epoch 0.75 >> (per-task accuracy): 77.87\n",
            "Epoch 0.75 >> (class accuracy): [91.4286, 94.6256, 73.3527, 80.8911, 71.1813, 50.4484, 84.7599, 76.8482, 76.8994, 73.6373]\n",
            "Epoch 0.8 >> (per-task accuracy): 78.71\n",
            "Epoch 0.8 >> (class accuracy): [91.6327, 94.8018, 74.2248, 78.9109, 68.7373, 56.9507, 84.6555, 76.751, 80.8008, 75.7185]\n",
            "Epoch 0.85 >> (per-task accuracy): 79.08\n",
            "Epoch 0.85 >> (class accuracy): [91.4286, 93.2159, 73.2558, 81.4851, 72.5051, 59.7534, 83.9248, 76.3619, 81.0062, 74.5292]\n",
            "Epoch 0.9 >> (per-task accuracy): 79.81\n",
            "Epoch 0.9 >> (class accuracy): [92.449, 94.978, 74.1279, 77.8218, 73.9308, 60.426, 86.5344, 76.5564, 80.4928, 77.4034]\n",
            "Epoch 0.95 >> (per-task accuracy): 80.7\n",
            "Epoch 0.95 >> (class accuracy): [92.9592, 95.5066, 75.5814, 80.6931, 77.5967, 62.8924, 87.5783, 77.1401, 79.2608, 74.6283]\n",
            "Epoch 1.0 >> (per-task accuracy): 81.1\n",
            "Epoch 1.0 >> (class accuracy): [92.8571, 95.5947, 76.4535, 79.703, 77.1894, 63.565, 85.595, 77.821, 81.0062, 77.998]\n",
            "OCS >> Task 1: {'accuracy': 23.88, 'per_class_accuracy': [30.7143, 3.2599, 2.3256, 34.6535, 29.5316, 4.9327, 57.0981, 19.4553, 25.3593, 34.4896], 'loss': 2.336793008041382}\n",
            "OCS >> Task 2: {'accuracy': 28.04, 'per_class_accuracy': [36.5306, 12.4229, 3.0039, 40.6931, 36.6599, 5.6054, 61.4823, 19.2607, 24.3326, 42.5173], 'loss': 2.208286792755127}\n",
            "OCS >> Task 3: {'accuracy': 32.21, 'per_class_accuracy': [48.4694, 33.3921, 3.7791, 52.4752, 37.169, 6.1659, 60.4384, 24.1245, 18.3778, 36.8682], 'loss': 2.033316694450378}\n",
            "OCS >> Task 4: {'accuracy': 36.48, 'per_class_accuracy': [54.3878, 51.9824, 2.3256, 52.8713, 37.4745, 8.0717, 62.8392, 26.1673, 18.8912, 46.779], 'loss': 1.9641458709716797}\n",
            "OCS >> Task 5: {'accuracy': 39.46, 'per_class_accuracy': [67.1429, 75.0661, 2.6163, 51.7822, 40.835, 10.8744, 55.5324, 24.8054, 11.7043, 48.2656], 'loss': 1.9155064739227294}\n",
            "OCS >> Task 6: {'accuracy': 41.67, 'per_class_accuracy': [74.3878, 91.0132, 2.8101, 58.5149, 26.3747, 13.2287, 51.8789, 19.6498, 3.3881, 66.997], 'loss': 1.9039946268081664}\n",
            "OCS >> Task 7: {'accuracy': 46.17, 'per_class_accuracy': [83.8776, 96.8282, 5.2326, 63.5644, 26.4766, 18.0493, 44.572, 47.2763, 2.8747, 63.2309], 'loss': 1.7985376533508302}\n",
            "OCS >> Task 8: {'accuracy': 50.0, 'per_class_accuracy': [90.3061, 98.2379, 8.0426, 66.4356, 32.5866, 30.6054, 58.977, 43.9689, 6.6735, 56.5907], 'loss': 1.6301232082366943}\n",
            "OCS >> Task 9: {'accuracy': 53.84, 'per_class_accuracy': [87.0408, 98.2379, 14.1473, 67.7228, 30.6517, 41.3677, 65.8664, 53.8911, 15.9138, 57.0862], 'loss': 1.4219802082061768}\n",
            "OCS >> Task 10: {'accuracy': 60.39, 'per_class_accuracy': [86.9388, 97.7093, 32.2674, 71.1881, 38.1874, 50.5605, 64.1962, 55.9339, 34.8049, 66.5015], 'loss': 1.2202528512954711}\n",
            "OCS >> Task 11: {'accuracy': 70.48, 'per_class_accuracy': [88.4694, 98.0617, 50.3876, 74.7525, 62.1181, 60.9865, 75.5741, 63.2296, 55.3388, 71.9524], 'loss': 0.9603013409614563}\n",
            "OCS >> Task 12: {'accuracy': 76.44, 'per_class_accuracy': [90.0, 98.1498, 64.5349, 74.2574, 80.8554, 67.1525, 79.0188, 66.9261, 72.5873, 68.0872], 'loss': 0.8163129072189331}\n",
            "OCS >> Task 13: {'accuracy': 81.1, 'per_class_accuracy': [92.8571, 95.5947, 76.4535, 79.703, 77.1894, 63.565, 85.595, 77.821, 81.0062, 77.998], 'loss': 0.7386641023635864}\n",
            "OCS >> (average accuracy): 49.24307692307692\n",
            "OCS >> (Forgetting): 0.49751666666666666\n",
            "Maximum per-task accuracies: [96.34]\n",
            "\n",
            "---- Task 14 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 9.9\n",
            "Epoch 0.05 >> (class accuracy): [1.5306, 0.0, 0.0, 4.2574, 0.611, 0.0, 0.0, 82.2957, 8.2136, 0.0]\n",
            "Epoch 0.1 >> (per-task accuracy): 19.35\n",
            "Epoch 0.1 >> (class accuracy): [95.4082, 0.0, 0.0, 45.1485, 0.0, 0.0, 0.0, 50.0, 3.0801, 0.0]\n",
            "Epoch 0.15 >> (per-task accuracy): 15.71\n",
            "Epoch 0.15 >> (class accuracy): [100.0, 0.0, 0.0, 29.0099, 0.0, 0.0, 0.0, 28.6965, 0.308, 0.0]\n",
            "Epoch 0.2 >> (per-task accuracy): 14.66\n",
            "Epoch 0.2 >> (class accuracy): [100.0, 8.37, 0.1938, 15.4455, 0.0, 0.0, 0.0, 21.2062, 0.1027, 1.3875]\n",
            "Epoch 0.25 >> (per-task accuracy): 18.96\n",
            "Epoch 0.25 >> (class accuracy): [100.0, 37.2687, 1.0659, 11.5842, 0.0, 0.0, 0.0, 26.751, 0.1027, 8.8206]\n",
            "Epoch 0.3 >> (per-task accuracy): 26.81\n",
            "Epoch 0.3 >> (class accuracy): [100.0, 67.7533, 6.2016, 13.9604, 0.4073, 0.0, 0.0, 41.8288, 1.7454, 27.3538]\n",
            "Epoch 0.35 >> (per-task accuracy): 35.04\n",
            "Epoch 0.35 >> (class accuracy): [99.898, 74.2731, 24.031, 21.3861, 2.444, 0.0, 0.0, 55.0584, 17.1458, 45.6888]\n",
            "Epoch 0.4 >> (per-task accuracy): 43.18\n",
            "Epoch 0.4 >> (class accuracy): [99.4898, 78.0617, 44.0891, 29.901, 5.3971, 0.0, 2.6096, 57.0039, 41.9918, 62.1407]\n",
            "Epoch 0.45 >> (per-task accuracy): 50.58\n",
            "Epoch 0.45 >> (class accuracy): [98.8776, 81.2335, 63.469, 38.9109, 15.0713, 0.0, 10.4384, 60.7977, 54.9281, 70.4658]\n",
            "Epoch 0.5 >> (per-task accuracy): 55.88\n",
            "Epoch 0.5 >> (class accuracy): [98.3673, 86.0793, 74.5155, 43.7624, 31.1609, 0.0, 21.7119, 54.0856, 56.3655, 80.9713]\n",
            "Epoch 0.55 >> (per-task accuracy): 60.76\n",
            "Epoch 0.55 >> (class accuracy): [97.7551, 90.1322, 79.9419, 49.2079, 47.9633, 0.3363, 36.2213, 49.3191, 61.7043, 83.6472]\n",
            "Epoch 0.6 >> (per-task accuracy): 64.65\n",
            "Epoch 0.6 >> (class accuracy): [96.6327, 93.1278, 82.1705, 55.1485, 60.6925, 7.0628, 45.7203, 48.6381, 64.6817, 82.1606]\n",
            "Epoch 0.65 >> (per-task accuracy): 68.48\n",
            "Epoch 0.65 >> (class accuracy): [96.0204, 94.4493, 82.9457, 60.7921, 66.4969, 18.3857, 58.4551, 51.2646, 68.4805, 78.6918]\n",
            "Epoch 0.7 >> (per-task accuracy): 71.2\n",
            "Epoch 0.7 >> (class accuracy): [95.9184, 95.859, 81.686, 68.4158, 71.9959, 28.6996, 67.8497, 54.9611, 69.4045, 69.7721]\n",
            "Epoch 0.75 >> (per-task accuracy): 73.19\n",
            "Epoch 0.75 >> (class accuracy): [95.4082, 96.2996, 81.8798, 72.9703, 73.0143, 36.3229, 74.9478, 61.3813, 69.0965, 64.0238]\n",
            "Epoch 0.8 >> (per-task accuracy): 74.0\n",
            "Epoch 0.8 >> (class accuracy): [94.898, 96.3877, 81.7829, 76.7327, 75.7637, 41.5919, 81.0021, 62.5486, 69.5072, 54.113]\n",
            "Epoch 0.85 >> (per-task accuracy): 75.1\n",
            "Epoch 0.85 >> (class accuracy): [94.3878, 96.2996, 80.2326, 78.6139, 76.0692, 47.3094, 82.9854, 64.1051, 72.4846, 53.6174]\n",
            "Epoch 0.9 >> (per-task accuracy): 75.66\n",
            "Epoch 0.9 >> (class accuracy): [93.4694, 96.3877, 78.9729, 78.1188, 77.0876, 55.157, 85.4906, 64.2996, 73.1006, 50.6442]\n",
            "Epoch 0.95 >> (per-task accuracy): 76.2\n",
            "Epoch 0.95 >> (class accuracy): [93.7755, 96.2115, 78.3915, 78.6139, 77.1894, 56.0538, 86.8476, 63.6187, 75.8727, 51.8335]\n",
            "Epoch 1.0 >> (per-task accuracy): 76.75\n",
            "Epoch 1.0 >> (class accuracy): [93.1633, 96.2115, 77.8101, 80.8911, 74.1344, 56.9507, 86.2213, 65.4669, 75.462, 57.4827]\n",
            "OCS >> Task 1: {'accuracy': 21.14, 'per_class_accuracy': [26.3265, 8.37, 1.1628, 28.1188, 33.2994, 0.4484, 40.9186, 22.2763, 22.5873, 29.0387], 'loss': 2.1373560386657715}\n",
            "OCS >> Task 2: {'accuracy': 27.0, 'per_class_accuracy': [36.5306, 25.1101, 1.938, 33.8614, 29.8371, 0.2242, 51.7745, 32.0039, 19.0965, 38.553], 'loss': 2.037619927215576}\n",
            "OCS >> Task 3: {'accuracy': 31.54, 'per_class_accuracy': [43.1633, 46.9604, 3.6822, 43.7624, 37.6782, 1.009, 54.9061, 31.2257, 7.0842, 41.9227], 'loss': 1.9359097087860107}\n",
            "OCS >> Task 4: {'accuracy': 34.22, 'per_class_accuracy': [51.5306, 60.8811, 3.0039, 49.604, 37.6782, 0.3363, 58.142, 20.9144, 4.7228, 49.8513], 'loss': 1.918214512825012}\n",
            "OCS >> Task 5: {'accuracy': 38.22, 'per_class_accuracy': [72.2449, 83.9648, 2.0349, 56.9307, 42.8717, 0.3363, 54.9061, 11.3813, 1.4374, 47.9683], 'loss': 1.8835169401168823}\n",
            "OCS >> Task 6: {'accuracy': 39.18, 'per_class_accuracy': [74.6939, 96.0352, 1.7442, 63.7624, 38.0855, 1.4574, 49.7912, 4.572, 0.8214, 51.0406], 'loss': 1.9202333045959472}\n",
            "OCS >> Task 7: {'accuracy': 41.39, 'per_class_accuracy': [84.6939, 97.7093, 3.1977, 70.0, 35.0305, 3.139, 45.3027, 4.0856, 1.0267, 59.663], 'loss': 1.8739649616241456}\n",
            "OCS >> Task 8: {'accuracy': 43.72, 'per_class_accuracy': [90.3061, 97.6211, 5.6202, 75.4455, 37.3727, 7.3991, 53.9666, 3.4047, 2.4641, 54.5094], 'loss': 1.815516358757019}\n",
            "OCS >> Task 9: {'accuracy': 47.95, 'per_class_accuracy': [91.7347, 98.6784, 9.9806, 81.7822, 43.1772, 14.0135, 60.5428, 7.6848, 7.3922, 56.1943], 'loss': 1.668437933921814}\n",
            "OCS >> Task 10: {'accuracy': 54.01, 'per_class_accuracy': [92.7551, 98.1498, 21.7054, 83.5644, 51.222, 25.0, 63.2568, 23.0545, 14.1684, 59.7621], 'loss': 1.516028951072693}\n",
            "OCS >> Task 11: {'accuracy': 62.76, 'per_class_accuracy': [93.1633, 98.0617, 39.8256, 86.8317, 67.6171, 38.6771, 71.7119, 36.5759, 32.1355, 57.1853], 'loss': 1.307899989128113}\n",
            "OCS >> Task 12: {'accuracy': 69.57, 'per_class_accuracy': [94.7959, 98.7665, 57.9457, 85.8416, 78.3096, 51.5695, 78.1837, 42.8988, 55.1335, 48.1665], 'loss': 1.1357006669998169}\n",
            "OCS >> Task 13: {'accuracy': 75.2, 'per_class_accuracy': [93.5714, 97.533, 74.031, 84.4554, 80.4481, 55.8296, 83.5073, 58.1712, 67.0431, 53.5183], 'loss': 1.0310248703002929}\n",
            "OCS >> Task 14: {'accuracy': 76.75, 'per_class_accuracy': [93.1633, 96.2115, 77.8101, 80.8911, 74.1344, 56.9507, 86.2213, 65.4669, 75.462, 57.4827], 'loss': 0.991337545967102}\n",
            "OCS >> (average accuracy): 47.332142857142856\n",
            "OCS >> (Forgetting): 0.5127076923076924\n",
            "Maximum per-task accuracies: [96.34]\n",
            "\n",
            "---- Task 15 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 10.22\n",
            "Epoch 0.05 >> (class accuracy): [7.9592, 0.0, 0.0, 2.8713, 0.8147, 0.0, 8.4551, 40.9533, 41.5811, 0.0]\n",
            "Epoch 0.1 >> (per-task accuracy): 15.86\n",
            "Epoch 0.1 >> (class accuracy): [80.102, 0.0, 0.0, 48.8119, 0.611, 0.0, 3.1315, 16.6342, 10.3696, 0.0]\n",
            "Epoch 0.15 >> (per-task accuracy): 13.68\n",
            "Epoch 0.15 >> (class accuracy): [98.8776, 0.0, 0.0, 35.0495, 0.2037, 0.0, 0.0, 3.4047, 0.8214, 0.0]\n",
            "Epoch 0.2 >> (per-task accuracy): 12.73\n",
            "Epoch 0.2 >> (class accuracy): [99.898, 1.8502, 0.7752, 24.6535, 0.1018, 0.0, 0.0, 0.7782, 0.7187, 0.0]\n",
            "Epoch 0.25 >> (per-task accuracy): 13.62\n",
            "Epoch 0.25 >> (class accuracy): [99.898, 13.4802, 1.2597, 20.495, 0.2037, 0.0, 0.0, 0.5837, 0.2053, 0.0]\n",
            "Epoch 0.3 >> (per-task accuracy): 17.11\n",
            "Epoch 0.3 >> (class accuracy): [99.898, 41.1454, 2.6163, 18.7129, 1.5275, 0.0, 0.0, 2.9183, 0.4107, 0.0]\n",
            "Epoch 0.35 >> (per-task accuracy): 20.08\n",
            "Epoch 0.35 >> (class accuracy): [99.898, 55.7709, 4.5543, 19.2079, 7.6375, 0.0, 0.0, 6.8093, 0.616, 0.3964]\n",
            "Epoch 0.4 >> (per-task accuracy): 25.44\n",
            "Epoch 0.4 >> (class accuracy): [99.898, 70.837, 12.5, 20.396, 20.2648, 0.0, 0.0, 16.4397, 2.3614, 3.4688]\n",
            "Epoch 0.45 >> (per-task accuracy): 35.52\n",
            "Epoch 0.45 >> (class accuracy): [99.898, 85.7269, 26.0659, 23.4653, 49.0835, 0.0, 0.0, 33.2685, 11.499, 15.6591]\n",
            "Epoch 0.5 >> (per-task accuracy): 43.79\n",
            "Epoch 0.5 >> (class accuracy): [99.898, 91.5419, 46.3178, 29.1089, 59.3686, 0.0, 0.8351, 32.6848, 26.2834, 40.2379]\n",
            "Epoch 0.55 >> (per-task accuracy): 47.76\n",
            "Epoch 0.55 >> (class accuracy): [99.5918, 93.2159, 61.6279, 36.3366, 75.6619, 0.0, 3.2359, 27.6265, 47.0226, 22.1011]\n",
            "Epoch 0.6 >> (per-task accuracy): 52.67\n",
            "Epoch 0.6 >> (class accuracy): [99.0816, 96.2996, 68.4109, 40.0, 78.7169, 0.0, 13.048, 37.2568, 56.4682, 25.9663]\n",
            "Epoch 0.65 >> (per-task accuracy): 54.37\n",
            "Epoch 0.65 >> (class accuracy): [98.6735, 97.533, 75.4845, 47.9208, 85.2342, 0.4484, 26.4092, 26.2646, 51.1294, 23.5877]\n",
            "Epoch 0.7 >> (per-task accuracy): 56.08\n",
            "Epoch 0.7 >> (class accuracy): [98.0612, 98.5022, 80.4264, 43.7624, 86.558, 2.3543, 35.8038, 24.8054, 54.4148, 25.5699]\n",
            "Epoch 0.75 >> (per-task accuracy): 57.21\n",
            "Epoch 0.75 >> (class accuracy): [97.449, 98.6784, 82.5581, 44.9505, 89.9185, 8.8565, 41.5449, 26.8482, 58.4189, 13.3796]\n",
            "Epoch 0.8 >> (per-task accuracy): 60.48\n",
            "Epoch 0.8 >> (class accuracy): [97.1429, 98.7665, 83.8178, 52.3762, 88.9002, 24.4395, 45.3027, 29.6693, 58.2136, 18.2359]\n",
            "Epoch 0.85 >> (per-task accuracy): 62.85\n",
            "Epoch 0.85 >> (class accuracy): [97.0408, 98.7665, 83.5271, 55.3465, 90.1222, 27.5785, 58.0376, 30.2529, 64.6817, 16.2537]\n",
            "Epoch 0.9 >> (per-task accuracy): 64.9\n",
            "Epoch 0.9 >> (class accuracy): [96.3265, 98.8546, 84.0116, 59.703, 88.5947, 31.3901, 64.8225, 31.5175, 68.5832, 18.9296]\n",
            "Epoch 0.95 >> (per-task accuracy): 66.4\n",
            "Epoch 0.95 >> (class accuracy): [96.1224, 98.7665, 83.624, 64.6535, 90.0204, 36.435, 70.8768, 39.2023, 69.0965, 9.6135]\n",
            "Epoch 1.0 >> (per-task accuracy): 69.13\n",
            "Epoch 1.0 >> (class accuracy): [95.8163, 98.6784, 82.655, 65.7426, 88.0855, 45.5157, 75.7829, 47.6654, 71.6632, 15.0644]\n",
            "OCS >> Task 1: {'accuracy': 20.98, 'per_class_accuracy': [42.6531, 19.5595, 2.3256, 31.0891, 46.945, 0.0, 36.3257, 1.6537, 23.2033, 6.7393], 'loss': 2.0603829917907714}\n",
            "OCS >> Task 2: {'accuracy': 24.89, 'per_class_accuracy': [53.9796, 24.6696, 2.907, 36.4356, 41.6497, 0.1121, 49.0605, 2.2374, 16.1191, 22.002], 'loss': 2.0061315788269045}\n",
            "OCS >> Task 3: {'accuracy': 28.54, 'per_class_accuracy': [57.551, 32.8634, 4.3605, 50.495, 47.4542, 0.3363, 54.2797, 1.9455, 5.9548, 29.2369], 'loss': 1.9665157222747802}\n",
            "OCS >> Task 4: {'accuracy': 30.96, 'per_class_accuracy': [63.4694, 31.3656, 3.9729, 58.7129, 45.723, 0.3363, 59.8121, 1.1673, 1.4374, 42.9138], 'loss': 1.9645170715332032}\n",
            "OCS >> Task 5: {'accuracy': 31.97, 'per_class_accuracy': [80.4082, 29.163, 2.6163, 65.6436, 44.501, 0.1121, 55.5324, 0.1946, 0.4107, 40.8325], 'loss': 1.972058111190796}\n",
            "OCS >> Task 6: {'accuracy': 33.18, 'per_class_accuracy': [82.8571, 39.1189, 1.3566, 71.5842, 41.9552, 0.7848, 50.0, 0.1946, 0.308, 41.8236], 'loss': 2.014647511672974}\n",
            "OCS >> Task 7: {'accuracy': 33.34, 'per_class_accuracy': [89.898, 34.978, 1.4535, 74.4554, 40.224, 2.2422, 44.9896, 0.2918, 0.5133, 43.112], 'loss': 2.021925179672241}\n",
            "OCS >> Task 8: {'accuracy': 33.6, 'per_class_accuracy': [92.2449, 38.9427, 2.0349, 75.2475, 42.4644, 4.3722, 45.5115, 0.1946, 0.924, 32.7056], 'loss': 2.017535449790955}\n",
            "OCS >> Task 9: {'accuracy': 37.34, 'per_class_accuracy': [92.8571, 63.9648, 5.5233, 74.6535, 43.9919, 10.7623, 47.5992, 1.5564, 2.9774, 25.5699], 'loss': 1.9569164751052857}\n",
            "OCS >> Task 10: {'accuracy': 42.46, 'per_class_accuracy': [94.3878, 80.7048, 12.0155, 71.1881, 57.4338, 23.5426, 52.1921, 3.1128, 6.9815, 18.6323], 'loss': 1.8630660799026488}\n",
            "OCS >> Task 11: {'accuracy': 49.44, 'per_class_accuracy': [95.2041, 91.5419, 27.4225, 73.7624, 70.1629, 34.7534, 59.9165, 7.0039, 16.4271, 13.776], 'loss': 1.7224433652877809}\n",
            "OCS >> Task 12: {'accuracy': 57.18, 'per_class_accuracy': [95.7143, 98.1498, 47.093, 72.1782, 78.0041, 46.9731, 65.0313, 18.1907, 37.7823, 8.7215], 'loss': 1.5754237884521485}\n",
            "OCS >> Task 13: {'accuracy': 64.24, 'per_class_accuracy': [96.1224, 97.4449, 71.3178, 71.4851, 86.3544, 52.6906, 73.5908, 29.572, 53.2854, 7.1358], 'loss': 1.4361407577514649}\n",
            "OCS >> Task 14: {'accuracy': 68.09, 'per_class_accuracy': [96.4286, 98.6784, 82.1705, 66.8317, 89.9185, 50.7848, 76.4092, 39.4942, 66.8378, 9.5144], 'loss': 1.360918367767334}\n",
            "OCS >> Task 15: {'accuracy': 69.13, 'per_class_accuracy': [95.8163, 98.6784, 82.655, 65.7426, 88.0855, 45.5157, 75.7829, 47.6654, 71.6632, 15.0644], 'loss': 1.3288478084564208}\n",
            "OCS >> (average accuracy): 41.68933333333334\n",
            "OCS >> (Forgetting): 0.5661071428571428\n",
            "Maximum per-task accuracies: [96.34]\n",
            "\n",
            "---- Task 16 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 10.63\n",
            "Epoch 0.05 >> (class accuracy): [0.102, 0.0, 0.0, 0.198, 12.7291, 0.0, 3.1315, 11.3813, 80.9035, 0.0]\n",
            "Epoch 0.1 >> (per-task accuracy): 18.52\n",
            "Epoch 0.1 >> (class accuracy): [86.6327, 0.0, 3.6822, 14.1584, 21.9959, 0.0, 1.7745, 21.1089, 38.193, 0.0]\n",
            "Epoch 0.15 >> (per-task accuracy): 17.65\n",
            "Epoch 0.15 >> (class accuracy): [93.9796, 0.0, 2.0349, 45.0495, 18.9409, 0.0, 0.0, 1.07, 17.5565, 0.0]\n",
            "Epoch 0.2 >> (per-task accuracy): 18.2\n",
            "Epoch 0.2 >> (class accuracy): [98.5714, 4.8458, 1.1628, 48.0198, 22.3014, 0.0, 0.0, 1.9455, 6.4682, 0.0]\n",
            "Epoch 0.25 >> (per-task accuracy): 18.38\n",
            "Epoch 0.25 >> (class accuracy): [99.7959, 14.2731, 0.2907, 28.5149, 29.3279, 0.0, 0.0, 3.6965, 8.3162, 0.0]\n",
            "Epoch 0.3 >> (per-task accuracy): 21.76\n",
            "Epoch 0.3 >> (class accuracy): [99.898, 36.1233, 2.5194, 34.7525, 33.0957, 0.0, 0.0, 8.2685, 0.0, 0.0]\n",
            "Epoch 0.35 >> (per-task accuracy): 22.71\n",
            "Epoch 0.35 >> (class accuracy): [99.898, 41.5859, 3.1977, 33.9604, 41.0387, 0.0, 0.0, 3.3074, 0.7187, 0.0]\n",
            "Epoch 0.4 >> (per-task accuracy): 26.84\n",
            "Epoch 0.4 >> (class accuracy): [99.898, 58.8546, 7.9457, 31.7822, 51.1202, 0.0, 0.0, 9.5331, 3.4908, 0.0]\n",
            "Epoch 0.45 >> (per-task accuracy): 28.66\n",
            "Epoch 0.45 >> (class accuracy): [100.0, 77.8855, 13.8566, 18.4158, 65.1731, 0.0, 0.0, 1.5564, 1.7454, 0.0]\n",
            "Epoch 0.5 >> (per-task accuracy): 30.89\n",
            "Epoch 0.5 >> (class accuracy): [100.0, 86.5198, 19.4767, 11.1881, 71.4868, 0.0, 0.0, 5.2529, 5.8522, 0.0]\n",
            "Epoch 0.55 >> (per-task accuracy): 40.89\n",
            "Epoch 0.55 >> (class accuracy): [99.898, 86.8722, 43.7016, 24.6535, 70.4684, 0.0, 0.1044, 25.6809, 47.9466, 0.0]\n",
            "Epoch 0.6 >> (per-task accuracy): 43.09\n",
            "Epoch 0.6 >> (class accuracy): [99.7959, 97.3568, 46.5116, 22.7723, 83.5031, 0.0, 1.6701, 34.144, 33.7782, 0.0]\n",
            "Epoch 0.65 >> (per-task accuracy): 45.04\n",
            "Epoch 0.65 >> (class accuracy): [99.4898, 98.8546, 58.2364, 24.6535, 83.4012, 0.0, 3.3403, 29.6693, 40.4517, 0.6938]\n",
            "Epoch 0.7 >> (per-task accuracy): 51.59\n",
            "Epoch 0.7 >> (class accuracy): [98.7755, 97.7974, 66.376, 41.1881, 91.446, 0.0, 20.0418, 35.3113, 54.1068, 0.0]\n",
            "Epoch 0.75 >> (per-task accuracy): 53.9\n",
            "Epoch 0.75 >> (class accuracy): [98.6735, 98.8546, 69.3798, 39.505, 90.835, 0.0, 14.1962, 61.8677, 53.5934, 0.0]\n",
            "Epoch 0.8 >> (per-task accuracy): 53.25\n",
            "Epoch 0.8 >> (class accuracy): [98.2653, 98.5903, 75.8721, 41.2871, 93.6864, 0.4484, 14.7182, 44.9416, 52.9774, 0.0]\n",
            "Epoch 0.85 >> (per-task accuracy): 55.01\n",
            "Epoch 0.85 >> (class accuracy): [97.9592, 98.5022, 78.1008, 38.9109, 94.0937, 0.5605, 26.618, 41.9261, 62.5257, 0.0]\n",
            "Epoch 0.9 >> (per-task accuracy): 59.46\n",
            "Epoch 0.9 >> (class accuracy): [96.5306, 97.7093, 77.7132, 55.2475, 92.668, 0.7848, 38.6221, 53.5992, 70.7392, 0.3964]\n",
            "Epoch 0.95 >> (per-task accuracy): 61.15\n",
            "Epoch 0.95 >> (class accuracy): [96.2245, 97.533, 80.814, 53.3663, 94.8065, 11.0987, 48.643, 46.9844, 69.9179, 3.1715]\n",
            "Epoch 1.0 >> (per-task accuracy): 64.89\n",
            "Epoch 1.0 >> (class accuracy): [95.8163, 97.0925, 80.7171, 59.1089, 92.3625, 9.9776, 63.9875, 58.5603, 73.306, 9.217]\n",
            "OCS >> Task 1: {'accuracy': 20.19, 'per_class_accuracy': [59.4898, 15.0661, 5.1357, 27.9208, 64.5621, 0.0, 25.1566, 0.0973, 5.4415, 0.0991], 'loss': 2.112762843704224}\n",
            "OCS >> Task 2: {'accuracy': 20.75, 'per_class_accuracy': [67.7551, 3.348, 4.1667, 34.7525, 60.7943, 0.1121, 37.6827, 0.2918, 1.6427, 0.0991], 'loss': 2.089455602264404}\n",
            "OCS >> Task 3: {'accuracy': 22.53, 'per_class_accuracy': [71.5306, 1.4097, 2.7132, 46.0396, 64.4603, 0.1121, 41.7537, 0.0, 0.7187, 0.1982], 'loss': 2.066560785293579}\n",
            "OCS >> Task 4: {'accuracy': 24.92, 'per_class_accuracy': [75.9184, 2.5551, 1.0659, 55.7426, 68.2281, 0.1121, 47.5992, 0.0973, 0.0, 1.6848], 'loss': 2.0643390480041504}\n",
            "OCS >> Task 5: {'accuracy': 26.11, 'per_class_accuracy': [86.6327, 0.7048, 1.2597, 63.7624, 66.3951, 0.0, 44.3633, 0.0, 0.2053, 1.7839], 'loss': 2.061204684448242}\n",
            "OCS >> Task 6: {'accuracy': 25.32, 'per_class_accuracy': [87.7551, 1.674, 0.8721, 59.505, 62.831, 0.0, 38.6221, 0.0, 0.616, 4.9554], 'loss': 2.080282021331787}\n",
            "OCS >> Task 7: {'accuracy': 25.29, 'per_class_accuracy': [92.551, 2.8194, 0.5814, 60.297, 57.5356, 0.0, 27.453, 0.0, 0.4107, 14.1724], 'loss': 2.0825469650268555}\n",
            "OCS >> Task 8: {'accuracy': 24.71, 'per_class_accuracy': [93.8776, 5.5507, 0.4845, 55.1485, 53.9715, 0.1121, 23.5908, 0.4864, 1.4374, 14.8662], 'loss': 2.0877670204162597}\n",
            "OCS >> Task 9: {'accuracy': 25.62, 'per_class_accuracy': [94.1837, 16.8282, 1.8411, 49.1089, 48.778, 0.5605, 23.1733, 1.8482, 6.2628, 14.5689], 'loss': 2.061386925125122}\n",
            "OCS >> Task 10: {'accuracy': 29.12, 'per_class_accuracy': [96.1224, 32.511, 4.6512, 39.802, 51.9348, 1.9058, 31.6284, 3.2101, 12.5257, 16.4519], 'loss': 2.009706100845337}\n",
            "OCS >> Task 11: {'accuracy': 35.65, 'per_class_accuracy': [96.2245, 55.3304, 13.1783, 37.0297, 63.6456, 5.0448, 37.5783, 5.1556, 28.6448, 12.0912], 'loss': 1.9367064226150512}\n",
            "OCS >> Task 12: {'accuracy': 43.79, 'per_class_accuracy': [96.6327, 82.2026, 29.2636, 31.9802, 70.6721, 7.5112, 46.1378, 11.4786, 50.2053, 6.3429], 'loss': 1.847288211250305}\n",
            "OCS >> Task 13: {'accuracy': 52.73, 'per_class_accuracy': [97.3469, 83.4361, 53.6822, 39.1089, 85.8452, 12.2197, 59.1858, 22.179, 66.3244, 2.9732], 'loss': 1.7317980012893677}\n",
            "OCS >> Task 14: {'accuracy': 58.02, 'per_class_accuracy': [97.3469, 88.8106, 67.9264, 36.9307, 90.6314, 11.7713, 63.048, 35.3113, 76.078, 6.2438], 'loss': 1.6534391885757447}\n",
            "OCS >> Task 15: {'accuracy': 62.96, 'per_class_accuracy': [96.7347, 95.9471, 77.5194, 48.1188, 91.8534, 11.2108, 63.3612, 49.9027, 78.5421, 8.5233], 'loss': 1.5992211435317993}\n",
            "OCS >> Task 16: {'accuracy': 64.89, 'per_class_accuracy': [95.8163, 97.0925, 80.7171, 59.1089, 92.3625, 9.9776, 63.9875, 58.5603, 73.306, 9.217], 'loss': 1.5933766639709472}\n",
            "OCS >> (average accuracy): 35.1625\n",
            "OCS >> (Forgetting): 0.6315933333333335\n",
            "Maximum per-task accuracies: [96.34]\n",
            "\n",
            "---- Task 17 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 11.19\n",
            "Epoch 0.05 >> (class accuracy): [0.3061, 0.0, 0.0, 0.297, 0.5092, 0.0, 0.0, 74.1245, 35.5236, 0.0]\n",
            "Epoch 0.1 >> (per-task accuracy): 15.81\n",
            "Epoch 0.1 >> (class accuracy): [78.8776, 0.0, 0.0, 14.3564, 0.2037, 0.0, 0.1044, 24.8054, 41.5811, 0.0]\n",
            "Epoch 0.15 >> (per-task accuracy): 14.32\n",
            "Epoch 0.15 >> (class accuracy): [50.5102, 0.0, 0.0, 92.7723, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.2 >> (per-task accuracy): 16.21\n",
            "Epoch 0.2 >> (class accuracy): [79.7959, 0.0, 0.0, 82.9703, 0.1018, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.25 >> (per-task accuracy): 10.57\n",
            "Epoch 0.25 >> (class accuracy): [100.0, 0.0, 0.0, 5.0495, 2.6477, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.3 >> (per-task accuracy): 12.93\n",
            "Epoch 0.3 >> (class accuracy): [100.0, 0.0, 0.0, 6.3366, 24.3381, 0.0, 1.0438, 0.0, 0.0, 0.0]\n",
            "Epoch 0.35 >> (per-task accuracy): 18.41\n",
            "Epoch 0.35 >> (class accuracy): [99.898, 4.2291, 0.0, 3.2673, 79.5316, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.4 >> (per-task accuracy): 12.04\n",
            "Epoch 0.4 >> (class accuracy): [100.0, 0.0, 0.0, 0.297, 22.5051, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.45 >> (per-task accuracy): 13.57\n",
            "Epoch 0.45 >> (class accuracy): [100.0, 12.1586, 0.0, 14.4554, 9.2668, 0.0, 0.0, 0.0, 0.2053, 0.0]\n",
            "Epoch 0.5 >> (per-task accuracy): 19.07\n",
            "Epoch 0.5 >> (class accuracy): [100.0, 41.0573, 0.0, 12.3762, 26.3747, 0.0, 0.0, 0.4864, 0.8214, 6.3429]\n",
            "Epoch 0.55 >> (per-task accuracy): 27.37\n",
            "Epoch 0.55 >> (class accuracy): [100.0, 54.185, 5.2326, 13.0693, 49.1853, 0.0, 0.0, 0.8755, 0.7187, 45.2924]\n",
            "Epoch 0.6 >> (per-task accuracy): 23.44\n",
            "Epoch 0.6 >> (class accuracy): [99.898, 30.2203, 0.0969, 1.1881, 22.7088, 0.0, 0.0, 0.0, 0.0, 77.8989]\n",
            "Epoch 0.65 >> (per-task accuracy): 28.61\n",
            "Epoch 0.65 >> (class accuracy): [100.0, 75.5066, 2.0349, 4.0594, 69.4501, 0.0, 0.0, 1.6537, 0.8214, 25.2725]\n",
            "Epoch 0.7 >> (per-task accuracy): 39.37\n",
            "Epoch 0.7 >> (class accuracy): [99.6939, 87.489, 33.8178, 17.7228, 64.5621, 0.0, 0.1044, 1.9455, 43.9425, 35.2825]\n",
            "Epoch 0.75 >> (per-task accuracy): 43.09\n",
            "Epoch 0.75 >> (class accuracy): [99.2857, 85.6388, 50.1938, 25.3465, 86.3544, 0.0, 2.5052, 1.5564, 66.8378, 5.0545]\n",
            "Epoch 0.8 >> (per-task accuracy): 47.86\n",
            "Epoch 0.8 >> (class accuracy): [98.5714, 80.4405, 67.1512, 42.4752, 84.9287, 0.0, 2.1921, 7.393, 66.4271, 20.5154]\n",
            "Epoch 0.85 >> (per-task accuracy): 44.33\n",
            "Epoch 0.85 >> (class accuracy): [98.0612, 94.2731, 43.7984, 40.099, 95.3157, 0.0, 3.6534, 1.2646, 54.8255, 2.6759]\n",
            "Epoch 0.9 >> (per-task accuracy): 50.15\n",
            "Epoch 0.9 >> (class accuracy): [98.6735, 96.2996, 59.1085, 54.1584, 92.057, 0.0, 20.1461, 22.7626, 41.2731, 6.442]\n",
            "Epoch 0.95 >> (per-task accuracy): 52.03\n",
            "Epoch 0.95 >> (class accuracy): [97.3469, 91.1013, 64.3411, 33.3663, 90.224, 0.2242, 30.5846, 14.1051, 73.2033, 17.3439]\n",
            "Epoch 1.0 >> (per-task accuracy): 55.57\n",
            "Epoch 1.0 >> (class accuracy): [97.551, 95.859, 77.907, 42.9703, 81.8737, 0.1121, 37.6827, 17.607, 55.1335, 38.7512]\n",
            "OCS >> Task 1: {'accuracy': 19.42, 'per_class_accuracy': [74.5918, 22.9956, 8.624, 8.9109, 60.5906, 0.0, 5.8455, 0.0, 3.4908, 8.5233], 'loss': 2.1580525451660155}\n",
            "OCS >> Task 2: {'accuracy': 18.63, 'per_class_accuracy': [82.0408, 4.5815, 7.4612, 11.2871, 47.8615, 0.0, 9.0814, 0.0, 1.848, 23.885], 'loss': 2.1485280662536623}\n",
            "OCS >> Task 3: {'accuracy': 19.13, 'per_class_accuracy': [85.5102, 1.4978, 3.6822, 18.0198, 39.1039, 0.0, 11.8998, 0.0, 0.8214, 32.9039], 'loss': 2.1331911071777343}\n",
            "OCS >> Task 4: {'accuracy': 18.86, 'per_class_accuracy': [90.102, 0.4405, 1.5504, 11.9802, 25.6619, 0.0, 15.6576, 0.0, 0.0, 45.4906], 'loss': 2.1354184715270996}\n",
            "OCS >> Task 5: {'accuracy': 19.57, 'per_class_accuracy': [96.3265, 0.0, 1.0659, 10.0, 28.0041, 0.0, 17.5365, 0.0, 0.0, 45.3915], 'loss': 2.136820962142944}\n",
            "OCS >> Task 6: {'accuracy': 19.34, 'per_class_accuracy': [97.3469, 0.1762, 0.969, 5.8416, 33.0957, 0.0, 16.2839, 0.0, 0.0, 42.4182], 'loss': 2.146920080947876}\n",
            "OCS >> Task 7: {'accuracy': 19.44, 'per_class_accuracy': [98.0612, 0.0881, 0.7752, 2.7723, 39.002, 0.0, 11.1691, 0.0, 0.4107, 44.7968], 'loss': 2.153160322570801}\n",
            "OCS >> Task 8: {'accuracy': 19.34, 'per_class_accuracy': [97.8571, 0.0, 0.3876, 1.6832, 44.8065, 0.0, 8.5595, 0.0, 0.4107, 42.4182], 'loss': 2.1628935085296632}\n",
            "OCS >> Task 9: {'accuracy': 19.5, 'per_class_accuracy': [97.7551, 0.2643, 0.5814, 2.2772, 42.057, 0.0, 9.1858, 0.0, 2.0534, 43.5084], 'loss': 2.1520668922424315}\n",
            "OCS >> Task 10: {'accuracy': 20.42, 'per_class_accuracy': [97.449, 2.467, 1.7442, 2.4752, 44.3992, 0.0, 12.3173, 0.0, 3.2854, 42.6165], 'loss': 2.1244800773620605}\n",
            "OCS >> Task 11: {'accuracy': 23.56, 'per_class_accuracy': [97.3469, 14.185, 5.5233, 3.5644, 51.9348, 0.0, 15.2401, 0.0, 7.2895, 41.7245], 'loss': 2.083010821533203}\n",
            "OCS >> Task 12: {'accuracy': 28.84, 'per_class_accuracy': [97.9592, 34.8899, 14.9225, 5.5446, 62.3218, 0.1121, 17.2234, 0.4864, 19.0965, 34.5887], 'loss': 2.026479217529297}\n",
            "OCS >> Task 13: {'accuracy': 37.61, 'per_class_accuracy': [98.1633, 54.185, 33.7209, 13.5644, 81.8737, 0.2242, 33.5073, 1.8482, 38.9117, 17.2448], 'loss': 1.9402165538787841}\n",
            "OCS >> Task 14: {'accuracy': 43.98, 'per_class_accuracy': [98.0612, 67.1366, 51.0659, 17.2277, 87.78, 0.1121, 36.952, 3.7938, 55.8522, 17.2448], 'loss': 1.8747170986175536}\n",
            "OCS >> Task 15: {'accuracy': 51.09, 'per_class_accuracy': [98.0612, 83.1718, 66.376, 29.901, 89.8167, 0.0, 41.1273, 7.0039, 65.4004, 22.9931], 'loss': 1.8203803001403809}\n",
            "OCS >> Task 16: {'accuracy': 55.05, 'per_class_accuracy': [97.551, 92.9515, 74.3217, 41.6832, 89.3075, 0.0, 38.5177, 17.3152, 63.655, 25.9663], 'loss': 1.8039336938858033}\n",
            "OCS >> Task 17: {'accuracy': 55.57, 'per_class_accuracy': [97.551, 95.859, 77.907, 42.9703, 81.8737, 0.1121, 37.6827, 17.607, 55.1335, 38.7512], 'loss': 1.8111045764923095}\n",
            "OCS >> (average accuracy): 28.78529411764706\n",
            "OCS >> (Forgetting): 0.6922875000000001\n",
            "Maximum per-task accuracies: [96.34]\n",
            "\n",
            "---- Task 18 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 10.28\n",
            "Epoch 0.05 >> (class accuracy): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 100.0, 0.0, 0.0]\n",
            "Epoch 0.1 >> (per-task accuracy): 8.74\n",
            "Epoch 0.1 >> (class accuracy): [0.2041, 0.0, 0.0, 16.3366, 0.0, 0.0, 0.0, 9.9222, 62.115, 0.0]\n",
            "Epoch 0.15 >> (per-task accuracy): 10.1\n",
            "Epoch 0.15 >> (class accuracy): [0.0, 0.0, 0.0, 100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.2 >> (per-task accuracy): 10.1\n",
            "Epoch 0.2 >> (class accuracy): [0.0, 0.0, 0.0, 100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.25 >> (per-task accuracy): 20.68\n",
            "Epoch 0.25 >> (class accuracy): [50.9184, 0.0, 0.0, 65.1485, 0.0, 0.0, 0.0, 12.6459, 57.5975, 21.8038]\n",
            "Epoch 0.3 >> (per-task accuracy): 23.16\n",
            "Epoch 0.3 >> (class accuracy): [54.2857, 19.9119, 0.0, 85.5446, 26.8839, 0.0, 2.2965, 8.8521, 18.5832, 13.4787]\n",
            "Epoch 0.35 >> (per-task accuracy): 16.81\n",
            "Epoch 0.35 >> (class accuracy): [100.0, 0.1762, 0.0, 0.5941, 3.055, 0.0, 0.0, 9.0467, 0.1027, 56.3925]\n",
            "Epoch 0.4 >> (per-task accuracy): 21.95\n",
            "Epoch 0.4 >> (class accuracy): [90.4082, 0.0, 0.0, 0.0, 0.2037, 0.0, 8.977, 41.0506, 0.0, 79.1873]\n",
            "Epoch 0.45 >> (per-task accuracy): 15.16\n",
            "Epoch 0.45 >> (class accuracy): [10.2041, 0.0, 0.0, 0.0, 0.0, 0.0, 96.6597, 20.0389, 0.0, 28.1467]\n",
            "Epoch 0.5 >> (per-task accuracy): 14.56\n",
            "Epoch 0.5 >> (class accuracy): [100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6263, 0.0, 0.2053, 46.3826]\n",
            "Epoch 0.55 >> (per-task accuracy): 22.66\n",
            "Epoch 0.55 >> (class accuracy): [99.3878, 0.0, 0.0, 0.0, 0.0, 4.0359, 0.0, 43.8716, 4.8255, 75.1239]\n",
            "Epoch 0.6 >> (per-task accuracy): 31.13\n",
            "Epoch 0.6 >> (class accuracy): [84.1837, 0.0, 34.1085, 0.0, 0.0, 0.5605, 42.6931, 60.9922, 20.1232, 69.2765]\n",
            "Epoch 0.65 >> (per-task accuracy): 30.27\n",
            "Epoch 0.65 >> (class accuracy): [98.1633, 0.0, 53.9729, 20.7921, 22.1996, 0.1121, 0.0, 91.3424, 0.4107, 13.4787]\n",
            "Epoch 0.7 >> (per-task accuracy): 37.78\n",
            "Epoch 0.7 >> (class accuracy): [96.5306, 59.3833, 62.8876, 13.8614, 12.22, 0.0, 11.7954, 92.5097, 0.2053, 18.1368]\n",
            "Epoch 0.75 >> (per-task accuracy): 38.2\n",
            "Epoch 0.75 >> (class accuracy): [93.8776, 0.0, 72.2868, 31.0891, 0.0, 0.0, 0.0, 94.5525, 40.5544, 46.8781]\n",
            "Epoch 0.8 >> (per-task accuracy): 30.91\n",
            "Epoch 0.8 >> (class accuracy): [94.6939, 0.0, 80.814, 5.4455, 0.0, 0.6726, 0.0, 0.0, 30.4928, 96.2339]\n",
            "Epoch 0.85 >> (per-task accuracy): 38.61\n",
            "Epoch 0.85 >> (class accuracy): [93.1633, 0.0, 88.2752, 25.4455, 0.0, 0.0, 0.1044, 87.4514, 27.6181, 60.555]\n",
            "Epoch 0.9 >> (per-task accuracy): 41.71\n",
            "Epoch 0.9 >> (class accuracy): [97.8571, 0.9692, 73.062, 47.8218, 38.5947, 0.3363, 0.0, 89.6887, 28.6448, 37.7602]\n",
            "Epoch 0.95 >> (per-task accuracy): 36.75\n",
            "Epoch 0.95 >> (class accuracy): [93.8776, 0.0, 85.3682, 10.495, 8.8595, 0.0, 0.0, 19.5525, 73.8193, 75.4212]\n",
            "Epoch 1.0 >> (per-task accuracy): 54.81\n",
            "Epoch 1.0 >> (class accuracy): [93.3673, 70.837, 81.0078, 82.9703, 3.1568, 0.0, 0.1044, 79.1829, 62.0123, 63.2309]\n",
            "OCS >> Task 1: {'accuracy': 17.75, 'per_class_accuracy': [41.8367, 1.8502, 14.7287, 10.6931, 8.6558, 0.0, 0.0, 64.2996, 7.7002, 26.0654], 'loss': 2.1268244163513184}\n",
            "OCS >> Task 2: {'accuracy': 19.66, 'per_class_accuracy': [45.102, 0.7048, 11.3372, 8.9109, 4.277, 0.0, 0.0, 80.0584, 4.5175, 39.6432], 'loss': 2.113703593444824}\n",
            "OCS >> Task 3: {'accuracy': 20.7, 'per_class_accuracy': [45.3061, 0.1762, 9.3992, 12.6733, 1.0183, 0.0, 0.3132, 83.5603, 2.1561, 50.1487], 'loss': 2.10467299156189}\n",
            "OCS >> Task 4: {'accuracy': 20.1, 'per_class_accuracy': [45.4082, 0.0881, 6.1047, 12.7723, 0.4073, 0.0, 0.2088, 64.9805, 0.0, 69.1774], 'loss': 2.128083892059326}\n",
            "OCS >> Task 5: {'accuracy': 19.02, 'per_class_accuracy': [51.7347, 0.0881, 6.1047, 13.1683, 0.5092, 0.0, 0.1044, 38.8132, 0.1027, 78.4936], 'loss': 2.141542428970337}\n",
            "OCS >> Task 6: {'accuracy': 17.54, 'per_class_accuracy': [50.7143, 0.2643, 4.845, 16.3366, 0.2037, 0.0, 0.2088, 23.4436, 0.4107, 78.2953], 'loss': 2.1749961322784426}\n",
            "OCS >> Task 7: {'accuracy': 17.85, 'per_class_accuracy': [58.0612, 0.793, 3.1008, 16.8317, 0.611, 0.0, 0.0, 17.3152, 1.232, 80.1784], 'loss': 2.1966819980621337}\n",
            "OCS >> Task 8: {'accuracy': 17.04, 'per_class_accuracy': [68.4694, 0.5286, 1.5504, 11.7822, 0.4073, 0.0, 0.0, 9.6304, 1.3347, 76.9078], 'loss': 2.229935450744629}\n",
            "OCS >> Task 9: {'accuracy': 18.13, 'per_class_accuracy': [72.551, 0.0, 2.5194, 17.1287, 1.7312, 0.0, 0.0, 6.323, 6.0575, 75.5203], 'loss': 2.2177807418823243}\n",
            "OCS >> Task 10: {'accuracy': 18.97, 'per_class_accuracy': [80.9184, 0.0, 4.2636, 14.8515, 2.9532, 0.0, 0.0, 5.0584, 10.4723, 72.0515], 'loss': 2.182572916793823}\n",
            "OCS >> Task 11: {'accuracy': 20.18, 'per_class_accuracy': [86.2245, 1.0573, 8.5271, 17.1287, 3.055, 0.0, 0.2088, 3.3074, 19.7125, 63.6274], 'loss': 2.123207529449463}\n",
            "OCS >> Task 12: {'accuracy': 22.09, 'per_class_accuracy': [89.4898, 5.9912, 17.6357, 13.8614, 1.222, 0.0, 0.0, 8.5603, 35.8316, 48.8603], 'loss': 2.041761363220215}\n",
            "OCS >> Task 13: {'accuracy': 29.45, 'per_class_accuracy': [93.6735, 26.4317, 36.6279, 25.5446, 0.4073, 0.0, 0.6263, 24.0272, 52.3614, 32.111], 'loss': 1.913846834564209}\n",
            "OCS >> Task 14: {'accuracy': 35.22, 'per_class_accuracy': [95.8163, 32.5991, 52.0349, 29.1089, 0.5092, 0.0, 0.5219, 48.249, 64.8871, 24.1824], 'loss': 1.821430523109436}\n",
            "OCS >> Task 15: {'accuracy': 44.04, 'per_class_accuracy': [95.2041, 56.0352, 64.6318, 47.6238, 0.4073, 0.0, 0.5219, 64.8833, 72.4846, 30.2279], 'loss': 1.736200132751465}\n",
            "OCS >> Task 16: {'accuracy': 49.56, 'per_class_accuracy': [95.0, 62.1145, 73.3527, 63.4653, 0.9165, 0.0, 0.2088, 77.5292, 74.846, 38.1566], 'loss': 1.6912175714492799}\n",
            "OCS >> Task 17: {'accuracy': 53.22, 'per_class_accuracy': [94.6939, 66.2555, 78.7791, 75.1485, 1.833, 0.0, 0.0, 81.0311, 71.3552, 51.9326], 'loss': 1.6789618913650513}\n",
            "OCS >> Task 18: {'accuracy': 54.81, 'per_class_accuracy': [93.3673, 70.837, 81.0078, 82.9703, 3.1568, 0.0, 0.1044, 79.1829, 62.0123, 63.2309], 'loss': 1.7072448444366455}\n",
            "OCS >> (average accuracy): 27.51833333333333\n",
            "OCS >> (Forgetting): 0.7042705882352942\n",
            "Maximum per-task accuracies: [96.34]\n",
            "\n",
            "---- Task 19 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 8.2\n",
            "Epoch 0.05 >> (class accuracy): [0.0, 0.0, 0.0, 10.7921, 0.0, 0.0, 0.0, 69.1634, 0.0, 0.0]\n",
            "Epoch 0.1 >> (per-task accuracy): 10.56\n",
            "Epoch 0.1 >> (class accuracy): [0.0, 0.0, 0.0, 96.2376, 0.0, 0.0, 0.0, 8.1712, 0.0, 0.0]\n",
            "Epoch 0.15 >> (per-task accuracy): 10.1\n",
            "Epoch 0.15 >> (class accuracy): [0.0, 0.0, 0.0, 100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.2 >> (per-task accuracy): 10.1\n",
            "Epoch 0.2 >> (class accuracy): [0.0, 0.0, 0.0, 100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.25 >> (per-task accuracy): 10.1\n",
            "Epoch 0.25 >> (class accuracy): [0.0, 0.0, 0.0, 100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.3 >> (per-task accuracy): 10.1\n",
            "Epoch 0.3 >> (class accuracy): [0.0, 0.0, 0.0, 100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 0.35 >> (per-task accuracy): 12.26\n",
            "Epoch 0.35 >> (class accuracy): [0.0, 0.0, 0.0, 99.703, 0.0, 0.0, 0.0, 21.2062, 0.1027, 0.0]\n",
            "Epoch 0.4 >> (per-task accuracy): 18.53\n",
            "Epoch 0.4 >> (class accuracy): [6.5306, 0.0, 1.6473, 81.6832, 0.0, 0.0, 0.0, 91.537, 0.5133, 0.0991]\n",
            "Epoch 0.45 >> (per-task accuracy): 27.48\n",
            "Epoch 0.45 >> (class accuracy): [62.8571, 0.0, 0.0, 93.2673, 0.0, 0.0, 0.0, 34.6304, 0.0, 82.6561]\n",
            "Epoch 0.5 >> (per-task accuracy): 22.78\n",
            "Epoch 0.5 >> (class accuracy): [47.6531, 0.0, 0.0, 80.297, 99.6945, 0.0, 0.0, 2.0428, 0.0, 0.0]\n",
            "Epoch 0.55 >> (per-task accuracy): 19.08\n",
            "Epoch 0.55 >> (class accuracy): [74.4898, 0.0, 0.0, 5.7426, 9.776, 0.0, 0.0, 99.6109, 0.0, 0.0]\n",
            "Epoch 0.6 >> (per-task accuracy): 14.89\n",
            "Epoch 0.6 >> (class accuracy): [46.8367, 0.0, 0.0, 0.0, 0.2037, 0.0, 0.0, 100.0, 0.0, 0.0]\n",
            "Epoch 0.65 >> (per-task accuracy): 19.72\n",
            "Epoch 0.65 >> (class accuracy): [78.8776, 0.0, 1.3566, 15.4455, 0.2037, 0.0, 0.0, 99.8054, 0.0, 0.0991]\n",
            "Epoch 0.7 >> (per-task accuracy): 16.64\n",
            "Epoch 0.7 >> (class accuracy): [63.5714, 0.0, 0.0, 1.2871, 0.0, 0.0, 0.0, 100.0, 0.0, 0.0]\n",
            "Epoch 0.75 >> (per-task accuracy): 19.41\n",
            "Epoch 0.75 >> (class accuracy): [93.7755, 0.0, 0.0, 0.0, 0.2037, 0.0, 0.0, 99.2218, 0.0, 0.0]\n",
            "Epoch 0.8 >> (per-task accuracy): 26.96\n",
            "Epoch 0.8 >> (class accuracy): [99.1837, 0.0, 43.0233, 25.1485, 9.5723, 0.0, 0.0, 90.6615, 0.0, 0.0]\n",
            "Epoch 0.85 >> (per-task accuracy): 26.48\n",
            "Epoch 0.85 >> (class accuracy): [96.5306, 0.0, 18.7016, 31.5842, 0.4073, 0.0, 21.7119, 95.0389, 0.0, 0.0991]\n",
            "Epoch 0.9 >> (per-task accuracy): 33.64\n",
            "Epoch 0.9 >> (class accuracy): [99.1837, 0.0, 52.1318, 46.9307, 26.1711, 0.0, 1.7745, 86.6732, 0.0, 21.3082]\n",
            "Epoch 0.95 >> (per-task accuracy): 25.29\n",
            "Epoch 0.95 >> (class accuracy): [97.0408, 19.6476, 4.0698, 19.2079, 8.0448, 0.0, 4.1754, 96.7899, 0.5133, 0.0]\n",
            "Epoch 1.0 >> (per-task accuracy): 25.74\n",
            "Epoch 1.0 >> (class accuracy): [96.9388, 0.0, 20.6395, 36.2376, 0.2037, 0.0, 3.9666, 97.7626, 0.0, 0.0]\n",
            "OCS >> Task 1: {'accuracy': 14.79, 'per_class_accuracy': [59.7959, 0.0, 0.3876, 0.7921, 0.1018, 0.0, 0.2088, 85.4086, 0.0, 0.0], 'loss': 2.237479430389404}\n",
            "OCS >> Task 2: {'accuracy': 15.44, 'per_class_accuracy': [60.5102, 0.0, 0.3876, 0.8911, 0.0, 0.0, 0.2088, 91.0506, 0.0, 0.0], 'loss': 2.2303991760253905}\n",
            "OCS >> Task 3: {'accuracy': 15.48, 'per_class_accuracy': [57.7551, 0.0, 0.2907, 0.5941, 0.0, 0.0, 0.8351, 93.8716, 0.0, 0.0], 'loss': 2.221743563079834}\n",
            "OCS >> Task 4: {'accuracy': 15.63, 'per_class_accuracy': [55.9184, 0.0, 0.1938, 0.7921, 0.0, 0.0, 1.8789, 95.9144, 0.0, 0.0991], 'loss': 2.221414346313477}\n",
            "OCS >> Task 5: {'accuracy': 15.73, 'per_class_accuracy': [56.5306, 0.0, 0.3876, 0.495, 0.0, 0.0, 1.1482, 97.179, 0.0, 0.0], 'loss': 2.2254349426269533}\n",
            "OCS >> Task 6: {'accuracy': 14.9, 'per_class_accuracy': [48.7755, 0.0, 0.0969, 0.396, 0.0, 0.0, 0.1044, 97.8599, 0.0, 0.0], 'loss': 2.237822801589966}\n",
            "OCS >> Task 7: {'accuracy': 14.9, 'per_class_accuracy': [49.4898, 0.0, 0.0969, 0.297, 0.0, 0.0, 0.0, 97.3735, 0.0, 0.0], 'loss': 2.2433852054595946}\n",
            "OCS >> Task 8: {'accuracy': 14.8, 'per_class_accuracy': [49.1837, 0.0, 0.0, 0.198, 0.1018, 0.0, 0.0, 96.7899, 0.0, 0.0], 'loss': 2.257111343383789}\n",
            "OCS >> Task 9: {'accuracy': 15.37, 'per_class_accuracy': [54.898, 0.0, 0.0, 1.0891, 0.0, 0.0, 0.3132, 95.8171, 0.0, 0.0], 'loss': 2.2548053215026855}\n",
            "OCS >> Task 10: {'accuracy': 15.95, 'per_class_accuracy': [60.4082, 0.0, 0.0969, 1.8812, 0.2037, 0.0, 0.1044, 95.3307, 0.0, 0.0], 'loss': 2.246705669784546}\n",
            "OCS >> Task 11: {'accuracy': 16.81, 'per_class_accuracy': [66.1224, 0.0, 0.4845, 3.1683, 0.3055, 0.0, 0.5219, 96.1089, 0.0, 0.0], 'loss': 2.229022299194336}\n",
            "OCS >> Task 12: {'accuracy': 17.87, 'per_class_accuracy': [73.9796, 0.0, 1.3566, 2.2772, 0.5092, 0.0, 2.5052, 96.8872, 0.0, 0.0], 'loss': 2.203391144180298}\n",
            "OCS >> Task 13: {'accuracy': 19.91, 'per_class_accuracy': [83.8776, 0.0, 4.6512, 7.5248, 0.1018, 0.0, 3.8622, 97.9572, 0.0, 0.0], 'loss': 2.164462364578247}\n",
            "OCS >> Task 14: {'accuracy': 22.29, 'per_class_accuracy': [91.0204, 0.0, 10.8527, 11.1881, 0.3055, 0.0, 10.1253, 98.4436, 0.0, 0.0], 'loss': 2.1296997997283937}\n",
            "OCS >> Task 15: {'accuracy': 24.64, 'per_class_accuracy': [93.6735, 0.0, 20.3488, 15.7426, 4.4807, 0.0, 12.3173, 98.7354, 0.0, 0.0], 'loss': 2.0960206367492678}\n",
            "OCS >> Task 16: {'accuracy': 26.59, 'per_class_accuracy': [95.3061, 0.0, 29.6512, 25.5446, 3.5642, 0.0, 11.691, 98.6381, 0.0, 0.0], 'loss': 2.0712369258880616}\n",
            "OCS >> Task 17: {'accuracy': 27.07, 'per_class_accuracy': [97.1429, 0.0, 31.8798, 28.2178, 0.9165, 0.0, 12.1086, 98.8327, 0.0, 0.0], 'loss': 2.0565655418396}\n",
            "OCS >> Task 18: {'accuracy': 27.05, 'per_class_accuracy': [97.0408, 0.0, 31.8798, 33.6634, 0.0, 0.0, 7.4113, 98.6381, 0.0, 0.0], 'loss': 2.0564744140625}\n",
            "OCS >> Task 19: {'accuracy': 25.74, 'per_class_accuracy': [96.9388, 0.0, 20.6395, 36.2376, 0.2037, 0.0, 3.9666, 97.7626, 0.0, 0.0], 'loss': 2.067570761871338}\n",
            "OCS >> (average accuracy): 18.997894736842103\n",
            "OCS >> (Forgetting): 0.7771666666666667\n",
            "Maximum per-task accuracies: [96.34]\n",
            "\n",
            "---- Task 20 (OCS) ----\n",
            "Epoch 0.05 >> (per-task accuracy): 8.61\n",
            "Epoch 0.05 >> (class accuracy): [1.1224, 0.0, 0.0, 0.0, 0.5092, 0.0, 0.1044, 58.7549, 24.6407, 0.0]\n",
            "Epoch 0.1 >> (per-task accuracy): 9.8\n",
            "Epoch 0.1 >> (class accuracy): [11.1224, 0.0, 0.0, 0.0, 0.8147, 0.0, 0.1044, 59.2412, 25.9754, 0.0]\n",
            "Epoch 0.15 >> (per-task accuracy): 12.37\n",
            "Epoch 0.15 >> (class accuracy): [38.2653, 0.0, 0.2907, 0.198, 1.4257, 0.0, 0.1044, 57.393, 25.8727, 0.0]\n",
            "Epoch 0.2 >> (per-task accuracy): 15.44\n",
            "Epoch 0.2 >> (class accuracy): [70.4082, 0.0, 2.0349, 0.5941, 2.9532, 0.0, 0.0, 52.9183, 26.078, 0.0]\n",
            "Epoch 0.25 >> (per-task accuracy): 17.23\n",
            "Epoch 0.25 >> (class accuracy): [87.9592, 0.0, 4.4574, 2.5743, 4.6843, 0.0, 0.0, 48.93, 24.6407, 0.0]\n",
            "Epoch 0.3 >> (per-task accuracy): 17.92\n",
            "Epoch 0.3 >> (class accuracy): [95.0, 0.0, 9.6899, 5.5446, 6.2118, 0.0, 0.0, 41.4397, 22.3819, 0.0]\n",
            "Epoch 0.35 >> (per-task accuracy): 18.67\n",
            "Epoch 0.35 >> (class accuracy): [98.2653, 0.0, 14.438, 9.1089, 9.776, 0.0, 0.0, 37.8405, 18.2752, 0.0]\n",
            "Epoch 0.4 >> (per-task accuracy): 18.85\n",
            "Epoch 0.4 >> (class accuracy): [99.1837, 0.0, 20.155, 11.1881, 12.0163, 0.0, 0.0, 33.9494, 12.8337, 0.0]\n",
            "Epoch 0.45 >> (per-task accuracy): 19.25\n",
            "Epoch 0.45 >> (class accuracy): [99.5918, 0.0, 25.2907, 14.0594, 14.3585, 0.0, 0.0, 29.2802, 10.6776, 0.0]\n",
            "Epoch 0.5 >> (per-task accuracy): 19.33\n",
            "Epoch 0.5 >> (class accuracy): [99.6939, 0.0, 28.9729, 14.5545, 15.9878, 0.0, 0.0, 26.3619, 8.4189, 0.0]\n",
            "Epoch 0.55 >> (per-task accuracy): 19.25\n",
            "Epoch 0.55 >> (class accuracy): [99.7959, 0.0, 32.4612, 14.3564, 16.5988, 0.0, 0.0, 23.8327, 6.0575, 0.0]\n",
            "Epoch 0.6 >> (per-task accuracy): 19.16\n",
            "Epoch 0.6 >> (class accuracy): [99.7959, 0.0, 34.4961, 14.0594, 17.5153, 0.0, 0.0, 21.9844, 4.3121, 0.0]\n",
            "Epoch 0.65 >> (per-task accuracy): 19.14\n",
            "Epoch 0.65 >> (class accuracy): [99.7959, 0.0, 36.7248, 13.3663, 19.0428, 0.0, 0.0, 19.5525, 3.4908, 0.0]\n",
            "Epoch 0.7 >> (per-task accuracy): 19.1\n",
            "Epoch 0.7 >> (class accuracy): [99.7959, 0.0, 39.0504, 12.6733, 19.6538, 0.0, 0.0, 17.8016, 2.5667, 0.0]\n",
            "Epoch 0.75 >> (per-task accuracy): 19.06\n",
            "Epoch 0.75 >> (class accuracy): [99.7959, 0.0, 40.5039, 12.0792, 20.0611, 0.0, 0.0, 16.6342, 2.0534, 0.0]\n",
            "Epoch 0.8 >> (per-task accuracy): 19.25\n",
            "Epoch 0.8 >> (class accuracy): [99.898, 0.0, 41.8605, 11.5842, 21.2831, 0.0, 0.0, 16.537, 1.848, 0.0]\n",
            "Epoch 0.85 >> (per-task accuracy): 19.37\n",
            "Epoch 0.85 >> (class accuracy): [99.898, 0.0881, 43.314, 11.4851, 21.8941, 0.0, 0.0, 16.3424, 1.1294, 0.0]\n",
            "Epoch 0.9 >> (per-task accuracy): 19.35\n",
            "Epoch 0.9 >> (class accuracy): [99.898, 0.1762, 44.186, 11.2871, 22.0978, 0.0, 0.0, 15.3696, 0.924, 0.0]\n",
            "Epoch 0.95 >> (per-task accuracy): 19.54\n",
            "Epoch 0.95 >> (class accuracy): [99.898, 0.2643, 46.0271, 11.1881, 22.1996, 0.0, 0.0, 15.5642, 0.5133, 0.0991]\n",
            "Epoch 1.0 >> (per-task accuracy): 19.6\n",
            "Epoch 1.0 >> (class accuracy): [99.898, 0.5286, 47.3837, 10.198, 22.4033, 0.0, 0.0, 15.3696, 0.4107, 0.0991]\n",
            "OCS >> Task 1: {'accuracy': 10.79, 'per_class_accuracy': [98.8776, 0.0, 2.0349, 6.3366, 2.1385, 0.0, 0.0, 0.3891, 0.0, 0.0], 'loss': 2.2874771713256834}\n",
            "OCS >> Task 2: {'accuracy': 11.23, 'per_class_accuracy': [98.6735, 0.0, 2.3256, 9.703, 2.2403, 0.0, 0.0, 0.9728, 0.2053, 0.0], 'loss': 2.2875738140106203}\n",
            "OCS >> Task 3: {'accuracy': 11.58, 'per_class_accuracy': [98.1633, 0.0, 1.938, 12.1782, 3.666, 0.0, 0.0, 1.6537, 0.0, 0.0], 'loss': 2.2878987007141114}\n",
            "OCS >> Task 4: {'accuracy': 10.96, 'per_class_accuracy': [99.0816, 0.0, 0.1938, 4.8515, 4.0733, 0.0, 0.0, 3.1128, 0.0, 0.1982], 'loss': 2.2896641445159913}\n",
            "OCS >> Task 5: {'accuracy': 10.88, 'per_class_accuracy': [99.6939, 0.0, 0.0969, 3.6634, 5.8045, 0.0, 0.0, 1.5564, 0.0, 0.0], 'loss': 2.290746248626709}\n",
            "OCS >> Task 6: {'accuracy': 10.7, 'per_class_accuracy': [99.4898, 0.0, 0.0, 2.5743, 5.9063, 0.0, 0.0, 1.07, 0.0, 0.0], 'loss': 2.2933010848999023}\n",
            "OCS >> Task 7: {'accuracy': 10.11, 'per_class_accuracy': [99.1837, 0.0, 0.1938, 0.396, 2.8513, 0.0, 0.0, 0.4864, 0.0, 0.0], 'loss': 2.293821435928345}\n",
            "OCS >> Task 8: {'accuracy': 10.32, 'per_class_accuracy': [99.5918, 0.0, 0.3876, 0.198, 4.7862, 0.0, 0.0, 0.2918, 0.0, 0.0], 'loss': 2.295656517028809}\n",
            "OCS >> Task 9: {'accuracy': 9.94, 'per_class_accuracy': [98.2653, 0.0, 0.3876, 0.0, 2.6477, 0.0, 0.0, 0.0973, 0.0, 0.0], 'loss': 2.2941543933868407}\n",
            "OCS >> Task 10: {'accuracy': 10.11, 'per_class_accuracy': [99.4898, 0.0, 0.6783, 0.0, 2.9532, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.29108796005249}\n",
            "OCS >> Task 11: {'accuracy': 10.07, 'per_class_accuracy': [99.4898, 0.0, 0.5814, 0.099, 2.5458, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.2875808536529543}\n",
            "OCS >> Task 12: {'accuracy': 10.0, 'per_class_accuracy': [99.7959, 0.0, 0.0969, 0.099, 2.0367, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.284169701385498}\n",
            "OCS >> Task 13: {'accuracy': 10.01, 'per_class_accuracy': [100.0, 0.0, 0.3876, 0.0, 1.7312, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': 2.277438439178467}\n",
            "OCS >> Task 14: {'accuracy': 10.14, 'per_class_accuracy': [100.0, 0.0, 1.0659, 0.198, 1.5275, 0.0, 0.0, 0.5837, 0.0, 0.0], 'loss': 2.2722635459899903}\n",
            "OCS >> Task 15: {'accuracy': 10.47, 'per_class_accuracy': [100.0, 0.0, 3.9729, 0.198, 1.4257, 0.0, 0.0, 0.8755, 0.1027, 0.0], 'loss': 2.2669894081115722}\n",
            "OCS >> Task 16: {'accuracy': 11.44, 'per_class_accuracy': [100.0, 0.0, 8.1395, 0.5941, 2.3422, 0.0, 0.0, 4.7665, 0.2053, 0.0], 'loss': 2.2610890815734863}\n",
            "OCS >> Task 17: {'accuracy': 12.25, 'per_class_accuracy': [100.0, 0.0, 13.7597, 0.396, 4.0733, 0.0, 0.0, 5.642, 0.1027, 0.0], 'loss': 2.2556199851989747}\n",
            "OCS >> Task 18: {'accuracy': 14.77, 'per_class_accuracy': [100.0, 1.2335, 28.0039, 0.7921, 6.721, 0.0, 0.0, 11.4786, 0.2053, 0.0], 'loss': 2.2495537952423095}\n",
            "OCS >> Task 19: {'accuracy': 16.7, 'per_class_accuracy': [100.0, 1.7621, 33.4302, 3.0693, 16.0896, 0.0, 0.0, 12.8405, 0.308, 0.0991], 'loss': 2.247036911010742}\n",
            "OCS >> Task 20: {'accuracy': 19.6, 'per_class_accuracy': [99.898, 0.5286, 47.3837, 10.198, 22.4033, 0.0, 0.0, 15.3696, 0.4107, 0.0991], 'loss': 2.2443473754882812}\n",
            "OCS >> (average accuracy): 11.6035\n",
            "OCS >> (Forgetting): 0.8515736842105264\n",
            "Maximum per-task accuracies: [96.34]\n",
            "\n",
            "{'num_tasks': 20, 'per_task_rotation': 9, 'memory_size': 200, 'dataset': 'noisy-rot-mnist', 'device': 'cuda', 'momentum': 0.75, 'mlp_hiddens': 256, 'dropout': 0.2, 'lr_decay': 0.75, 'n_classes': 10, 'seq_lr': 0.005, 'stream_size': 100, 'ocspick': True, 'batch_size': 5, 'ref_hyp': 10.0, 'n_substeps': 20}\n"
          ]
        }
      ],
      "source": [
        "# NEW\n",
        "DATASET = 'noisy-rot-mnist'\n",
        "HIDDENS = 256\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "config = {\n",
        "    'num_tasks': 20,\n",
        "    'per_task_rotation': 9,\n",
        "    'memory_size': 200,\n",
        "    'dataset': DATASET,\n",
        "    'device': DEVICE,\n",
        "    'momentum': 0.7,\n",
        "    'mlp_hiddens': HIDDENS,\n",
        "    'dropout': 0.15,\n",
        "    'lr_decay': 0.7 if 'rot-mnist' in DATASET else 0.76,\n",
        "    'n_classes': 10,\n",
        "    'seq_lr': 0.006,\n",
        "    'stream_size': 100,\n",
        "    'ocspick': True,\n",
        "    'batch_size': 5,\n",
        "    #  'tau': 950.0,\n",
        "    'ref_hyp': 10. if 'rot-mnist' in DATASET else 50\n",
        "}\n",
        "\n",
        "log_dir =  f\"./summery/{config['dataset']}\"\n",
        "summary = SummaryWriter(log_dir)\n",
        "\n",
        "experiment = Experiment(api_key=\"hidden_key\", project_name=\"mnist\", disabled=True)\n",
        "\n",
        "loaders = get_all_loaders(config)\n",
        "\n",
        "\n",
        "def evaluate_model(model, task, loaders, config):\n",
        "    accuracies, losses = [], []\n",
        "    for t in range(1, task + 1):\n",
        "        metrics = eval_single_epoch(model, loaders['sequential'][t]['val'], config)\n",
        "        accuracies.append(metrics['accuracy'])\n",
        "        losses.append(metrics['loss'])\n",
        "        print(f'OCS >> Task {t}: {metrics}')\n",
        "    return accuracies, losses\n",
        "\n",
        "def main():\n",
        "    setup_experiment(experiment, config)\n",
        "\n",
        "    max_accuracies = [0.0] * config['num_tasks']\n",
        "    for task in range(1, config['num_tasks'] + 1):\n",
        "        print(f'---- Task {task} (OCS) ----')\n",
        "        model = train_task_sequentially(task, loaders, config, summary)\n",
        "\n",
        "        accuracies, _ = evaluate_model(model, task, loaders, config)\n",
        "        max_accuracies = [max(acc, max_acc) for acc, max_acc in zip(accuracies, max_accuracies)]\n",
        "\n",
        "        avg_accuracy = np.mean(accuracies)\n",
        "        if task > 1:\n",
        "            forgetting = np.mean(np.array(max_accuracies[:task - 1]) - np.array(accuracies[:task - 1]))/ 100\n",
        "        else:\n",
        "            forgetting = 0.0\n",
        "\n",
        "        print(f\"OCS >> (average accuracy): {avg_accuracy}\")\n",
        "        print(f\"OCS >> (Forgetting): {forgetting}\")\n",
        "        summary.add_scalar('cl_average_accuracy', avg_accuracy, task - 1)\n",
        "        print(f'Maximum per-task accuracies: {max_accuracies}\\n')\n",
        "\n",
        "    print(config)\n",
        "    experiment.end()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOJ2P5YEMeHA"
      },
      "outputs": [],
      "source": [
        "noisy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "---- Task 1 (OCS) ----\n",
        "OCS >> (average accuracy): 96.34\n",
        "OCS >> (Forgetting): 0.0\n",
        "Maximum per-task accuracies: [96.34]\n",
        "\n",
        "---- Task 2 (OCS) ----\n",
        "OCS >> (average accuracy): 95.07\n",
        "OCS >> (Forgetting): 0.022700000000000102\n",
        "Maximum per-task accuracies: [96.34]\n",
        "\n",
        "---- Task 3 (OCS) ----\n",
        "OCS >> (average accuracy): 91.67999999999999\n",
        "OCS >> (Forgetting): 0.06744999999999997\n",
        "Maximum per-task accuracies: [96.34]\n",
        "\n",
        "---- Task 4 (OCS) ----\n",
        "OCS >> (average accuracy): 86.46000000000001\n",
        "OCS >> (Forgetting): 0.12986666666666669\n",
        "Maximum per-task accuracies: [96.34]\n",
        "\n",
        "---- Task 5 (OCS) ----\n",
        "OCS >> (average accuracy): 80.56\n",
        "OCS >> (Forgetting): 0.19400000000000003\n",
        "Maximum per-task accuracies: [96.34]\n",
        "\n",
        "---- Task 6 (OCS) ----\n",
        "OCS >> (average accuracy): 71.925\n",
        "OCS >> (Forgetting): 0.29005999999999993\n",
        "Maximum per-task accuracies: [96.34]\n",
        "\n",
        "---- Task 7 (OCS) ----\n",
        "OCS >> (average accuracy): 71.44142857142857\n",
        "OCS >> (Forgetting): 0.28664999999999996\n",
        "Maximum per-task accuracies: [96.34]\n",
        "\n",
        "---- Task 8 (OCS) ----\n",
        "OCS >> (average accuracy): 64.3625\n",
        "OCS >> (Forgetting): 0.36010000000000003\n",
        "Maximum per-task accuracies: [96.34]\n",
        "\n",
        "---- Task 9 (OCS) ----\n",
        "OCS >> (average accuracy): 59.75111111111111\n",
        "OCS >> (Forgetting): 0.40275000000000005\n",
        "Maximum per-task accuracies: [96.34]\n",
        "\n",
        "---- Task 10 (OCS) ----\n",
        "OCS >> (average accuracy): 58.386\n",
        "OCS >> (Forgetting): 0.4117333333333334\n",
        "Maximum per-task accuracies: [96.34]\n",
        "\n",
        "---- Task 11 (OCS) ----\n",
        "OCS >> (average accuracy): 54.01090909090909\n",
        "OCS >> (Forgetting): 0.4536800000000001\n",
        "Maximum per-task accuracies: [96.34]\n",
        "\n",
        "---- Task 12 (OCS) ----\n",
        "OCS >> (average accuracy): 52.0975\n",
        "OCS >> (Forgetting): 0.469609090909091\n",
        "Maximum per-task accuracies: [96.34]\n",
        "\n",
        "---- Task 13 (OCS) ----\n",
        "OCS >> (average accuracy): 49.24307692307692\n",
        "OCS >> (Forgetting): 0.49751666666666666\n",
        "Maximum per-task accuracies: [96.34]\n",
        "\n",
        "---- Task 14 (OCS) ----\n",
        "OCS >> (average accuracy): 47.332142857142856\n",
        "OCS >> (Forgetting): 0.5127076923076924\n",
        "Maximum per-task accuracies: [96.34]\n",
        "\n",
        "---- Task 15 (OCS) ----\n",
        "OCS >> (average accuracy): 41.68933333333334\n",
        "OCS >> (Forgetting): 0.5661071428571428\n",
        "Maximum per-task accuracies: [96.34]\n",
        "\n",
        "---- Task 16 (OCS) ----\n",
        "OCS >> (average accuracy): 35.1625\n",
        "OCS >> (Forgetting): 0.6315933333333335\n",
        "Maximum per-task accuracies: [96.34]\n",
        "\n",
        "---- Task 17 (OCS) ----\n",
        "OCS >> (average accuracy): 28.78529411764706\n",
        "OCS >> (Forgetting): 0.6922875000000001\n",
        "Maximum per-task accuracies: [96.34]\n",
        "\n",
        "---- Task 18 (OCS) ----\n",
        "OCS >> (average accuracy): 27.51833333333333\n",
        "OCS >> (Forgetting): 0.7042705882352942\n",
        "Maximum per-task accuracies: [96.34]\n",
        "\n",
        "---- Task 19 (OCS) ----\n",
        "OCS >> (average accuracy): 18.997894736842103\n",
        "OCS >> (Forgetting): 0.7771666666666667\n",
        "Maximum per-task accuracies: [96.34]\n",
        "\n",
        "---- Task 20 (OCS) ----\n",
        "OCS >> (average accuracy): 11.6035\n",
        "OCS >> (Forgetting): 0.8515736842105264\n",
        "Maximum per-task accuracies: [96.34]"
      ],
      "metadata": {
        "id": "gcFEavH3Kyn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data for each task\n",
        "tasks = list(range(1, 21))  # Task numbers from 1 to 20\n",
        "average_accuracy =[\n",
        "    96.34, 95.07, 91.68, 86.46, 80.56, 71.925, 71.44142857142857,\n",
        "    64.3625, 59.75111111111111, 58.386, 54.01090909090909, 52.0975,\n",
        "    49.24307692307692, 47.332142857142856, 41.68933333333334, 35.1625,\n",
        "    28.78529411764706, 27.51833333333333, 18.997894736842103, 11.6035\n",
        "]\n",
        "forgetting = [\n",
        "    0.0, 0.022700000000000102, 0.06744999999999997, 0.12986666666666669,\n",
        "    0.19400000000000003, 0.29005999999999993, 0.28664999999999996, 0.36010000000000003,\n",
        "    0.40275000000000005, 0.4117333333333334, 0.4536800000000001, 0.469609090909091,\n",
        "    0.49751666666666666, 0.5127076923076924, 0.5661071428571428, 0.6315933333333335,\n",
        "    0.6922875000000001, 0.7042705882352942, 0.7771666666666667, 0.8515736842105264\n",
        "]\n",
        "\n",
        "# Create figure and axis\n",
        "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Plot average accuracy\n",
        "color = 'tab:blue'\n",
        "ax1.set_xlabel('Task')\n",
        "ax1.set_ylabel('Average Accuracy (%)', color=color)\n",
        "ax1.plot(tasks, average_accuracy, 'o-', color=color, label='Average Accuracy')\n",
        "ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "# Create a second y-axis to plot forgetting\n",
        "ax2 = ax1.twinx()\n",
        "color = 'tab:red'\n",
        "ax2.set_ylabel('Forgetting', color=color)\n",
        "ax2.plot(tasks, forgetting, 's-', color=color, label='Forgetting')\n",
        "ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "# Title and legend\n",
        "plt.title('Average Accuracy and Forgetting per Task')\n",
        "ax1.legend(loc='upper left')\n",
        "ax2.legend(loc='upper right')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "DYAuZ-UoOe_D",
        "outputId": "9be789b0-0e19-4ef6-c1b0-282f2341bfb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Rest of the plotting code\n"
      ],
      "metadata": {
        "id": "ao6Zyr3uNpRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rot mnist"
      ],
      "metadata": {
        "id": "kjbMMhDAQaVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "---- Task 1 (OCS) ----\n",
        "OCS >> (average accuracy): 97.86\n",
        "OCS >> (Forgetting): 0.0\n",
        "Maximum per-task accuracies: [97.86]\n",
        "\n",
        "---- Task 2 (OCS) ----\n",
        "OCS >> (average accuracy): 97.37\n",
        "OCS >> (Forgetting): 0.011700000000000018\n",
        "Maximum per-task accuracies: [97.86]\n",
        "\n",
        "---- Task 3 (OCS) ----\n",
        "OCS >> (average accuracy): 95.64333333333333\n",
        "OCS >> (Forgetting): 0.03375\n",
        "Maximum per-task accuracies: [97.86]\n",
        "\n",
        "---- Task 4 (OCS) ----\n",
        "OCS >> (average accuracy): 92.52250000000001\n",
        "OCS >> (Forgetting): 0.07119999999999994\n",
        "Maximum per-task accuracies: [97.86]\n",
        "\n",
        "---- Task 5 (OCS) ----\n",
        "OCS >> (average accuracy): 88.26599999999999\n",
        "OCS >> (Forgetting): 0.11939999999999998\n",
        "Maximum per-task accuracies: [97.86]\n",
        "\n",
        "---- Task 6 (OCS) ----\n",
        "OCS >> (average accuracy): 82.74166666666666\n",
        "OCS >> (Forgetting): 0.1802\n",
        "Maximum per-task accuracies: [97.86]\n",
        "\n",
        "---- Task 7 (OCS) ----\n",
        "OCS >> (average accuracy): 78.38857142857144\n",
        "OCS >> (Forgetting): 0.2254166666666667\n",
        "Maximum per-task accuracies: [97.86]\n",
        "\n",
        "---- Task 8 (OCS) ----\n",
        "OCS >> (average accuracy): 72.27125000000001\n",
        "OCS >> (Forgetting): 0.2900142857142858\n",
        "Maximum per-task accuracies: [97.86]\n",
        "\n",
        "---- Task 9 (OCS) ----\n",
        "OCS >> (average accuracy): 69.60777777777777\n",
        "OCS >> (Forgetting): 0.3138375\n",
        "Maximum per-task accuracies: [97.86]\n",
        "\n",
        "---- Task 10 (OCS) ----\n",
        "OCS >> (average accuracy): 66.56800000000001\n",
        "OCS >> (Forgetting): 0.3426444444444444\n",
        "Maximum per-task accuracies: [97.86]\n",
        "\n",
        "---- Task 11 (OCS) ----\n",
        "OCS >> (average accuracy): 62.862727272727284\n",
        "OCS >> (Forgetting): 0.3762\n",
        "Maximum per-task accuracies: [97.86]\n",
        "\n",
        "---- Task 12 (OCS) ----\n",
        "OCS >> (average accuracy): 61.919999999999995\n",
        "OCS >> (Forgetting): 0.3831636363636364\n",
        "Maximum per-task accuracies: [97.86]\n",
        "\n",
        "---- Task 13 (OCS) ----\n",
        "OCS >> (average accuracy): 58.15538461538462\n",
        "OCS >> (Forgetting): 0.4221999999999999\n",
        "Maximum per-task accuracies: [97.86]\n",
        "\n",
        "---- Task 14 (OCS) ----\n",
        "OCS >> (average accuracy): 57.738571428571426\n",
        "OCS >> (Forgetting): 0.41934615384615376\n",
        "Maximum per-task accuracies: [97.86]\n",
        "\n",
        "---- Task 15 (OCS) ----\n",
        "OCS >> (average accuracy): 55.688\n",
        "OCS >> (Forgetting): 0.4362357142857143\n",
        "Maximum per-task accuracies: [97.86]\n",
        "\n",
        "---- Task 16 (OCS) ----\n",
        "OCS >> (average accuracy): 52.900000000000006\n",
        "OCS >> (Forgetting): 0.4622933333333333\n",
        "Maximum per-task accuracies: [97.86]\n",
        "\n",
        "---- Task 17 (OCS) ----\n",
        "OCS >> (average accuracy): 50.43058823529412\n",
        "OCS >> (Forgetting): 0.48718125\n",
        "Maximum per-task accuracies: [97.86]\n",
        "\n",
        "---- Task 18 (OCS) ----\n",
        "OCS >> (average accuracy): 39.75333333333333\n",
        "OCS >> (Forgetting): 0.5918764705882353\n",
        "Maximum per-task accuracies: [97.86]\n",
        "\n",
        "---- Task 19 (OCS) ----\n",
        "OCS >> (average accuracy): 38.33736842105263\n",
        "OCS >> (Forgetting): 0.5993055555555555\n",
        "Maximum per-task accuracies: [97.86]\n",
        "\n",
        "---- Task 20 (OCS) ----\n",
        "OCS >> (average accuracy): 14.636500000000002\n",
        "OCS >> (Forgetting): 0.835205263157895\n",
        "Maximum per-task accuracies: [97.86]\n"
      ],
      "metadata": {
        "id": "INJ3cjIiOAM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Updated data for each task\n",
        "tasks = list(range(1, 21))  # Task numbers from 1 to 20\n",
        "average_accuracies = [\n",
        "    97.86, 97.37, 95.64333333333333, 92.52250000000001, 88.26599999999999, 82.74166666666666,\n",
        "    78.38857142857144, 72.27125000000001, 69.60777777777777, 66.56800000000001, 62.862727272727284,\n",
        "    61.919999999999995, 58.15538461538462, 57.738571428571426, 55.688, 52.900000000000006,\n",
        "    50.43058823529412, 39.75333333333333, 38.33736842105263, 14.636500000000002\n",
        "]\n",
        "forgetting = [\n",
        "    0.0, 0.011700000000000018, 0.03375, 0.07119999999999994, 0.11939999999999998, 0.1802,\n",
        "    0.2254166666666667, 0.2900142857142858, 0.3138375, 0.3426444444444444, 0.3762, 0.3831636363636364,\n",
        "    0.4221999999999999, 0.41934615384615376, 0.4362357142857143, 0.4622933333333333, 0.48718125,\n",
        "    0.5918764705882353, 0.5993055555555555, 0.835205263157895\n",
        "]\n",
        "\n",
        "# Create figure and axis\n",
        "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Plot average accuracy\n",
        "color = 'tab:blue'\n",
        "ax1.set_xlabel('Task')\n",
        "ax1.set_ylabel('Average Accuracy (%)', color=color)\n",
        "ax1.plot(tasks, average_accuracies, 'o-', color=color, label='Average Accuracy')\n",
        "ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "# Create a second y-axis to plot forgetting\n",
        "ax2 = ax1.twinx()\n",
        "color = 'tab:red'\n",
        "ax2.set_ylabel('Forgetting', color=color)\n",
        "ax2.plot(tasks, forgetting, 's-', color=color, label='Forgetting')\n",
        "ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "# Title and legend\n",
        "plt.title('Average Accuracy and Forgetting per Task (OCS)')\n",
        "ax1.legend(loc='upper left')\n",
        "ax2.legend(loc='upper right')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GkqgVWApUNBo",
        "outputId": "4aec3c2f-347e-414f-bb42-f8892981f898",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ybyWyrTwUO4i"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}